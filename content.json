{"meta":{"title":"SunJun","subtitle":null,"description":"简单就好","author":"SunJun","url":""},"pages":[{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"client/index.html","permalink":"/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-01-03T07:10:24.711Z","comments":false,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"[さくら荘のSunJun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-11-16T09:29:49.000Z","comments":true,"path":"comment/index.html","permalink":"/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"lab/index.html","permalink":"/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"donate/index.html","permalink":"/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-01-03T07:45:22.173Z","comments":true,"path":"links/index.html","permalink":"/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-01-03T07:36:10.187Z","comments":false,"path":"music/index.html","permalink":"/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-11-16T09:29:49.000Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-11-16T09:29:49.000Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-11-16T09:29:49.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"}],"posts":[{"title":"常见web服务攻击和防御手段","slug":"2019-11-04-common_means_web_attack","date":"2019-11-04T08:50:00.000Z","updated":"2020-01-03T09:35:02.420Z","comments":true,"path":"2019/11/04/2019-11-04-common_means_web_attack/","link":"","permalink":"/2019/11/04/2019-11-04-common_means_web_attack/","excerpt":"","text":"常见web服务攻击和防御手段XSS跨站脚本攻击XSS (Cross-Site Scripting)，跨站脚本攻击，因为缩写和 CSS重叠，所以只能叫 XSS。XSS 的原理是恶意攻击者往 Web 页面里插入恶意可执行网页脚本代码，当用户浏览该页之时，嵌入其中 Web 里面的脚本代码会被执行，从而可以达到攻击者盗用户信息或其他侵犯用户安全隐私的目的。 反射型XSS：一般是通过给别人发送带有恶意脚本代码参数的 URL，当URL被打开时特有的恶意代码参数被HTML解析执行。 防御手段： 前端渲染数据都要从后端来 尽量不要从URL、document.referrer、document.forms等这些api获取数据并直接渲染 尽量不要使用 eval, new Function()，document.write()，document.writeln()等这些可以直接执行字符串的方法 如果必须要用的话也需要对传入的字符串进行转义 前端渲染的时候也需要对字符串做转义编码 持久性XSS：一般是通过留言、评论等这些可以和后端交互的功能将带有脚本的内容存到后端数据库的，当用户查询评论的时候就可能被查询出来渲染并执行 防御手段： CSP：本质上也就是设置白名单的方式： 设置 HTTP Header 中的 Content-Security-Policy 只允许加载本站资源: Content-Security-Policy: default-src ‘self’ 只允许加载 HTTPS 协议图片: Content-Security-Policy: img-src https://* 允许加载任何来源框架: Content-Security-Policy: child-src ‘none’ 设置 meta 标签的方式 转义字符：转义用户输入的引号、尖括号、斜杆 HttpOnly Cookie：设置cookie时将属性设置成HttpOnly就可以避免cookie被恶意js获取 CSRF跨站请求伪造它利用用户已登录的身份，在用户毫不知情的情况下，以用户的名义完成非法操作。用户已登录A网站，此时访问B网站时B网站通过获取A网站的cookie发送请求给A网站完成非法操作。 防御手段： - 设置cookie SameSite属性，cookie不随跨域请求发送 - Referer Check，但是https跳转到http由于安全考虑浏览器不会携带referer，所以无法完全依赖referer检查来防止CSRF攻击 - 每次请求都产生一个随机的Token，服务端验证Token - 关键操作必须输入验证码 点击劫持原理：攻击者将需要攻击的网站通过 iframe 嵌套的方式嵌入自己的网页中，并将 iframe 设置为透明，在页面中透出一个按钮诱导用户点击。如何防御： - 通过HTTP头`X-FRAME-OPTIONS`限制iframe嵌套， - js防御，js中判断页面处于iframe中时不显示网页内容, 通过`self==top`判断窗口是否被iframe嵌套 URL跳转漏洞服务端解析url参数中的网站并跳转，恶意用户可以伪造跳转链接用来攻击安全意识薄弱的用户。 如何防御： - refere限制，限制访问来源 - 通过加入用户不可控的token来进行校验 SQL注入利用潜在的数据库漏洞访问和修改数据 注入的原理：比如前端需要输入用户名密码以登录系统，后端通过SELECT * FROM user WHERE username=&#39;admin&#39; AND psw=&#39;password&#39;查询用户信息，这时输入一个username是admin--就可以直接以admin账号登录系统 如何防御： - 控制web应用访问的操作权限 - 后端检查输入数据是否符合预期 - 对于特殊字符进行转义或编码处理 - 使用数据库提供的参数化查询（预编译)，不进行sql拼接 OS命令注入攻击和sql注入原理一样，只不过SQL注入是针对数据库的，而OS命令注入是针对操作系统的。 原理：构造命令提交给web应用程序，web应用程序提取构造的命令，拼接到被执行的命令中，因注入的命令打破了原有命令结构，导致web应用执行了额外的命令。举例： exec.Command(fmt.Sprintf(&quot;git clone %s&quot;,repo)) golang中我们让前端输入一个git仓库地址来克隆git仓库代码，这时如果输入一个正常的仓库地址可以正常执行。但是如果我们传入一个这样的参数repo adress &amp;&amp; rm -rf /*而正好我们的程序又是以root启动，那么后面的命令就会被执行，根目录会被删除。 如何防御： - 后端对前端提前的数据进行规则校验（例如正则校验） - 执行命令前对参数进行转义和过滤 - 不要直接拼接命令通过工具做拼接、转义预处理","categories":[{"name":"network-security","slug":"network-security","permalink":"/categories/network-security/"}],"tags":[{"name":"web","slug":"web","permalink":"/tags/web/"}],"keywords":[{"name":"network-security","slug":"network-security","permalink":"/categories/network-security/"}]},{"title":"Go源码阅读-私有库拉取问题","slug":"2019-10-29-go-primary-repo","date":"2019-10-29T06:24:30.000Z","updated":"2020-01-03T09:35:02.381Z","comments":true,"path":"2019/10/29/2019-10-29-go-primary-repo/","link":"","permalink":"/2019/10/29/2019-10-29-go-primary-repo/","excerpt":"","text":"go get拉取私有库报错http拉取问题(&gt;=1.6)go get github.com/sjatsh/transfo # cd .; git clone -- https://github.com/sjatsh/transfo /Users/jane/go/src/github.com/sjatsh/transfo Cloning into &#39;/Users/jane/go/src/github.com/sjatsh/transfo&#39;... fatal: could not read Username for &#39;https://github.com&#39;: terminal prompts disabled package github.com/sjatsh/transfo: exit status 128 我们刚开始学习golang的时候我们需要go get拉取某个私有库的时候经常就会遇到类似的问题，这个问题的关键在于 terminal prompts disabled 这个错误信息，问题是由于http拉取私有仓库时需要用户名密码，但是git却没有提示输入用户名密码，直接看源码： 参见源码97行：go/get.go at release-branch.go1.6 · golang/go · GitHub func runGet(cmd *base.Command, args []string) { // 这段代码默认设置 GIT_TERMINAL_PROMPT = 0 if os.Getenv(&quot;GIT_TERMINAL_PROMPT&quot;) == &quot;&quot; { os.Setenv(&quot;GIT_TERMINAL_PROMPT&quot;, &quot;0&quot;) } } os.Setenv(&quot;GIT_TERMINAL_PROMPT&quot;, &quot;0&quot;) 这行标识关闭git命令行提示 ，也就导致了上面提到的terminal prompts disabled错误。 解决办法：export GIT_TERMINAL_PROMPT=1 手动设置git开启命令行提示，之后再go get就会提示输入用户名密码，一切正常： 这个issue有对这个问题的详细讨论：http://golang.org/issue/9341golang.org/issue/9341 替换成ssh方式后存在的问题替换go get时git将http拉取换成ssh拉取代码 git config --global url.&quot;git@github.com:&quot;.insteadOf &quot;https://github.com/&quot; go get通过ssh方式拉取代码卡死问题(&lt;1.8版本)开启ssh ControlMaster配置后： Host * ControlPath ~/.ssh/master-%r@%h:%p ControlMaster auto ServerAliveInterval 30 由于开启 ControlMaster 后ssh会使用守护进程维护一个master链接，之后所有请求都会重用这一个TCP链接，他的stdout和stderr也不会退出。然而go get时使用子进程运行git clone并且监听子进程stdout和stderr退出，但是开启ControlMaster后ssh的守护进程不会退出，go get就会一直卡死。 1.8之后版本默认设置GIT_SSH_COMMAND=ssh -o ControlMaster=no禁用ssh ControlMaster 解决。 参见1.8版本源码151行：go/get.go at release-branch.go1.8 · golang/go · GitHub","categories":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"/tags/golang/"}],"keywords":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}]},{"title":"Rabin-Karp在golang中的实现","slug":"2019-09-26-rabin-karp","date":"2019-09-26T04:01:30.000Z","updated":"2020-01-03T09:35:02.437Z","comments":true,"path":"2019/09/26/2019-09-26-rabin-karp/","link":"","permalink":"/2019/09/26/2019-09-26-rabin-karp/","excerpt":"","text":"简介Rabin-Karp字符串快速查找算法和FNV hash算法是golang中strings包中字符串查所用到的具体算法，算法的核心就在于循环hash，而 FNV则是散列方法的具体算法实现。 算法思想Rabin-Karp算法思想： 假设待匹配字符串长度M，目标字符串长度N（N&gt;M） 首先计算待匹配字符串hash，计算目标字符串前M个字符hash 比较前两个hash值，比较次数N-M+1 若hash不相等，继续计算目标字符串下一个长度为M的hash并继续循环比较 若hash相等则再次判断字符串是否相等已确保正确性 FNV hash： 将字符串看作是字符串长度的整数，这个数的进制是一个质数。计算出来结果之后，按照哈希的范围求余数得到结果。 其中不同机制对应质数分别是： 32 bit FNV_prime = 224 + 28 + 0x93 = 16777619 64 bit FNV_prime = 240 + 28 + 0xb3 = 1099511628211 128 bit FNV_prime = 288 + 28 + 0x3b = 309485009821345068724781371 256 bit FNV_prime = 2168 + 28 + 0x63 = 374144419156711147060143317175368453031918731002211 512 bit FNV_prime = 2344 + 28 + 0x57 = 35835915874844867368919076489095108449946327955754392558399825615420669938882575 126094039892345713852759 1024 bit FNV_prime = 2680 + 28 + 0x8d = 50164565101131186554345988110352789550307653454047907443030175238311120551081474 51509157692220295382716162651878526895249385292291816524375083746691371804094271 873160484737966720260389217684476157468082573 以上这几个数都是质数（哈希的理论基石，质数分辨定理)，简单地说就是：n个不同的质数可以“分辨”的连续整数的个数和他们的乘积相等。“分辨”就是指这些连续的整数不可能有完全相同的余数序列。证明详见 如果想要得到不是上面进制的hash： 比如想得到24位的哈希值，方法：取上面比24大的最小的位数，当然是32了，先算对应32位哈希值，再转换成24位的。转换方法：32 - 24 = 8， 好了把得到的32砍成两段，高8位最和低24位。第8位与低24位中的低8位做抑或，得到的24位值是最终结果。（hash&gt;&gt;24) ^ (hash &amp; 0xFFFFFF); 如果想得到范围在0~9999的哈希值，方法：取上面比9999大的最小的位数，当然是32，先算对应32位哈希值，再mod（9999 +１）。 如上所述，结合Rabin-Karp的思想加上FNV hash就可以实现所谓的字符串快速查找算法了。 结合golang源码src/strings/strings.go // Rabin-Karp 中需要使用的32位FNV hash算法中的基础质数（相当于进制） const primeRK = 16777619 // hash散列方法， 返回字符串hash以及 primeRK的k-1（len(sep)-1）次方 func hashStr(sep string) (uint32, uint32) { hash := uint32(0) for i := 0; i &lt; len(sep); i++ { hash = hash*primeRK + uint32(sep[i]) // 循环得到字符串hash } // 位运算巧妙的获取 primeRK 的 len(sqp)-1 次方 var pow, sq uint32 = 1, primeRK for i := len(sep); i &gt; 0; i &gt;&gt;= 1 { if i&amp;1 != 0 { pow *= sq } sq *= sq } return hash, pow } func indexRabinKarp(s, substr string) int { // Rabin-Karp search hashss, pow := hashStr(substr) n := len(substr) var h uint32 // 计算目标字符串前n位hash并与待匹配字符串hash进行对比 for i := 0; i &lt; n; i++ { h = h*primeRK + uint32(s[i]) } // hash相同并且字符串相等则返回当前位置下标 if h == hashss &amp;&amp; s[:n] == substr { return 0 } // Rabin-Karp 算法的精华所在，相面详细介绍 for i := n; i &lt; len(s); { h *= primeRK h += uint32(s[i]) h -= pow * uint32(s[i-n]) i++ if h == hashss &amp;&amp; s[i-n:i] == substr { return i - n } } return -1 } 结合源码可以知道：如果现在我们要求第i位往后k个长度字符串的hash可以列个公式 其中：s[i] 表示第i位字节对应32位整数也就是上面uint32(s[i]) （这里强转一下也就是对2^32次方取余了），R 就是 对应进制的FNV_prime。 由上述类推H(i+1)的hash公式就是： 由此可以看出来：每次我们其实不用重新计算整个字符串的hash而是直接原hash值乘以R加上s[k-1]并且减去s[i]R^(k-1)，这里也就是 FNV_prime的k-1次方，对应上面代码： var pow, sq uint32 = 1, primeRK for i := len(sep); i &gt; 0; i &gt;&gt;= 1 { if i&amp;1 != 0 { pow *= sq } sq *= sq } 相对于暴力匹配O(mn)是时间复杂度， Rabin-Karp 的时间复杂度在O(m+n)， 最坏的情况每次hash相同字符串不相同时间复杂度会变成O(mn)但是这种情况比较罕见。 Rabin-Karp 还有个优点在于他可以进行多模式匹配，比如论文重复性检测，只要预热计算出所有带匹配字符串的hash，目标字符串的遍历比较时只是多一步比较所有待匹配字符串hash。如果待匹配字符串个数是k，那么 Rabin-Karp 的时间复杂度是O(nk)。","categories":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"/tags/golang/"}],"keywords":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}]},{"title":"go平滑重启选型和项目实践","slug":"2019-09-17-graceful-reload","date":"2019-09-17T13:38:00.000Z","updated":"2020-01-03T09:35:02.445Z","comments":true,"path":"2019/09/17/2019-09-17-graceful-reload/","link":"","permalink":"/2019/09/17/2019-09-17-graceful-reload/","excerpt":"","text":"什么是平滑重启当线上代码需要更新时,我们平时一般的做法需要先关闭服务然后再重启服务. 这时线上可能存在大量正在处理的请求, 这时如果我们直接关闭服务会造成请求全部 中断, 影响用户体验; 在重启重新提供服务之前, 新请求进来也会502. 这时就出现两个需要解决的问题: 老服务正在处理的请求必须处理完才能退出(优雅退出) 新进来的请求需要正常处理,服务不能中断(平滑重启) 本文主要结合linux和Golang中相关实现来介绍如何选型与实践过程. 优雅退出在实现优雅重启之前首先需要解决的一个问题是如何优雅退出：我们知道在go 1.8.x后，golang在http里加入了shutdown方法，用来控制优雅退出。社区里不少http graceful动态重启，平滑重启的库，大多是基于http.shutdown做的。 http shutdown 源码分析先来看下http shutdown的主方法实现逻辑。用atomic来做退出标记的状态，然后关闭各种的资源，然后一直阻塞的等待无空闲连接，每500ms轮询一次。 var shutdownPollInterval = 500 * time.Millisecond func (srv *Server) Shutdown(ctx context.Context) error { // 标记退出的状态 atomic.StoreInt32(&amp;srv.inShutdown, 1) srv.mu.Lock() // 关闭listen fd，新连接无法建立。 lnerr := srv.closeListenersLocked() // 把server.go的done chan给close掉，通知等待的worekr退出 srv.closeDoneChanLocked() // 执行回调方法，我们可以注册shutdown的回调方法 for _, f := range srv.onShutdown { go f() } // 每500ms来检查下，是否没有空闲的连接了，或者监听上游传递的ctx上下文。 ticker := time.NewTicker(shutdownPollInterval) defer ticker.Stop() for { if srv.closeIdleConns() { return lnerr } select { case &lt;-ctx.Done(): return ctx.Err() case &lt;-ticker.C: } } } … 是否没有空闲的连接 func (s *Server) closeIdleConns() bool { s.mu.Lock() defer s.mu.Unlock() quiescent := true for c := range s.activeConn { st, unixSec := c.getState() if st == StateNew &amp;&amp; unixSec &lt; time.Now().Unix()-5 { st = StateIdle } if st != StateIdle || unixSec == 0 { quiescent = false continue } c.rwc.Close() delete(s.activeConn, c) } return quiescent } 关闭server.doneChan和监听的文件描述符// 关闭doen chan func (s *Server) closeDoneChanLocked() { ch := s.getDoneChanLocked() select { case &lt;-ch: // Already closed. Don&#39;t close again. default: // Safe to close here. We&#39;re the only closer, guarded // by s.mu. close(ch) } } // 关闭监听的fd func (s *Server) closeListenersLocked() error { var err error for ln := range s.listeners { if cerr := (*ln).Close(); cerr != nil &amp;&amp; err == nil { err = cerr } delete(s.listeners, ln) } return err } // 关闭连接 func (c *conn) Close() error { if !c.ok() { return syscall.EINVAL } err := c.fd.Close() if err != nil { err = &amp;OpError{Op: &quot;close&quot;, Net: c.fd.net, Source: c.fd.laddr, Addr: c.fd.raddr, Err: err} } return err } 这么一系列的操作后，server.go的serv主监听方法也就退出了。func (srv *Server) Serve(l net.Listener) error { ... for { rw, e := l.Accept() if e != nil { select { // 退出 case &lt;-srv.getDoneChan(): return ErrServerClosed default: } ... return e } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(ctx) } } 那么如何保证用户在请求完成后，再关闭连接的？func (s *Server) doKeepAlives() bool { return atomic.LoadInt32(&amp;s.disableKeepAlives) == 0 &amp;&amp; !s.shuttingDown() } // Serve a new connection. func (c *conn) serve(ctx context.Context) { defer func() { ... xiaorui.cc ... if !c.hijacked() { // 关闭连接，并且标记退出 c.close() c.setState(c.rwc, StateClosed) } }() ... ctx, cancelCtx := context.WithCancel(ctx) c.cancelCtx = cancelCtx defer cancelCtx() c.r = &amp;connReader{conn: c} c.bufr = newBufioReader(c.r) c.bufw = newBufioWriterSize(checkConnErrorWriter{c}, 4&lt;&lt;10) for { // 接收请求 w, err := c.readRequest(ctx) if c.r.remain != c.server.initialReadLimitSize() { c.setState(c.rwc, StateActive) } ... ... // 匹配路由及回调处理方法 serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() if c.hijacked() { return } ... // 判断是否在shutdown mode, 选择退出 if !w.conn.server.doKeepAlives() { return } } ... 优雅重启方法演进从linux系统的角度 直接使用exec，把代码段替换成新的程序的代码， 废弃原有的数据段和堆栈段并为新程序分配新的数据段与堆栈段，唯一留下的就是进程号。 这样就会存在的一个问题就是老进程无法优雅退出，老进程正在处理的请求无法正常处理完成后退出。并且新进程服务的启动并不是瞬时的，新进程在listen之后accept之前，新连接可能因为syn queue队列满了而被拒绝(这种情况很少, 但在并发很高的情况下是有可能出现)。这里结合下图与TCP三次握手的过程来看可能会好理解很多，个人感觉有种豁然开朗的感觉. 通过fork后exec创建新进程， exec前在老进程中通过fcntl(fd, F_SETFD, 0);清除FD_CLOEXEC标志，之后exec新进程就会继承老进程 的fd并可以直接使用。之后新进程和老进程listen相同的fd同时提供服务， 在新进程正常启动服务后发送信号给老进程, 老进程优雅退出。之后所有请求 都到了新进程也就完成了本次优雅重启。结合实际线上环境存在的问题: 这时新的子进程由于父进程的退出, 系统会把它的父进程改成1号进程,由于线上环境大多数服务都是通过 supervisor进行管理的,这就会存在一个问题, supervisor会认为服务异常退出, 会重新启动一个新进程. 通过给文件描述符设置SO_REUSEPORT标志让两个进程监听同一个端口, 这里存在的问题是这里使用的是两个不同的FD监听同一个端口，老进程退出的时候。 syn queue队列中还未被accept的连接会被内核kill掉。 通过ancilliary data系统调用使用UNIX域套接字在进程之间传递文件描述符， 这样也可以实现优雅重启。但是这样的实现会比较复杂， HAProxy中 实现了该模型。 直接fork然后exec调用，子进程会继承所有父进程打开的文件描述符， 子进程拿到的文件描述符从3递增， 顺序与父进程打开顺序一致。子进程通过epoll_ctl 注册fd并注册事件处理函数(这里以epoll模型为例)， 这样子进程就能和父进程监听同一个端口的请求了(此时父子进程同时提供服务)， 当子进程正常启动并提供服务后 发送SIGHUP给父进程， 父进程优雅退出此时子进程提供服务， 完成优雅重启。 Golang中的实现从上面看， 相对来说比较容易的实现是直接forkandexec的方式最简单， 那么接下来讨论下在Golang中的具体实现。 我们知道Golang中socket的fd默认是设置了FD_CLOEXEC标志的(net/sys_cloexec.go参考源码) // Wrapper around the socket system call that marks the returned file // descriptor as nonblocking and close-on-exec. func sysSocket(family, sotype, proto int) (int, error) { // See ../syscall/exec_unix.go for description of ForkLock. syscall.ForkLock.RLock() s, err := socketFunc(family, sotype, proto) if err == nil { syscall.CloseOnExec(s) } syscall.ForkLock.RUnlock() if err != nil { return -1, os.NewSyscallError(&quot;socket&quot;, err) } if err = syscall.SetNonblock(s, true); err != nil { poll.CloseFunc(s) return -1, os.NewSyscallError(&quot;setnonblock&quot;, err) } return s, nil } 所以在exec后fd会被系统关闭，但是我们可以直接通过os.Command来实现。这里有些人可能有点疑惑了不是FD_CLOEXEC标志的设置，新起的子进程继承的fd会被关闭。事实是os.Command启动的子进程可以继承父进程的fd并且使用, 阅读源码我们可以知道os.Command中通过Stdout,Stdin,Stderr以及ExtraFiles 传递的描述符默认会被Golang清除FD_CLOEXEC标志, 通过Start方法追溯进去我们可以确认我们的想法。(syscall/exec_{GOOS}.go我这里是macos的源码实现参考源码) // dup2(i, i) won&#39;t clear close-on-exec flag on Linux, // probably not elsewhere either. _, _, err1 = rawSyscall(funcPC(libc_fcntl_trampoline), uintptr(fd[i]), F_SETFD, 0) if err1 != 0 { goto childerror } 结合supervisor时的问题实际项目中, 线上服务一般是被supervisor启动的, 如上所说的我们如果通过父子进程, 子进程启动后退出父进程这种方式的话存在的问题就是子进程会被1号进程接管, 导致supervisor 认为服务挂掉重启服务,为了避免这种问题我们可以使用master, worker的方式。这种方式基本思路就是: 项目启动的时候程序作为master启动并监听端口创建socket描述符但是不对外提供服务, 然后通过os.Command创建子进程通过Stdin, Stdout, Stderr,ExtraFiles和Env传递标椎输入输出错误和文件描述符以及环境变量. 通过环境变量子进程可以知道自己是子进程并通过os.NewFile将fd注册到epoll中, 通过fd创建TCPListener对象, 绑定handle处理器之后accept接受请求并处理， 参考伪代码: f := os.NewFile(uintptr(3+i), &quot;&quot;) l, err := net.FileListener(f) if err != nil { return fmt.Errorf(&quot;failed to inherit file descriptor: %d&quot;, i) } server:=&amp;http.Server{Handler: handler} server.Serve(l) 上述过程只是启动了worker进程并提供服务, 真正的优雅重启, 可以通过接口(由于线上环境发布机器可能没有权限,只能曲线救国)或者发送信号给worker进程,worker 发送信号给master, master进程收到信号后起一个新worker, 新worker启动并正常提供服务后发送一个信号给master,master发送退出信号给老worker,老worker退出. 日志收集的问题， 如果项目本身日志是直接打到文件，可能会存在fd滚动等问题(目前没有研究透彻). 目前的解决方案是项目log全部输出到stdout由supervisor来收集到日志文件， 创建worker的时候stdout, stderr是可以继承过去的，这就解决了日志的问题， 如果有更好的方式环境一起探讨。 参考文章谈谈golang网络库的入门认识深入理解Linux TCP backloggo优雅升级/重启工具调研记一次惊心的网站TCP队列问题排查经历accept和accept4的区别","categories":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"/tags/golang/"}],"keywords":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}]},{"title":"","slug":"2018-04-25-java_classloader","date":"2019-09-17T13:17:33.000Z","updated":"2020-01-03T10:12:29.136Z","comments":true,"path":"2019/09/17/2018-04-25-java_classloader/","link":"","permalink":"/2019/09/17/2018-04-25-java_classloader/","excerpt":"layout: posttitle: “Java类加载器”tag: “classloader”date: “2018-04-25 00:00:00 +0800” categories: “java”基本概念最初为了满足Java Applet的需要而被开发出来的，Java Applet 需要从远程下载Java类到浏览器执行，现在Web容器和OSGi中得到了广泛的使用。类加载器负责读取 Java 字节代码，并转换成 java.lang.Class 类的实例。每个 java.lang.Class 实例用来表示一个Java类，通过 newInstance() 创建该类对象，newInstance()只能调用无参构造器。","text":"layout: posttitle: “Java类加载器”tag: “classloader”date: “2018-04-25 00:00:00 +0800” categories: “java”基本概念最初为了满足Java Applet的需要而被开发出来的，Java Applet 需要从远程下载Java类到浏览器执行，现在Web容器和OSGi中得到了广泛的使用。类加载器负责读取 Java 字节代码，并转换成 java.lang.Class 类的实例。每个 java.lang.Class 实例用来表示一个Java类，通过 newInstance() 创建该类对象，newInstance()只能调用无参构造器。除此之外，ClassLoader还负责加载 Java 应用所需的资源，如图像文件和配置文件等。 除了BootstrapClassLoader是由C++或其他语言编写，其他都是 java.lang.ClassLoader 的实例，ClassLoader 中与加载类相关的方法： 方法 说明 getParent() 返回该类加载器的父类加载器。 loadClass(String name) 加载名称为name的类 findClass(String name) 查找名称为 name的类 findLoadedClass(String name) 查找名称为 name的已经被加载过的类 defineClass(String name, byte[] b, int off, int len) 把字节数组 b中的内容转换成 Java 类 resolveClass(Class&lt;?&gt; c) 链接指定Java类 注意：Class.forName(className)加载class的同时会初始化静态代码块，ClassLoader.loadClass(className)则不会初始化静态块。 bootstrap class loader一切从bootstrap class loader开始，bootstrap class loader启动加载jre/lib/下的rt.jar、resources.jar、charsets.jar等核心类库。之后加载ExtClassLoader、 AppClassLoader BootstrapClassLoader: 由c++实现，负责在虚拟机启动时加载Jdk核心类库以及加载后两个类加载器。 ExtClassLoader: Launcher子类，继承自URLClassLoader，负责加载{JAVA_HOME}/jre/lib/ext/目录下的jar包。 AppClassLoader: Launcher子类，继承自URLClassLoader，负责加载应用程序classpath目录下的所有jar和class文件。 ExtClassLoader、AppClassLoader 最终还是调用ClassLoader中的native方法，defineClass0、defineClass1、defineClass2。 注意：经常看资料说的类载入器的层级体系(classloader hierarchy)，并不是我们一般认为的层级体系（父子类继承的层级体系）。类别载入器的层级体系另有所指，不要混淆。 ExtClassLoader、AppClassLoader其实都是由BootstrapClassLoader加载，他们调用getClassLoader()返回都是null表示加载他们的都是BootstrapClassLoader，只不过AppClassLoader设置其parent是ExtClassLoader。 类载入器的双亲委派模式上面说的类别载入器的层级体系就是为了实现双亲委派模式，所谓双亲委派模式就是类别载入器在载入类的时候，会首先请示 Parent 使用 Parent 搜索路径帮忙载入，如果 Parent 找不到，那么才用自己的搜索路径去搜索加载类 注意：需要注意的是，如果我们自己的Main.class调用了Test.class中的方法，但是Main.class放在 Parent 加载器搜索路径中，但是Test.class放在子类加载器搜索路径或其他路径，就会出现NoClassDefFoundError，这也是Java的安全策略。 类载入器的功用从下面这张图来分析： 类载入器是一个非常复杂的系统，为什么要设计这么一个复杂的系统呢？肯定是有道理的，除了动态性实现动态加载以外，其实更重要的莫过于安全性。 这张图说明了两件事： 第一：假设我们用URLClassLoader到网上随便下载一个class，这个class不可能是AppClassLoader、ExtClassLoader或者是Bootstrap Loader路径下的同名类（指全路径），因此蓄意破坏者没有办法植入代码到我们的系统，除非直接黑如你的电脑替换文件，这其实也不是Java考虑的范围了，这是操作系统安全的该考虑的。 第二：类载入器无法看到同级别类载入器载入的类，除了意味着不同的类別载入器可以载入完全相同的类之外，也排除了误用或使用別人的恶意代码的机会。 疑问上面说到一个类由一个载入器加载，他调用的方法所在的类也必须在该载入器的搜索路径下。但是就会存在一个问题，JDBC是怎么实现的？java.sql.Driver属于基础类库，由BootstrapClassLoader加载，但是JDBC Driver的实现是由各个厂商实现的，按照上面的逻辑BootstrapClassLoade应该会在jre/lib/下面找对应实现类，但是很明显我们的驱动的jar包是在classpath下面，JDBC的实现后续继续再讨论。","categories":[],"tags":[],"keywords":[]},{"title":"","slug":"2018-02-01-win_ics","date":"2019-09-16T16:33:58.000Z","updated":"2020-01-03T10:12:29.172Z","comments":true,"path":"2019/09/17/2018-02-01-win_ics/","link":"","permalink":"/2019/09/17/2018-02-01-win_ics/","excerpt":"layout: posttitle: “windows ICS”tag: “windows”date: “2018-02-01 00:00:00 +0800” categories: “system”什么是windows ICS全称windows internet connect sharing,用于在电脑两个网卡之间共享网络，让windows主机成实现一个简单的路由功能。","text":"layout: posttitle: “windows ICS”tag: “windows”date: “2018-02-01 00:00:00 +0800” categories: “system”什么是windows ICS全称windows internet connect sharing,用于在电脑两个网卡之间共享网络，让windows主机成实现一个简单的路由功能。 设置在更改适配器选项中选择连接外网的网卡右键属性，选择共享，选择允许其他用户通过此计算机的internet连接来连接并选择需要共享的网卡。默认被共享的网卡（后面称LAN口）ip是192.168.137.1，电脑接上该网口后会自动分配一个192.168.137段的ip。 存在的问题如果我们修改LAN口ip后会发现，连接lan口的设备获取的ip不是LAN口的ip段。 修改注册表通过修改注册表SYSTEM\\CurrentControlSet\\services\\SharedAccess\\Parameters中的ScopeAddress、ScopeAddressBackup、StandaloneDhcpAddress,其中ScopeAddress、ScopeAddressBackup设置修改后的LAN口ip，StandaloneDhcpAddress修改为新的LAN口DNS地址。 如何获取局域网内其他设备的ip地址如果现在有个需求需要我们获取和LAN口连接的设备的ip地址，而此设备又是dhcp，最好的方式可能就是通过arp命令来获取了（个人认为，有什么其他的更好的方式欢迎指正）。具体如何使用arp命令扫描局域网内的ip地址，可以搜索arp的使用方法。上面这些方法都可以通过go代码实现。","categories":[],"tags":[],"keywords":[]},{"title":"","slug":"2018-10-09-sputnik","date":"2019-09-16T16:33:58.000Z","updated":"2020-01-03T09:55:50.212Z","comments":true,"path":"2019/09/17/2018-10-09-sputnik/","link":"","permalink":"/2019/09/17/2018-10-09-sputnik/","excerpt":"layout: posttitle: “互联网编年史-sputnik”description: “互联网编年史-sputnik”tag: “sputnik”date: “2018-10-09 00:00:00 +0800” categories: “net” 第一颗人造卫星的发射成功，直接导致了冷战时期美俄两国的太空竞赛 &emsp;1957年苏联发射了人类第一颗人造卫星”Sputnik”。 &emsp;在Sputnik进入太空之后的60年间，人造卫星在科学、军事和民生等各个方面都获得了极其广泛的应用，对人类通讯方式带来了颠覆性的影响，揭开并深化了冷战，催生了互联网，进而彻底改变了人们认识自我、观看世界的方式。","text":"layout: posttitle: “互联网编年史-sputnik”description: “互联网编年史-sputnik”tag: “sputnik”date: “2018-10-09 00:00:00 +0800” categories: “net” 第一颗人造卫星的发射成功，直接导致了冷战时期美俄两国的太空竞赛 &emsp;1957年苏联发射了人类第一颗人造卫星”Sputnik”。 &emsp;在Sputnik进入太空之后的60年间，人造卫星在科学、军事和民生等各个方面都获得了极其广泛的应用，对人类通讯方式带来了颠覆性的影响，揭开并深化了冷战，催生了互联网，进而彻底改变了人们认识自我、观看世界的方式。 &emsp;1958年美国成功发射了自己的第一颗人造卫星Explorer I。但那个时候，驱动大国太空探索项目的动力不是探索，而是意识形态竞争和国家荣誉。Sputnik 的政治意义很清楚的一点是： 在它之前，没人想着太空竞赛。 &emsp;美国国会在 1958 年 1月通过了太空行动计划，同年 10 月 1 日 NASA 诞生。 &emsp;Sputnik的升空不仅仅标志着人类第一颗人造卫星的升空，这激发了美国想要在登月计划中领先的决心，或许可以说如果没有最初苏联的那颗人造卫星，美国的阿波罗计划不会这么快实现，大国进行太空探索的动力也不会这么足。 &emsp;1961年4月12日，苏联人加加林乘坐东方 1 号宇宙飞船花了 1 小时 48 分，绕着地球飞了一圈，完成了人类历史上第一次宇宙飞行。 &emsp;1969年7月20日，阿波罗11号第5次载人任务，也是人类的第一次登月。 &emsp;这一系列的太空对抗引发了美国对科技教育的倾斜，美国《国防教育法》通过后，国家拨了几十亿美元，让主修数学、工程和科学的学生享受低利率的贷款，到 1968 年，国家科学基金会的预算从 10 年前的 3400 万美元增长到 5 亿美元。 通讯卫星促进了全球化，并改变了人们观看自我、观看世界的方式 &emsp;NASA 在成立 2 年后终于发射了第一颗专门用于全球通信的人造卫星“回声 1 号”，这是一个直径 30 米的大气球，表面镀铝，只能反射地面电磁信号，但没有放大和指向作用，因受到陨石撞击，不久就爆毁了。 &emsp;而美国第一颗真正有实用价值的民用通信卫星，是 NASA 在 1962 年 7 月 10 日为美国电话电报公司发射的“电星号”，造价 100 万美元，可以在美国缅因州东部的安多佛地面站和设在英国的贡希利镇和法国菲尼斯泰尔省普勒默——博多的欧洲站之间，传送多路电话通信和电视图像。 &emsp;现代互联网的雏形，源自美俄冷战时期的太空竞赛。在“害怕核战争的人们，如何在冷战期间推动了互联网的诞生”这篇文章中，有过详细的解读。简单来说，如果不是军队有发动战争的需求，互联网很可能就不会诞生。或者不会诞生在美国国防部高级研究计划局（Advanced Research Projects Agency，简称 ARPA）手中，这是一个在 1958 年建立的，旨在帮助美国在太空竞赛中赶超苏联的组织。 &emsp;如今，全球互联网的用户数已经突破 30 亿。","categories":[],"tags":[],"keywords":[]},{"title":"git workflow","slug":"2018-08-21-git_workflow","date":"2018-08-21T09:30:00.000Z","updated":"2020-01-03T09:35:02.423Z","comments":true,"path":"2018/08/21/2018-08-21-git_workflow/","link":"","permalink":"/2018/08/21/2018-08-21-git_workflow/","excerpt":"Git与SVN的比较原理上 Git直接记录文件快照，SVN每次提交记录哪些文件更新更新了哪些行 Git有本地仓库，SVN没有本地仓库 Git大多数是本地操作，SVN大多数操作需要联网 操作上 Git先提交到本地仓库然后推送到远程仓库，SVN直接推送到远程仓库 Git有各种”反悔”指令，SVN没有 Git有真正的branch，而SVN只是工作空间的副本","text":"Git与SVN的比较原理上 Git直接记录文件快照，SVN每次提交记录哪些文件更新更新了哪些行 Git有本地仓库，SVN没有本地仓库 Git大多数是本地操作，SVN大多数操作需要联网 操作上 Git先提交到本地仓库然后推送到远程仓库，SVN直接推送到远程仓库 Git有各种”反悔”指令，SVN没有 Git有真正的branch，而SVN只是工作空间的副本 Gitflow工作流Gitflow为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。 历史分支develop和master是两个常驻分支，master分支记录了正式发布的历史，而develop分支作为功能的集成分支。因此，master分支的每次提交都应分配一个版本号。 功能分支新的功能分支应该从develop分支迁出一个feature分支，新功能开发完成之后再合并回develop分支，常用命令： 开发功能agit checkout -b feature-a develop 开发完成后合并回develop分支git checkout develop git merge --no-ff feature-a git push git branch -d feature-a 发布分支 当开发完成时，直接从develop分支checkout出release分支git checkout -b release-0.1 develop release分支用于发布，发布完成后将release分支合并到master分支并且打标签，方便后续跟踪每次发布。`git checkout mastergit merge –no-ff release-0.1git push git tag -a 0.1 -m “release 0.1 publish” mastergit push –tags #### 维护分支/热修复 1. 从`master`分支拉出一个`hotfix`分支维护分支用于bug修复、快速给发布版本打补丁 git checkout -b hotfix master 2. bug修复完成立即合回`master`分支和`develop`分支，完成维护后删除`hotfix`分支 git checkout mastergit merge –no-ff hotfixgit push git checkout developgit merge –no-ff hotfixgit push git branch -d hotfix 3. `master`上打新tag git tag -a 0.2 -m “release 0.2 publish” mastergit push –tags` 优点 单个功能独立开发，并行开发互不干扰 master和develop分支分别记录发布和功能开发的历史 由于有发布分支，其他暂不发布的功能的开发不受发布的影响，可以继续提交 维护分支能快速打补丁，不影响正在开发的功能 缺点 复杂，分支繁多 Git GUI不支持，纯命令行 对开发者要求高（理解工作流，熟悉Git命令） 所有功能分支基于不稳定的develop 需要维护两个长期分支master和develop GitHub Flow 所有在master上的东西都是可发布的（已发布或马上发布） 开发新功能时，从master拉一个名称清晰的新分支 在本地提交到这个分支的同时把它push到远程仓库 当你需要得到反馈或帮助，或者该分支准备merge时，打开一个pull request 该分支被review且同意合并后，合并到master push到master后，应该立即发布 优点 操作简单 主干的代码有质量保证 缺点 测试线和正式环境的发布没有区分 Git-DevelopGit-Develop模式将develop分支作为固定的持续集成和发布分支 每一个功能都从master拉一个功能分支。 在这个功能分支上开发，功能完成到发布时，提交code review，通过后自动合并到develop。 待所有计划发布的变更分支代码都合并到develop后，rebase master到develop，完成发布。 应用发布成功后打一个tag。 develop分支的发布版本合并回master。 优点 操作相对简单 流程稍作改动，即可区分测试线和正式环境 每次开发都基于正式版本最新的代码（master)，和当前开发的其他分支不产生依赖关系 master始终是已发布状态 Pull RequestsPull requests不是一种工作流，而是一个能让开发者更方便地进行协作的功能，可以在提议的修改合并到正式项目之前对修改进行讨论。 开发者在本地仓库新建一个功能分支。 功能完成后，开发者push分支修改到远程仓库中。 开发者发起Pull requests。 团队成员收到通知，进行code review，讨论和修改。 项目维护者合并功能到仓库中并关闭Pull Requests。","categories":[{"name":"vcs","slug":"vcs","permalink":"/categories/vcs/"}],"tags":[{"name":"git","slug":"git","permalink":"/tags/git/"}],"keywords":[{"name":"vcs","slug":"vcs","permalink":"/categories/vcs/"}]},{"title":"关于TCP那些事（二）","slug":"2018-08-20-something_about_tcp2","date":"2018-08-19T17:20:00.000Z","updated":"2020-01-03T09:55:50.201Z","comments":true,"path":"2018/08/20/2018-08-20-something_about_tcp2/","link":"","permalink":"/2018/08/20/2018-08-20-something_about_tcp2/","excerpt":"TCP重传机制TCP确认机制属于累积确认，接收端给发送端的Ack确认只会确认最后一个连续的包，SeqNum和Ack是以字节数为单位，所以ack的时候，不能跳着确认，只能确认最大的连续收到的包，不然发送端就以为之前的都收到了。","text":"TCP重传机制TCP确认机制属于累积确认，接收端给发送端的Ack确认只会确认最后一个连续的包，SeqNum和Ack是以字节数为单位，所以ack的时候，不能跳着确认，只能确认最大的连续收到的包，不然发送端就以为之前的都收到了。 超时重传发送端发了1,2,3,4,5一共五份数据，接收端收到了1，2，于是回ack 3，然后收到了4（注意此时3没收到）。这种方式发送端会因为没有收到3的ACK重传，一旦收到3后会ACK4，意味着3、4都收到了。但是这种当时有很严重的问题，如果没收到3就会死等3，即使4、5都收到了发送端也不知道，因为没有收到回复。这可能到会方端悲观的认为4、5都没有收到而导致重传。 就会有两种选择: 只重传timeout的包 重传timeout后的所有的包 第一种节省带宽但是慢，第二种快但是浪费带宽，但是总体来说两种都不是最好的方式。 快速重传TCP有Fast Retransmit快速重传机制，不以时间驱动，以数据驱动重传。包没有连续到达就ACK最后那个连续的包，发送方如果连续3次收到相同的ACK就重传该包，快速重传的好处在于不用等待timeout。但是快速重传只是解决一个问题那就是timeout问题，但是依然面临这个艰难的选择，就是只重传超时的那一个包还是重传所有包。 SACK方法另一种选择是Selective Acknowledgment (SACK)，这个需要在TCP头加上一个SACK，ACK还是快速重传，SACK则是汇报收到的数据碎版。 这样就知道哪些数据收到了哪些没有收到，这样就优化了快速重传算法，但是需要两边都支持在 Linux下，可以通过tcp_sack参数打开这个功能（Linux 2.4后默认打开）。 还需要考虑一个问题： 接收方Reneging，所谓Reneging的意思就是接收方有权把已经报给发送端SACK里的数据给丢了。这样干是不被鼓励的，因为这个事会把问题复杂化了，但是，接收方这么做可能会有些极端情况，比如要把内存给别的更重要的东西。 发送方也不能完全依赖SACK，还是要依赖ACK，并维护Time-Out，如果后续的ACK没有增长，那么还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为Ack。 注意：SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。详细的东西请参看《TCP ACK的性能权衡》 Duplicate SACK – 重复收到数据的问题Duplicate SACK又称D-SACK，其主要使用了SACK来告诉发送方有哪些数据被重复接收了。 D-SACK使用了SACK的第一个段来做标志： 如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK 如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"/tags/tcp/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"关于TCP那些事（一）","slug":"2018-08-19-something_about_tcp1","date":"2018-08-19T15:20:00.000Z","updated":"2020-01-03T09:55:50.256Z","comments":true,"path":"2018/08/19/2018-08-19-something_about_tcp1/","link":"","permalink":"/2018/08/19/2018-08-19-something_about_tcp1/","excerpt":"TCP头格式 需要注意的几点： TCP包没有IP地址，那是IP层的事。只有源端口和目标端口。 一个TCP连接需要四个元组来表示是同一个连接（src_ip, src_port, dst_ip, dst_port）准确说是五元组，还有一个是协议。但因为这里只是说TCP协议，所以，这里我只说四元组。 Sequence Number是包的序号，用来解决网络包乱序（reordering）问题。 Acknowledgement Number就是ACK——用于确认收到，用来解决不丢包的问题。 Window又叫Advertised-Window，也就是著名的滑动窗口（Sliding Window），用于解决流控。 TCP Flag，包类型，用于控制TCP状态机。","text":"TCP头格式 需要注意的几点： TCP包没有IP地址，那是IP层的事。只有源端口和目标端口。 一个TCP连接需要四个元组来表示是同一个连接（src_ip, src_port, dst_ip, dst_port）准确说是五元组，还有一个是协议。但因为这里只是说TCP协议，所以，这里我只说四元组。 Sequence Number是包的序号，用来解决网络包乱序（reordering）问题。 Acknowledgement Number就是ACK——用于确认收到，用来解决不丢包的问题。 Window又叫Advertised-Window，也就是著名的滑动窗口（Sliding Window），用于解决流控。 TCP Flag，包类型，用于控制TCP状态机。 TCP的状态机TCP三次握手与四次挥手 为什么建立连接需要3次握手，断开需要4次挥手： 对于3次握手，主要是初始化Sequence Number，双方需要互通双方的初始化Sequence Number以及MSS（最大报文长度）、WSOPT（窗口比例因子）等等，这就是叫SYN而不是CONNECT原因，并且三次握手正好双方都对对方的报文进行了一次收发操作，说明双方都处于可用状态。四次或更多次其实也可以，只是没有必要因为接下来就要开始数据发送，只要有数据发送就说明连接是正常的 四次挥手，其实仔细看是2次挥手，因为TCP是全双工的，发送方和接受方都需要Fin和Ack。因为有一方是被动的，所以看上去就成了所谓的4次挥手，如果两边同时断开连接都会先进入CLOSING状态，然后到TIME_WAIT。 连接时的SYN超时，如果服务端发送SYN-ACK后Client掉线，那么服务端就会处于一个中间状态，连接既没有成功也没有失败。服务端会因为超时重传，Linux下默认重试次数为5次时间间隔从1s开始每次翻倍，所以需要2^6-1=63s才会断开这个连接。 SYN Flood攻击，可以通过只发送SYN就下线服务端就需要等待63s才会断开连接，攻击者可以很快把服务端SYN连接耗尽。 tcp_syncookies的参数来应对这个事——当SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意，请先千万别用tcp_syncookies来处理正常的大负载的连接的情况。因为，synccookies是妥协版的TCP协议，并不严谨。 tcp_synack_retries，可以通过这个参数设置SYNACK重试次数。 tcp_max_syn_backlog，通过这个参数增加SYN连接数。 tcp_abort_on_overflow，设置这个参数处理不过来就直接拒绝。 ISN的初始化，如果连接建好后始终用1来做ISN，如果client发了30个segment过去，但是网络断了，于是 client重连，又用了1做ISN，但是之前连接的那些包到了，于是就被当成了新连接的包，此时，client的Sequence Number 可能是3，而Server端认为client端的这个号是30了。RFC793中说了ISN和一个假时钟绑定，每4微妙加一直到超过2^32一个ISN周期大约需要4.55小时，一个包在网络上存活时间不会超过MSL，所以只要MSL小于4.55小时就不会重用ISN。 MSL和TIME_WAIT,为什么TIME_WAIT是2MSL？ 确保有足够的时间让对方收到ACK，如果被动关闭连接的一方没有收到ACK就会触发被动方重发FIN，一来一回最大正好是2MSL 有足够的时间让这个连接不和后面新建立的连接混在一次，如果连接被重用延迟收到的包可能会和新连接混在一起。 TIME_WAIT过多，在大量并发短连接的情况下就可能出现大量TIME_WAIT，修改tcp_tw_reuse与tcp_tw_recycle,这两个参数默认是关闭的。 tcp_tw_reuse加上tcp_timestamps可以保证协议角度上的安全，tcp_tw_reuse只对客户端有效对服务端无效，tcp_timestamps需要双方都打开，否则默写场景可能还是会有问题。 tcp_tw_recycle默认认为对端已经开启tcp_timestamps，比较时间戳变大了就可以重用。但是如果对端是NAT网或者IP被另一台电脑重用，通常服务器也是在负载后面负载会把 timestamp 都给清空，这个时候情况就复杂了。 tcp_max_tw_buckets，控制并发TIME_WAIT数默认是180000,默认超出限制会把多的destory掉然后打time wait bucket table overflow，可以通过这个参数抗DDos攻击，可以根据实际情况调整大小。 tcp_tw_reuse和tcp_tw_recycle来解决TIME_WAIT的问题是非常非常危险的，因为这两个参数违反了TCP协议","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"/tags/tcp/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"突破算法之-排列组合","slug":"2018-08-19-permutation_algorithms","date":"2018-08-18T18:00:00.000Z","updated":"2020-01-03T10:12:29.179Z","comments":true,"path":"2018/08/19/2018-08-19-permutation_algorithms/","link":"","permalink":"/2018/08/19/2018-08-19-permutation_algorithms/","excerpt":"算法原理思路是开一个数组，其下标表示1到n个数，数组元素的值为1表示其下标代表的数被选中，为0则没选中。首先初始化，将数组前m个元素置1，表示第一个组合为前m个数。然后从左到右扫描数组元素值的”10”组合，找到第一个”10”组合后将其变为“01”组合，同时将其左边的所有”1”全部移动到数组的最左端。当第一个”1”移动到数组的m-n的位置，即n个”1”全部移动到最右端时，就得到了最后一个组合。","text":"算法原理思路是开一个数组，其下标表示1到n个数，数组元素的值为1表示其下标代表的数被选中，为0则没选中。首先初始化，将数组前m个元素置1，表示第一个组合为前m个数。然后从左到右扫描数组元素值的”10”组合，找到第一个”10”组合后将其变为“01”组合，同时将其左边的所有”1”全部移动到数组的最左端。当第一个”1”移动到数组的m-n的位置，即n个”1”全部移动到最右端时，就得到了最后一个组合。 Golang实现github地址 package permulation import ( &quot;fmt&quot; &quot;math/big&quot; &quot;sort&quot; ) // permutationOfIndex n选m组合索引 func combinationNumOfIndex(n, m int) [][]int { if m &lt; 1 || m &gt; n { fmt.Println(&quot;Illegal argument. Param m must between 1 and len(nums).&quot;) return [][]int{} } // all permutation result result := make([][]int, 0, combinationNum(n, m).Int64()) // save all of index result indexS := make([]int, n) // first index array for i := 0; i &lt; n; i++ { if i &lt; m { indexS[i] = 1 } else { indexS[i] = 0 } } // 第一组组合索引 result = addTo(result, indexS) for { find := false for i := 0; i &lt; n-1; i++ { // if index i==1 and index i+1==0, // 找出第一组1,0下标, 交换后将下标左边1全部左移 if 1 == indexS[i] &amp;&amp; 0 == indexS[i+1] { find = true indexS[i], indexS[i+1] = 0, 1 if i &gt; 1 { moveOneToLeft(indexS[:i]) } result = addTo(result, indexS) break } } if !find { break } } return result } // addTo addTo func addTo(result [][]int, indexS []int) [][]int { newIndexS := make([]int, len(indexS)) copy(newIndexS, indexS) return append(result, newIndexS) } // moveOneToLeft moveOneToLeft func moveOneToLeft(indexS []int) { num := 0 for i := 0; i &lt; len(indexS); i++ { if 1 == indexS[i] { num++ } } for i := 0; i &lt; len(indexS); i++ { if i &lt; num { indexS[i] = 1 } else { indexS[i] = 0 } } } // findByIndexS findByIndexS func findByIndexS(numS []int, indexS [][]int) [][]int { if 0 == len(indexS) { return [][]int{} } result := make([][]int, len(indexS)) for i, index := range indexS { line := make([]int, 0) for j, num := range index { if 1 == num { line = append(line, numS[j]) } } result[i] = line } return result } // permutation 全排列（字典法） func permutation(nums []int) [][]int { result := make([][]int, 0) //从小到大排序 sort.Slice(nums, func(j, k int) bool { return nums[j] &lt; nums[k] }) result = addTo(result, nums) for { find := false pos1, pos2 := 0, 0 //从右向左找到第一个降序数的下标位 for j := len(nums) - 2; j &gt;= 0; j-- { if nums[j] &lt; nums[j+1] { find = true pos1 = j break } } //已经找出所有排列 if !find { break } //从右向第一下标位左找到最小的比第一个下标位的数大的数 for j := len(nums) - 1; j &gt; pos1; j-- { if nums[j] &gt;= nums[pos1] { pos2 = j break } } // 交换 nums[pos1], nums[pos2] = nums[pos2], nums[pos1] for j, k := pos1+1, len(nums)-1; j &lt; k; j, k = j+1, k-1 { nums[j], nums[k] = nums[k], nums[j] } result = addTo(result, nums) } return result } // recursionPermutation 递归加回溯全排列 func recursionPermutation(nums []int, index int) [][]int { result := make([][]int, 0) if index == len(nums)-1 { result = addTo(result, nums) return result } for i := index; i &lt; len(nums); i++ { nums[i], nums[index] = nums[index], nums[i] result = append(result, recursionPermutation(nums, index+1)...) nums[i], nums[index] = nums[index], nums[i] } return result } // permutationNum 排列数 func permutationNum(n, m int) *big.Int { return big.NewInt(1).Div(factorial(n), factorial(n-m)) } // combinationNum 组合数 func combinationNum(n, m int) *big.Int { return big.NewInt(1).Div(factorial(n), big.NewInt(1).Mul(factorial(n-m), factorial(m))) } // factorial 阶乘 func factorial(n int) *big.Int { result := big.NewInt(1) for i := 2; i &lt;= n; i++ { result = result.Mul(result, big.NewInt(int64(i))) } return result }","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"permutation","slug":"permutation","permalink":"/tags/permutation/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"突破算法之-快速排序","slug":"2018-08-1-quick_sort","date":"2018-08-18T17:30:00.000Z","updated":"2020-01-03T10:12:29.197Z","comments":true,"path":"2018/08/19/2018-08-1-quick_sort/","link":"","permalink":"/2018/08/19/2018-08-1-quick_sort/","excerpt":"算法原理快速排序是图灵奖得主C.R.A. Hoare与1960年提出的一种划分交换排序，它采用了一种分治策略。分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题的解组合为原问题的解。","text":"算法原理快速排序是图灵奖得主C.R.A. Hoare与1960年提出的一种划分交换排序，它采用了一种分治策略。分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题的解组合为原问题的解。 利用分治法可将快速排序的分为三步： 在数据集之中，选择一个元素作为”基准”（pivot） 所有小于”基准”的元素，都移到”基准”的左边；所有大于”基准”的元素，都移到”基准”的右边。这个操作称为分区 (partition) 操作，分区操作结束后，基准元素所处的位置就是最终排序后它的位置。 对”基准”左边和右边的两个子集，不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 Golang实现package main import &quot;fmt&quot; /** 1. 从数列中挑出一个元素，称为&quot;基准&quot;（pivot） 2. 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 3. 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 */ func main() { // http://jbcdn2.b0.upaiyun.com/2012/01/Visual-and-intuitive-feel-of-7-common-sorting-algorithms.gif array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} sort(array, 0, len(array)-1) fmt.Println(array) } func sort(array []int, left, right int) { if left &gt; right { return } var storeIndex = partition(array, left, right) sort(array, left, storeIndex-1) sort(array, storeIndex+1, right) } func partition(array []int, left, right int) int { // 数组分区，左小右大 var storeIndex = left // 直接选最右边的元素为基准元素 var pivot = array[right] for i := left; i &lt; right; i++ { if array[i] &lt; pivot { if i != storeIndex { array[storeIndex], array[i] = array[i], array[storeIndex] } // 交换位置后，storeIndex 自增 1，代表下一个可能要交换的位置 storeIndex++ } } // 将基准元素放置到最后的正确位置上 array[right], array[storeIndex] = array[storeIndex], array[right] return storeIndex } github地址","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"sort","slug":"sort","permalink":"/tags/sort/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"突破算法之-二分插入排序","slug":"2018-08-19-binary_insertion_sort","date":"2018-08-18T16:00:00.000Z","updated":"2020-01-03T09:35:02.409Z","comments":true,"path":"2018/08/19/2018-08-19-binary_insertion_sort/","link":"","permalink":"/2018/08/19/2018-08-19-binary_insertion_sort/","excerpt":"二分插入排序原理二分法是对直接插入的改进，由直接插入排序的循环遍历找出插入点改为二分法找出插入点，时间复杂度O(nlogn)。","text":"二分插入排序原理二分法是对直接插入的改进，由直接插入排序的循环遍历找出插入点改为二分法找出插入点，时间复杂度O(nlogn)。二分法插入排序是在插入第i个元素时，对前面的0～i-1元素进行折半，先跟他们中间的那个元素比，如果小，则对前半再进行折半，否则对后半进行折半，直到left&gt;right，然后再把第i个元素前1位与目标位置之间的所有元素后移，再把第i个元素放在目标位置上。 Golang实现package main import &quot;fmt&quot; // 二分插入排序 // 算法思想简单描述： //在插入第i个元素时，对前面的0～i-1元素进行折半，先跟他们 //中间的那个元素比，如果小，则对前半再进行折半，否则对后半 //进行折半，直到left&gt;right，然后再把第i个元素前1位与目标位置之间 //的所有元素后移，再把第i个元素放在目标位置上。 func main() { array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} var start, end, temp, mid int for i := 1; i &lt; len(array); i++ { start = 0 end = i - 1 temp = array[i] for start &lt;= end { mid = (start + end) / 2 if temp &lt; array[mid] { end = mid - 1 } else { start = mid + 1 } } //循环完后，start=end+1,此时start为当前插入数字所待坑位 //把坑位给当前插入的数据挪出来 for j := i - 1; j &gt;= start; j-- { array[j+1] = array[j] } //将当前插入数字挪入它该待的坑位 array[start] = temp } fmt.Println(array) } github地址","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"突破算法之-希尔排序","slug":"2018-08-18-shell_sort","date":"2018-08-18T12:24:00.000Z","updated":"2020-01-03T10:12:29.200Z","comments":true,"path":"2018/08/18/2018-08-18-shell_sort/","link":"","permalink":"/2018/08/18/2018-08-18-shell_sort/","excerpt":"算法要点 希尔(Shell)排序又称为缩小增量排序，它是一种插入排序。它是对直接插入排序算法的一种优化。 该方法因DL．Shell于1959年提出而得名。 算法思想它的作法不是每次一个元素挨一个元素的比较。而是初期选用大跨步（增量较大）间隔比较，使记录跳跃式接近它的排序位置；然后增量缩小；最后增量为 1 ，这样记录移动次数大大减少，提高排序效率。","text":"算法要点 希尔(Shell)排序又称为缩小增量排序，它是一种插入排序。它是对直接插入排序算法的一种优化。 该方法因DL．Shell于1959年提出而得名。 算法思想它的作法不是每次一个元素挨一个元素的比较。而是初期选用大跨步（增量较大）间隔比较，使记录跳跃式接近它的排序位置；然后增量缩小；最后增量为 1 ，这样记录移动次数大大减少，提高排序效率。 希尔排序对插入排序一下两点做出改进： 插入排序在对几乎已经排好序的数据操作时， 效率高， 即可以达到线性排序的效率 但插入排序一般来说是低效的， 因为插入排序每次只能将数据移动一位 思路： 首先去一个整数d1(d1&lt;n)作为步长，把记录分成d1个分组，所有距离为d1的倍数的记录看成一组，然后在各组组内进行插入排序 然后取d2(d2&lt;d1) 重复上述分组和插入排序操作，直到di=1(i&gt;=1)即所有记录成为一组，最后对这组进行排。一般选d1为n/2、d2=d1/2,…,di=1。 Golang实现package main import &quot;fmt&quot; /** 1 设置gap序列即增量序列，最后一次gap必须是1 2 将相距gap的一组数按照插入排序（注意 插入排序从第二个开始） 3 插入排序 增量为gap 而不是1 */ func main() { array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} n := len(array) for gap := n / 2; gap &gt; 0; gap /= 2 { for i := gap; i &lt; n; i++ { preIndex := i - gap current := array[i] for ; preIndex &gt;= 0 &amp;&amp; array[preIndex] &gt; current; { array[preIndex+gap] = array[preIndex] preIndex -= gap } array[preIndex+gap] = current } } fmt.Println(array) } github地址 算法分析算法性能 参数 结果 排序类型 插入排序 排序方法 希尔排序 平均时间复杂度 O(Nlog2N) 最坏时间复杂度 O(N1.5) 最好时间复杂度 空间复杂度 O(1) 稳定性 不稳定 复杂性 较复杂 时间复杂度步长的选择是希尔排序的关键，只要最终步长为1都可以。 希尔排序的增量序列的选择： Hibbard序列：{1,3,7……2k-1}，k为大于0的自然数，使用Hibbard序列的希尔排序平均运行时间为θ(n5/4)，最坏情形为O(n3/2)。 Sedgewick序列：令i为自然数，将94-92+1的所有结果与4-3*2+1的所有结果进行并集运算，所得数列{1,5,19,41,109……}。使用此序列的希尔排序最坏情形为O(n4/3)，平均情形为O(n7/6) 希尔排序的性能（使用Sedgewick序列）在数据量较大时依然是不错的。如果说插入排序是我们的“初级排序”，用于较少数据或趋于有序数据的情况，那么希尔排序就是我们的“中级排序”，用于数据量偏多的情况。当然，当数据量极大时，我们将用上我们的“高级排序”——快速排序。","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"sort","slug":"sort","permalink":"/tags/sort/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"突破算法之-直接插入排序","slug":"2018-08-17-insertion_sort","date":"2018-08-17T06:30:00.000Z","updated":"2020-01-03T10:12:29.148Z","comments":true,"path":"2018/08/17/2018-08-17-insertion_sort/","link":"","permalink":"/2018/08/17/2018-08-17-insertion_sort/","excerpt":"直接插入排序原理插入排序与选择排序最大的不同在于维护有序列的方式不同： 选择排序通过从无序列中找出最大或最小值放在有序列尾，直到循环到最后一个元素整个数组有序 插入排序是依次把无序列中元素从有序列尾开始比较插入第一个比它小的或者比它大的元素后面，直到循环到最后一个元素整个序列有序","text":"直接插入排序原理插入排序与选择排序最大的不同在于维护有序列的方式不同： 选择排序通过从无序列中找出最大或最小值放在有序列尾，直到循环到最后一个元素整个数组有序 插入排序是依次把无序列中元素从有序列尾开始比较插入第一个比它小的或者比它大的元素后面，直到循环到最后一个元素整个序列有序 Golang代码实现package main import &quot;fmt&quot; /** 一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 1. 从第一个元素开始，该元素可以认为已经被排序； 2. 取出下一个元素，在已经排序的元素序列中从后向前扫描； 3. 如果该元素（已排序）大于新元素，将该元素移到下一位置； 4. 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 5. 将新元素插入到该位置后； 6. 重复步骤2~5。 */ func main() { var preIndex, current int array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} for i := 1; i &lt; len(array); i++ { preIndex = i - 1 current = array[i] for preIndex &gt;= 0 &amp;&amp; array[preIndex] &gt; current { array[preIndex+1] = array[preIndex] preIndex-- } array[preIndex+1] = current } fmt.Println(array) } github地址","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"algorithms","slug":"algorithms","permalink":"/tags/algorithms/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"突破算法之-冒泡排序","slug":"2018-08-17-bubble_sort","date":"2018-08-17T05:30:00.000Z","updated":"2020-01-03T09:35:02.399Z","comments":true,"path":"2018/08/17/2018-08-17-bubble_sort/","link":"","permalink":"/2018/08/17/2018-08-17-bubble_sort/","excerpt":"冒泡排序原理对数组内所有n个元素依次进行比较和交换位置，较大的元素往上浮较小的元素往下沉，外层进行n-1次循环后数组会从小到大排好序，时间复杂度O(n^2)。","text":"冒泡排序原理对数组内所有n个元素依次进行比较和交换位置，较大的元素往上浮较小的元素往下沉，外层进行n-1次循环后数组会从小到大排好序，时间复杂度O(n^2)。 Golang代码实现package main import &quot;fmt&quot; /** 1. 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 2. 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 3. 针对所有的元素重复以上的步骤，除了最后一个； 4. 重复步骤1~3，直到排序完成。 */ func main() { array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} len := len(array) for i := 0; i &lt; len-1; i++ { for j := i+1; j &lt; len; j++ { if array[i] &gt; array[j] { array[i], array[j] = array[j], array[i] } } } fmt.Println(array) } &lt;a href=”https://github.com/sjatsh/algorithms/blob/master/src/github.com/sjatsh/algorithms/sort/bubble.go&quot; target=”_blank””&gt;github地址","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"HTTP幂等性","slug":"2018-07-16-http_idempotence","date":"2018-07-15T16:00:00.000Z","updated":"2020-01-03T10:12:29.228Z","comments":true,"path":"2018/07/16/2018-07-16-http_idempotence/","link":"","permalink":"/2018/07/16/2018-07-16-http_idempotence/","excerpt":"原文 HTTP协议是一种分布式的面向资源的网络应用层协议，Web API如此流行很大程度上应归功于简单有效的HTTP协议。然而，正如简单的Java语言并不意味着高质量的Java程序，简单的HTTP协议也不意味着高质量的Web API。要想设计出高质量的Web API，还需要深入理解分布式系统及HTTP协议的特性。","text":"原文 HTTP协议是一种分布式的面向资源的网络应用层协议，Web API如此流行很大程度上应归功于简单有效的HTTP协议。然而，正如简单的Java语言并不意味着高质量的Java程序，简单的HTTP协议也不意味着高质量的Web API。要想设计出高质量的Web API，还需要深入理解分布式系统及HTTP协议的特性。 http幂等性定义http1.1规范中幂等性的定义是： Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identicalrequests is the same as for a single request. HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用。幂等性属于语义范畴，正如编译器只能帮助检查语法错误一样，HTTP规范也没有办法通过消息格式等语法手段来定义它，这可能是它不太受到重视的原因之一。但实际上，幂等性是分布式系统设计中十分重要的概念。 分布式事务 vs 幂等性为什么需要幂等性？从一个账户充钱的远程API（可以是HTTP的也可以不是HTTP的）： bool withdraw(account_id, amount) withdraw是从账号account_id中扣除amount数额的钱，扣除成功则返回true，否则返回false账户余额不变。一种可能的情况是，服务器已经成功处理请求并返回，但是由于网络等原因可能导致客户端并没有收到响应结果。如果是在网页上，一些不恰当的设计可能让用户认为上次操作失败然后刷新网页，这就会导致withdraw被调用两次，账户再次被扣钱。如图所示： 这个问题的解决方案一是采用分布式事务，通过引入支持分布式事务的中间件来保证withdraw功能的事务性。分布式事务的优点是对于调用者很简单，复杂性都交给了中间件来管理。缺点则是一方面架构太重量级，容易被绑在特定的中间件上，不利于异构系统的集成；另一方面分布式事务虽然能保证事务的ACID性质，而但却无法提供性能和可用性的保证。 另一种更轻量级的解决方案是幂等设计： int create_ticket() bool idempotent_withdraw(ticket_id, account_id, amount) create_ticket的语义是获取一个服务器端生成的唯一的处理号ticket_id，它将用于标识后续的操作。idempotent_withdraw和withdraw的区别在于关联了一个ticket_id，一个ticket_id表示的操作至多只会被处理一次，每次调用都将返回第一次调用时的处理结果。这样，idempotent_withdraw就符合幂等性了，客户端就可以放心地多次调用。 基于幂等性的解决方案中一个完整的取钱流程被分解成了两个步骤：1.调用create_ticket()获取ticket_id；2.调用idempotent_withdraw(ticket_id, account_id, amount)。虽然create_ticket不是幂等的，但在这种设计下，它对系统状态的影响可以忽略，加上idempotent_withdraw是幂等的，所以任何一步由于网络等原因失败或超时，客户端都可以重试，直到获得结果。 和分布式事务相比，幂等设计的优势在于它的轻量级，容易适应异构环境，以及性能和可用性方面。在某些性能要求比较高的应用，幂等设计往往是唯一的选择。 HTTP的幂等性HTTP GET方法用于获取资源，不应有副作用，所以是幂等的。 HTTP DELETE方法用于删除资源，有副作用，但它应该满足幂等性。比如：DELETE http://www.forum.com/article/4231 ，调用一次和N次对系统产生的副作用是相同的，即删掉id为4231的帖子；因此，调用者可以多次调用或刷新页面而不必担心引入错误。 比较容易混淆的是HTTP POST和PUT。POST和PUT的区别容易被简单地误认为”POST表示创建资源，PUT表示更新资源”；而实际上，二者均可用于创建资源，更为本质的差别是在幂等性方面。 The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identifiedby the Request-URI in the Request-Line. …… If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain anentity which describes the status of the request and refers to the new resource, and a Location header. The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existingresource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not pointto an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create theresource with that URI. POST所对应的URI并非创建的资源本身，而是资源的接收者。比如：POST http://www.forum.com/articles的语义是在 http://www.forum.com/articles 下创建一篇帖子，HTTP响应中应包含帖子的创建状态以及帖子的URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性。 而PUT所对应的URI是要创建或更新的资源本身。比如：PUT http://www.forum/articles/4231 的语义是创建或更新ID为4231的帖子。对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性。","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"http","slug":"http","permalink":"/tags/http/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"Java 内存可见性与指令重排","slug":"2018-04-20-java_volatile","date":"2018-04-19T16:00:00.000Z","updated":"2020-01-03T10:12:29.143Z","comments":true,"path":"2018/04/20/2018-04-20-java_volatile/","link":"","permalink":"/2018/04/20/2018-04-20-java_volatile/","excerpt":"内存可见性的问题多线程系统中共享变量在主存，线程保存一份变量的副本，某个线程对共享变量的改动通过改动变量副本然后同步到主存中的共享变量，但是其他线程保存的可能还是之前的变量副本，此时可能就会存在问题。","text":"内存可见性的问题多线程系统中共享变量在主存，线程保存一份变量的副本，某个线程对共享变量的改动通过改动变量副本然后同步到主存中的共享变量，但是其他线程保存的可能还是之前的变量副本，此时可能就会存在问题。 全局变量open： boolean open=true; 线程A经过一些操作之后把open设置成false： //线程A open = false; resource.close(); 此时线程B通过判断open状态进行资源访问： //线程B while(open) { doSomethingWithResource(resource); } 此时将open置为false后关闭资源，但是此时open变量可能并没有同步到线程B，此时open对线程B不可见。但是实际资源已经被关闭，此时再对已关闭的资源进行操作就会产生问题。 volatile提供内存可见性volatile可见性原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile关键字的作用之一就是保证变量修改的实时可见性。 指令重排问题指令重排导致单例模式失效： public class Singleton { private static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if(instance == null) { synchronzied(Singleton.class) { if(instance == null) { instance = new Singleton(); //非原子操作 } } } return instance; } } 其中instance= new Singleton()并不是原子操作，实际上会被抽象成几个JVM指令： memory = allocate(); //1：分配内存空间 ctorInstance(memory); //2：初始化对象 instance = memory; //3：instance引用指向内存空间 其中1、2，1、3相互依赖，但是2、3并不是相互依赖，JVM可以对他们进行指令优化重排： memory = allocate(); //1：分配内存空间 instance = memory; //2：instance引用指向内存空间 ctorInstance(memory); //3：初始化对象 由于instance引用在第二步已经指向内存所以不为空，但是还没有对对象进行初始化操作，这时getInstance判断instance不为空拿去使用就会导致出错。 内存屏障内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种CPU指令，用于控制特定条件下的重排序和内存可见性问题。Java编译器也会根据内存屏障的规则禁止重排序。内存屏障可以被分为以下几种类型 LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。 为了保证final字段的特殊语义，也会在下面的语句加入内存屏障：x.finalField = v; StoreStore; sharedRef = x; final域重排序在写final域的时候有两个规则： JVM禁止编译器把final域的写重排序到构造函数之外 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障，这个屏障禁止处理器把final域的写重排序到构造函数之外。 写final域的重排序规则可以确保:在对象引用为任意线程可见之前,对象的final域已经被正确初始化过了。","categories":[{"name":"java","slug":"java","permalink":"/categories/java/"}],"tags":[{"name":"volatile","slug":"volatile","permalink":"/tags/volatile/"}],"keywords":[{"name":"java","slug":"java","permalink":"/categories/java/"}]},{"title":"mysql8.0.11精简版","slug":"2018-04-20-mysql_8011","date":"2018-04-19T16:00:00.000Z","updated":"2020-01-03T10:12:29.182Z","comments":true,"path":"2018/04/20/2018-04-20-mysql_8011/","link":"","permalink":"/2018/04/20/2018-04-20-mysql_8011/","excerpt":"","text":"mysql8.0.11精简版仅供学习交流：百度网盘地址 密码：8g3h","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"mysql bin/redo log顺序一致性问题","slug":"2018-04-13-mysql_tow_phase_commit","date":"2018-04-12T16:00:00.000Z","updated":"2020-01-03T10:08:01.049Z","comments":true,"path":"2018/04/13/2018-04-13-mysql_tow_phase_commit/","link":"","permalink":"/2018/04/13/2018-04-13-mysql_tow_phase_commit/","excerpt":"为什么mysql既有bin log也有redo logbin log是属于mysql server层的log，主要是用来做主从复制和即时点恢复时使用的，redo log是InnoDB存储引擎层的，用来保证事务安全。不管用哪种存储引擎都有bin log，而redo log只有InnoDB有，这是由mysql体系架构决定的。","text":"为什么mysql既有bin log也有redo logbin log是属于mysql server层的log，主要是用来做主从复制和即时点恢复时使用的，redo log是InnoDB存储引擎层的，用来保证事务安全。不管用哪种存储引擎都有bin log，而redo log只有InnoDB有，这是由mysql体系架构决定的。 bin logmysql5.1之后有bin log，主要用于主备复制。 未开启bin log未开启bin log情况下，InnoDB通过redo log、undo log进行数据的回复和事务回滚，以此保证CrashSafe。可以通过innodb_flush_log_at_trx_commit来控制何时写入redo log，用户可以根据需求来自行调整。innodb_flush_log_at_trx_commit = 0|1|2 0：每N秒将Redo Log Buffer的记录写入Redo Log文件，并且将文件刷入硬件存储1次。N由innodb_flush_log_at_timeout控制。 1：每个事务提交时，将记录从Redo Log Buffer写入Redo Log文件，并且将文件刷入硬件存储。 2：每个事务提交时，仅将记录从Redo Log Buffer写入Redo Log文件。Redo Log何时刷入硬件存储由操作系统和innodb_flush_log_at_timeout决定。这个选项可以保证在MySQL宕机，而操作系统正常工作时，数据的完整性。 crash recovery时，已经在InnoDB内部提交的事务用redo log恢复，所有prepare但是没有commit的transactions用undo log做rollback。 开启bin log在开启bin log的情况下，要保证主备数据一致性就必须保证bin log和redo log日志的一致性，mysql引入两阶段提交（two phase commit or 2pc）。mysql内部会将一个普通的事物作为一个XA事物（分布式事物）来处理。 为每一个事务分配一个XID commit分为prepare和commit两个阶段 bin log作为事务的协调者(Transaction Coordinator)，bin log event作为协调者日志 prepare阶段：sql成功执行，生成xid、redo log、undo log。调用prepare将事务状态设为TRX_PREPARED，并将redo log、undo log刷磁盘 commit阶段： 记录bin log，write()&amp;fsync()，只要fsync()成功事务就一定要提交了，此时生成Xid_log_event，失败调用ha_rollback_trans回滚。 InnoDB commit完成事务提交，清除undo log、刷redo log日志，设置事务状态为TRX_NOT_STARTED 两阶段提交通过innodb_support_xa控制，默认true开启。为了保证CrashSafe，上面fsync可以通过sync_binlog=1&amp;innodb_flush_log_at_trx_commit=1保证。 innodb_flush_log_at_trx_commit(redo log) 0：每秒一次写入log file，并且fsync。 1：每次事务提交写入log file并fsync（默认）。 2：每次事务提交写入log file不fsync，何时fsync由操作系统决定。 sync_binlog(binlog) 0：何时fsync由OS决定。 N：N次事务提交后fsync。 crash recovery开启bin log的mysql在crash recovery时找出所有prepare阶段生成的xid并找出对应的Xid_log_event Xid_log_event存在，事务提交 Xid_log_event不存在，事务回滚 从上面可以看出，只要是bin log fsync()刷盘成功生成Xid_log_event此时事务必定commit成功。 bin log、redo log顺序一致两阶段提交只能保证bin log、redo log数据的一致性，但是却不能保证bin log与redo log的存储顺序以直性。但是在并发场景下却无法保证所有事物在bin log、redo log中提交顺序的一致性。当我们使用xtrabackup或ibbackup进行数据备份时就可能造成数据的不一致，如图： 5.6之前通过prepare_commit_mutex来保证XA事务的顺序一致性，每次只有一个XA事务能够获得该锁。 上图所示MySQL开启Binary log时使用prepare_commit_mutex保证二进制日志和存储引擎顺序保持一致，prepare_commit_mutex的锁机制造成高并发提交事务的时候性能非常差而且bin log也无法group commit。 mysql5.6之后引入三阶段提交，移除prepare_commit_mutex，并且bin log&amp;redo log都是group commit。MySQL组提交","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"mysql undo log与MVCC的实现原理","slug":"2018-04-12-mysql_undo_log_mvcc","date":"2018-04-11T16:00:00.000Z","updated":"2020-01-03T10:08:01.056Z","comments":true,"path":"2018/04/12/2018-04-12-mysql_undo_log_mvcc/","link":"","permalink":"/2018/04/12/2018-04-12-mysql_undo_log_mvcc/","excerpt":"###mysql架构 概念上可以分为四层： 接入层：不同客户端通过mysql协议与mysql连接通讯，接入层进行权限验证、连接池管理线程管理等。 服务层：sql解析器、优化器、数据缓冲、缓存等。 存储引擎：常用的MyISAM、InnoDB等存储引擎。 文件系统：保存数据、索引、日志等。","text":"###mysql架构 概念上可以分为四层： 接入层：不同客户端通过mysql协议与mysql连接通讯，接入层进行权限验证、连接池管理线程管理等。 服务层：sql解析器、优化器、数据缓冲、缓存等。 存储引擎：常用的MyISAM、InnoDB等存储引擎。 文件系统：保存数据、索引、日志等。 MVCCMVCC(Multi Version Concurrency Control)多版本并发控制。InnoDB可重复读隔离级别中聚集索引叶子节点中保存了回滚指针，指向修改前的存放在undo log中的数据，InnoDB也正是通过回滚指针和undo log实现MVCC和事务回滚。 undo log中的内容只是串行化的结果，记录了多个事务的过程，不属于多版本共存。但理想的MVCC是难以实现的，当事务仅修改一行记录使用理想的MVCC模式是没有问题的，可以通过比较版本号进行回滚，但当事务影响到多行数据时，理想的MVCC就无能为力了。 问题：transaction1修改row1、row2，row1修改失败row2修改成功，此时事务需要回滚row1、row2不需要回滚，此时row1上是没有锁的并且可能被transaction2修改，但是我们现在回滚row1的内容就可能造成破坏transaction2的结果。 思考：理想MVCC难以实现的根本原因在于企图通过乐观锁代替二段提交。修改两行数据，但为了保证其一致性，与修改两个分布式系统中的数据并无区别，而二提交是目前这种场景保证一致性的唯一手段。 二段提交的本质是锁定 乐观锁的本质是消除锁定，本质上存在矛盾 理想的MVCC难以实现，Innodb只是借了MVCC这个名字，提供了非阻塞读 redo/undo/bin log redo log：内存中的修改操作都会记录在redo log中，当内存和磁盘中的数据不一致或事物提交时会进行一次flush操作。 undo log：记录数据的反向操作，用于操作的撤回和数据的恢复，通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本，实现MVCC。 bin log：binlog是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的, 另外通过解析binlog能够实现mysql到其他数据源（如ElasticSearch)的数据复制。 redo/bin log一致性保证为了防止写完bin log但是redo log的事务没提交从而导致的不一致，innodb 使用了两阶段提交，其基本逻辑就是上锁： InnoDB prepare write/sync redo/undo log (prepare_commit_mutex mysql5.6前) write/sync Binlog InnoDB commit (写入COMMIT标记后释放prepare_commit_mutex)","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"mysql clustered index","slug":"2018-04-12 -mysql_clustered_index","date":"2018-04-11T16:00:00.000Z","updated":"2020-01-03T10:12:29.237Z","comments":true,"path":"2018/04/12/2018-04-12 -mysql_clustered_index/","link":"","permalink":"/2018/04/12/2018-04-12 -mysql_clustered_index/","excerpt":"mysql聚集索引的理解InnoDB使用聚集索引存储数据，到底什么是聚集索引，聚集索引直接定义了表中数据的真实的物理存储顺序。聚集索引就像一个电话簿，按照姓氏排序，并且电话号码紧跟着后面。一个表有且只有一个聚集索引，默认主键作为聚集索引，没有主键则以第一个唯一非空键作为聚集索引，如果也没有则InnoDB会生成一个隐藏id作为聚集索引。","text":"mysql聚集索引的理解InnoDB使用聚集索引存储数据，到底什么是聚集索引，聚集索引直接定义了表中数据的真实的物理存储顺序。聚集索引就像一个电话簿，按照姓氏排序，并且电话号码紧跟着后面。一个表有且只有一个聚集索引，默认主键作为聚集索引，没有主键则以第一个唯一非空键作为聚集索引，如果也没有则InnoDB会生成一个隐藏id作为聚集索引。 注：聚集索引中的每个叶子节点包含主键值、事务ID、回滚指针(rollback pointer用于事务和MVCC)和余下的列 聚集索引的优缺点优点： 通过主键查询和区间查询非常快，聚集索引能够快速定位区间的开始位置与结束位置。 如果某个字段经常用来排序并且该字段就是聚集索引，那么排序时间就是数据查询时间。 缺点： 聚集索引被定义的很大，辅助索引也会变得很大，因为辅助索引都存储了聚集索引的key。 聚集索引被个更改会涉及聚集索引的重排，聚集索引的更改代价很高。 插入可能比较慢，如果数据不是按照主键的顺序插入。 辅助索引InnoDB不存在辅助索引的更新，辅助索引的更新意味着先删除再添加。 聚集索引与辅助索引的区别聚集索引既存储了索引，也存储了行值。辅助索引存储的是主键值和索引key，辅助索引一般包含两次查找，一次查找索引自身，一次查找主键（聚集索引）获取数据行。","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"mysql InnoDB locks","slug":"2018-04-11-mysql_row_lock","date":"2018-04-10T16:00:00.000Z","updated":"2020-01-03T10:04:00.490Z","comments":true,"path":"2018/04/11/2018-04-11-mysql_row_lock/","link":"","permalink":"/2018/04/11/2018-04-11-mysql_row_lock/","excerpt":"###InnoDB的锁机制 数据库锁为了支持更好的并发，提供数据的一致性和完整性。锁的类型有：共享锁（S）、排他锁（X）、意向共享（IS）、意向排他（IX）。为了更好的并发，InnoDB通过MVCC提供非锁定读：不等待行上的锁释放，读取行的一个快照。","text":"###InnoDB的锁机制 数据库锁为了支持更好的并发，提供数据的一致性和完整性。锁的类型有：共享锁（S）、排他锁（X）、意向共享（IS）、意向排他（IX）。为了更好的并发，InnoDB通过MVCC提供非锁定读：不等待行上的锁释放，读取行的一个快照。 InnoDB支持的锁定方式 Record Lock：记录锁，锁直接加在索引记录上面，锁住的是key而不是记录值。 Gap Lock：间隙锁，锁的一个范围，不包括记录本身，防止其他事务的插入操作，以此防止幻读的发生。 Next-Key Lock：锁定一个范围，并且锁定记录本身。 锁定方式的选择 如果条件没有走索引，会进行全表扫描，所以上升为表锁。 如果条件为索引字段，但是并非唯一索引（包括主键索引），那么此时会使用Next-Key Lock，为什么用Next-Key Lock： 符合条件的记录上加上排他锁，会锁定当前非唯一索引和对应的主键索引的值 保证锁定的区间不能插入新的数据 如果条件为唯一索引，使用Record Lock","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"mysql transaction isolation","slug":"2018-04-09-mysql_transaction_isolation","date":"2018-04-08T16:00:00.000Z","updated":"2020-01-03T09:35:02.406Z","comments":true,"path":"2018/04/09/2018-04-09-mysql_transaction_isolation/","link":"","permalink":"/2018/04/09/2018-04-09-mysql_transaction_isolation/","excerpt":"###何为隔离级别 隔离级别决定事务间的可见程度，一个事务的修改结果在什么时候能被其他事务看到，由此SQL1992规范对隔离性定义了不同的隔离级别用来划分这个可见范围。","text":"###何为隔离级别 隔离级别决定事务间的可见程度，一个事务的修改结果在什么时候能被其他事务看到，由此SQL1992规范对隔离性定义了不同的隔离级别用来划分这个可见范围。 mysql四种事务隔离级别SQL1992中定义的四种事务隔离级别：READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ、SERIALIZABLE，mysql InnoDB默认REPEATABLE READ，mysql可以用transaction-isolation或者在配置文件中可以为所有连接设置隔离搞我winiseI my.ini设置： transaction-isolation = {READ-UNCOMMITTED | READ-COMMITTED | REPEATABLE-READ | SERIALIZABLE} SET TRANSACTION设置： SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE} global设置全局事务隔离级别，session设置session 查询全局或者当前session隔离级别： SELECT @@global.tx_isolation; SELECT @@session.tx_isolation; SELECT @@tx_isolation; 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 隔离级别： 未提交读：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 已提交读：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 可串行化：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 现象/结果： 脏读：当前事务可以读取和使用其他事务中未提交的数据。 不可重复读：一个事务中由于另一个事务对同一数据造成修改，从而导致事务内两次读取到的数据不一致。 幻读：幻读可能的情况很多 第一个事务中在插入数据前先查询发现数据不存在，接下来准备执行insert，此时事务二中执行相同操作并提交。此时事务一中insert时就会报错：数据已存在。就好像产生了幻觉一样明明查询的时候没有该数据，但是insert的时候却报错已存在。 或者第一个事务对全表执行update操作，此时事务二insert了一条数据，事务一用户之后就会发现表中还会有没有更新的数据，好像产生幻觉。 当事务隔离级别是可重复读的时候，并且innodb_locks_unsafe_for_binlog=off（默认），搜索index并加行锁的时候使用next-key locks可以有效避免幻读。但这里需要注意加行锁字段必须是索引字段，否则会表锁。","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"gateway framework","slug":"2018-04-07-gateway_framework","date":"2018-04-06T16:00:00.000Z","updated":"2020-01-03T09:35:02.378Z","comments":true,"path":"2018/04/07/2018-04-07-gateway_framework/","link":"","permalink":"/2018/04/07/2018-04-07-gateway_framework/","excerpt":"gateway framework","text":"gateway framework CDN：CDN是将源站内容分发至最接近用户的节点，使用户可就近取得所需内容，提高用户访问的响应速度和成功率。 高防：把域名解析到高防IP（Web业务把域名解析指向高防IP；非Web业务，把业务IP替换成高防IP），并配置源站IP。所有公网流量都经过高防IP机房，通过端口协议转发的方式将访问流量通过高防IP转发到源站IP，同时将恶意攻击流量在高防IP上进行清洗过滤后将正常流量返回给源站IP，从而确保源站IP稳定访问。抗DDOS必须硬件上抗住，一般服务器被DDOS会默认当作客户责任从而suspend掉客户的机器，但是高防服务器，机房会用硬件防火墙或者大带宽帮你抗住。 WAF：传统防火墙无法对应用层的攻击进行有效抵抗，并且IPS也无法从根本上防护应用层的攻击。因此出现了保护Web应用安全的Web应用防火墙系统。WAF是一种基础的安全保护模块，通过特征提取、分块检索进行特征匹配，或者配置黑白名单，主要针对HTTP访问的Web程序保护。 网关集群一般通过nginx+lua对请求解析并将请求代理负载到后端应用服务器或者ULB。","categories":[{"name":"framework","slug":"framework","permalink":"/categories/framework/"}],"tags":[],"keywords":[{"name":"framework","slug":"framework","permalink":"/categories/framework/"}]},{"title":"hsts preload","slug":"2018-04-07-hsts_preload_list","date":"2018-04-06T16:00:00.000Z","updated":"2020-01-03T10:12:29.140Z","comments":true,"path":"2018/04/07/2018-04-07-hsts_preload_list/","link":"","permalink":"/2018/04/07/2018-04-07-hsts_preload_list/","excerpt":"服务器https跳转的问题一般我们的站点强制跳转https都是通过nginx、caddy进行http的强制https跳转。这个过程就可能存在问题，我们在浏览器直接输入域名example.com的时候发起的是一个明文的http请求很容易被攻击者拦截并定向到钓鱼网站","text":"服务器https跳转的问题一般我们的站点强制跳转https都是通过nginx、caddy进行http的强制https跳转。这个过程就可能存在问题，我们在浏览器直接输入域名example.com的时候发起的是一个明文的http请求很容易被攻击者拦截并定向到钓鱼网站，并且如果使用的是国内的vps时很可能要求域名必须备案，没有备案直接进行http访问的时候，会直接跳转到一个提示页面很是恶心~ hsts既然建立HTTPS连接之前的这一次HTTP明文请求和重定向有可能被攻击者劫持，那么解决这一问题的思路自然就变成了如何避免出现这样的HTTP请求。我们期望的浏览器行为是，当用户让浏览器发起HTTP请求的时候，浏览器将其转换为HTTPS请求，直接略过上述的HTTP请求和重定向，从而使得中间人攻击失效，规避风险。HSTS最为核心的是一个HTTP响应头（HTTP Response Header）。正是它可以让浏览器得知，在接下来的一段时间内，当前域名只能通过HTTPS进行访问，并且在浏览器发现当前连接不安全的情况下，强制拒绝用户的后续访问要求。HSTS Header的语法:Strict-Transport-Security: &lt;max-age=&gt;[; includeSubDomains][; preload] max-age是必选参数，是一个以秒为单位的数值，它代表着HSTS Header的过期时间，通常设置为1年，即31536000秒。 includeSubDomains是可选参数，如果包含它，则意味着当前域名及其子域名均开启HSTS保护。 preload是可选参数，只有当你申请将自己的域名加入到浏览器内置列表的时候才需要使用到它。关于浏览器内置列表，下文有详细介绍。 hsts preload listHSTS存在一个比较薄弱的环节，那就是浏览器没有当前网站的HSTS信息的时候，或者第一次访问网站的时候，依然需要一次明文的HTTP请求和重定向才能切换到HTTPS，以及刷新HSTS信息。而就是这么一瞬间却给攻击者留下了可乘之机，使得他们可以把这一次的HTTP请求劫持下来，继续中间人攻击。 加入hsts preload listhttps://hstspreload.org/设置好hsts响应头，并且证书有效可以在这里提交加入hsts preload list，这样就可以让浏览直接强制访问https而不需要在这之前发起一次http请求。同样也避免了国内vps要求域名备案的问题（当然能备案还是备案一下，就目前知道.me已经不能备案了）。","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"https","slug":"https","permalink":"/tags/https/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"clion wsl","slug":"2018-04-06-clion_wsl","date":"2018-04-05T16:00:00.000Z","updated":"2020-01-03T09:35:02.454Z","comments":true,"path":"2018/04/06/2018-04-06-clion_wsl/","link":"","permalink":"/2018/04/06/2018-04-06-clion_wsl/","excerpt":"clion wslclion新版开始支持wsl，这样就可以很方便的直接在windows上编译linux特性的C代码。 Bash on Windows 在控制面板\\程序\\程序和功能 启用或关闭 Windows 功能中勾选 适用于 Linux 的 Windows 子系统 在设置中打开开发人员模式 重启后powershell输入","text":"clion wslclion新版开始支持wsl，这样就可以很方便的直接在windows上编译linux特性的C代码。 Bash on Windows 在控制面板\\程序\\程序和功能 启用或关闭 Windows 功能中勾选 适用于 Linux 的 Windows 子系统 在设置中打开开发人员模式 重启后powershell输入 lxrun /install /y 命令行直接输入bash就可以启动了 ubuntu on windowsmicrosoft store 直接安装ubuntu子系统 重装openssh sudo apt-get remove openssh-server sudo apt-get install openssh-server 修改sshd设置vim /etc/ssh/sshd_config UsePrivilegeSeparation no PasswordAuthentication yes ListenAddress 0.0.0.0 重启sshd sudo service ssh --full-restart 可能的问题sshd ../sysdeps/posix/getaddrinfo.c:2603: getaddrinfo: Assertion IN6_IS_ADDR_V4MAPPED (sin6-&gt;sin6_addr.s6_addr32) failed. 原因ipv6的问题，修改sshd_config，ListenAddress 0.0.0.0 sshd fail because getaddrinfo UsePrivilegeSeparation no 自启动ssh一旦bash进程被关闭，ssh就无法连接： 添加启动项，让bash开机自启 使用sudo service ssh start启动需要输入密码所以需要visudo免密码操作 具体操作： 使用vbs启动bash并运行sudo service ssh start set ws=wscript.createobject(&quot;wscript.shell&quot;) ws.run &quot;C:\\Windows\\System32\\bash.exe&quot;,0 ws.run &quot;C:\\Windows\\System32\\bash.exe -c &#39;sudo /usr/sbin/service ssh start&#39;&quot;,0 其他启动项类似也可以加在脚本后面，把vbs文件放入如下目录： %AppData%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup 运行sudo visudo,添加配置username ALL = (root) NOPASSWD: /usr/sbin/service 愉快的写linux c吧 #include&lt;stdio.h&gt; #include&lt;pthread.h&gt; #include&lt;string.h&gt; #include&lt;sys/types.h&gt; #include&lt;unistd.h&gt; pthread_t main_tid; void print_ids(const char *str) { pid_t pid; //进程id pthread_t tid; //线程id pid = getpid(); //获取当前进程id tid = pthread_self(); //获取当前线程id printf(&quot;%s pid: %u tid: %u (0x%x)\\n&quot;, str, (unsigned int) pid, (unsigned int) tid, (unsigned int) tid); } void *func(void *arg) { print_ids(&quot;new thread:&quot;); return ((void *) 0); } int main() { int err; err = pthread_create(&amp;main_tid, NULL, func, NULL); //创建线程 if (err != 0) { printf(&quot;create thread error: %s/n&quot;, strerror(err)); return 1; } printf(&quot;main thread: pid: %u tid: %u (0x%x)\\n&quot;, (unsigned int) getpid(), (unsigned int) pthread_self(), (unsigned int) pthread_self()); print_ids(&quot;main thread:&quot;); sleep(1); return 0; } CMakeList.txt最后加上 find_package(Threads) target_link_libraries(projectName ${CMAKE_THREAD_LIBS_INIT})","categories":[{"name":"ide","slug":"ide","permalink":"/categories/ide/"}],"tags":[],"keywords":[{"name":"ide","slug":"ide","permalink":"/categories/ide/"}]},{"title":"why codis not redis cluster","slug":"2018-04-04-choose_codis_not_redis_cluster","date":"2018-04-03T16:00:00.000Z","updated":"2020-01-03T09:35:02.357Z","comments":true,"path":"2018/04/04/2018-04-04-choose_codis_not_redis_cluster/","link":"","permalink":"/2018/04/04/2018-04-04-choose_codis_not_redis_cluster/","excerpt":"why not redis cluster 无中心的设计，很难把程序写对 代码有点吓人，clusterProcessPacket有426行，大脑很难处理到所有的状态切换 迟迟没有正式版，等了4年之久 缺乏Best Practise，还没人写一个Redis Cluster的若干条注意事项 这个系统高度耦合，升级困难 如果slot所在的redis master、slave同时挂掉，整个集群不可用。codis是否一样?待验证…","text":"why not redis cluster 无中心的设计，很难把程序写对 代码有点吓人，clusterProcessPacket有426行，大脑很难处理到所有的状态切换 迟迟没有正式版，等了4年之久 缺乏Best Practise，还没人写一个Redis Cluster的若干条注意事项 这个系统高度耦合，升级困难 如果slot所在的redis master、slave同时挂掉，整个集群不可用。codis是否一样?待验证… why codiscodis改进redis的几个点: redis 数据量太大的话（22G 以上），处理性能就开始下降 无法区分冷热数据，内存浪费严重 RDB Block 住整个服务 写操作太频繁，AOF刷盘太多，很容易rewrite https://segmentfault.com/a/1190000012908434","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[],"keywords":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}]},{"title":"redis sentinel/cluster","slug":"2018-04-04-redis_sentinel_cluster","date":"2018-04-03T16:00:00.000Z","updated":"2020-01-03T10:12:29.195Z","comments":true,"path":"2018/04/04/2018-04-04-redis_sentinel_cluster/","link":"","permalink":"/2018/04/04/2018-04-04-redis_sentinel_cluster/","excerpt":"单例模式redis单例存在的问题： 单点故障 数据备份 数据量大了以后的性能问题","text":"单例模式redis单例存在的问题： 单点故障 数据备份 数据量大了以后的性能问题 主从模式主从模式指的是使用一个redis实例作为master，其余的实例作为slave,主机和从机的数据完全一致，主机支持数据的写入和读取等各项操作，而从机则只支持与主机数据的同步和读取（也可以设置可写，但是会在master同步后丢失），也就是说，客户端可以将数据写入到主机，由主机自动将数据的写入操作同步到从机。主从模式很好的解决了数据备份问题，并且由于主从服务数据几乎是一致的，因而可以将写入数据的命令发送给主机执行，而读取数据的命令发送给不同的从机执行，从而达到读写分离的目的。如下所示主机redis-A分别有redis-B、redis-C、redis-D、redis-E四个从机： 注意： 主从模式主要是为了解决上面后两个问题而提供的一种解决方案，但是依然存在单点故障的问题。 master down掉会导致slave也不可用，可手动选择一个slave升格为master，使系统能够继续提供服务。然而整个过程很麻烦需要人工介入，难以实现自动化。 sentinel模式为了解决主从模式master down掉会导致slave也不可用，需要人工接入的问题，redis2.8提供sentinel工具，实现自动化的系统监控和故障恢复功能sentinel的作用就是监控Redis系统的运行状况： 监控主数据库和从数据库是否正常运行。 主数据库出现故障时自动将从数据库转换为主数据库。 可以用info replication查看主从情况 注意： sentinel 1 很多 Bug，被官方弃用，用 redis2.8 以及 sentinel 2 主从切换配置要加上 sentinel can-failover server1 yes cluster模式及时使用sentinel每个redis实例也是全量存储，浪费内存并且会有木桶效应。集群至少需要3主3从修改每个实例的配置文件： cluster-enabled yes cluster-config-file nodes-xx.conf 使用redis自带的集群工具创建集群： 安装rubygems：yum install ruby 、yum install rubygems 、gem install redis 分别启动6个redis实例 redis-trib.rb create –replicas 1 127.0.0.1:6379 … 其中replicas 1表示给每个master分配多少个slave 总结sentinel 主从切换自动化 本身也支持集群，用sentinel集群来监控redis集群 自身也需要多数，避免单点问题 只是一个运行在特殊模式下的 Redis实例 可以通过发布与订阅来自动发现正在监视相同主实例的其他Sentinel failover 过程对代码客户端是透明的 cluster 一个redis集群包含16384个哈希槽（hash slot） redis集群对节点使用了主从复制功能，提高稳定性（HA） redis集群的节点间通过gossip协议通信 要点 主从模式是为了HA sentinel是一种自动failover和高可用（HA）解决方案 cluster是一种分片的方案，自带failover，主要解决的是在线动态横向扩容","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"/tags/redis/"}],"keywords":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}]},{"title":"值传递和引用传递","slug":"2018-04-03-pass_by_value_and_reference","date":"2018-04-02T16:00:00.000Z","updated":"2020-01-03T10:12:29.234Z","comments":true,"path":"2018/04/03/2018-04-03-pass_by_value_and_reference/","link":"","permalink":"/2018/04/03/2018-04-03-pass_by_value_and_reference/","excerpt":"到底什么是值传递和引用传递基础中的基础，首先不要纠结字面意思，Java中（byte、short、int、long、float、double）、字符型（char）、布尔型（boolean）这些基本类型数据直接保存在栈中，而类、接口、数组数据是保存在堆中，栈只是保存一个指向堆内存的指针。","text":"到底什么是值传递和引用传递基础中的基础，首先不要纠结字面意思，Java中（byte、short、int、long、float、double）、字符型（char）、布尔型（boolean）这些基本类型数据直接保存在栈中，而类、接口、数组数据是保存在堆中，栈只是保存一个指向堆内存的指针。 值传递和引用传递的本质就是传递值还是内存地址","categories":[{"name":"java","slug":"java","permalink":"/categories/java/"}],"tags":[{"name":"transmit","slug":"transmit","permalink":"/tags/transmit/"}],"keywords":[{"name":"java","slug":"java","permalink":"/categories/java/"}]},{"title":"B-Tree和B+Tree","slug":"2018-04-02-b-tree-b++tree","date":"2018-04-01T16:00:00.000Z","updated":"2020-01-03T09:55:50.153Z","comments":true,"path":"2018/04/02/2018-04-02-b-tree-b++tree/","link":"","permalink":"/2018/04/02/2018-04-02-b-tree-b++tree/","excerpt":"大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，为什么B-Tree和B+Tree在被如此广泛用于索引？先从数据结构的角度来分析。 几种常见的树结构二叉查找树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），红黑树(Red-Black Tree )，B-tree/B+-tree/ B*-tree (B~Tree)。前三都是二叉查找树，查找时间复杂度O(log2N)与树的深度有关，降低深度自然也能提高查询效率。","text":"大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，为什么B-Tree和B+Tree在被如此广泛用于索引？先从数据结构的角度来分析。 几种常见的树结构二叉查找树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），红黑树(Red-Black Tree )，B-tree/B+-tree/ B*-tree (B~Tree)。前三都是二叉查找树，查找时间复杂度O(log2N)与树的深度有关，降低深度自然也能提高查询效率。 面临的问题实际的大规模存储中，在实现索引查询的背景下，树能存储的数据量是有限的，大数据量情况下导致树过深造成磁盘I/O读写过于频繁（磁盘构造有关），导致查询效率底下。那么减小树的深度就成为有效的提高查询效率的手段。那么很自然一个想法就是多叉树结构。 磁盘的构造 磁盘上数据通过一个三维地址唯一标示：柱面号、盘面号、块号(磁道上的盘块)。 磁盘的读写需要经过三个步骤： 定位（查找）：移动臂根据柱面号使磁头移动到所需要的柱面 确定盘面：这时根据盘面号来确定指定盘面上的磁道 确定块号：将指定块号的磁道段移动至磁头下 访问具体信息由三步时间组成： 查找时间(seek time) Ts：完成上面步骤一所需时间，代价最高，最大到0.1s左右。 等待时间(latency time) Tl：上述三步耗时。 传输时间(transmission time) Tt: 数据通过系统总线传送到内存的时间。 B-Tree、B+Tree为什么适合做索引 B-Tree、B+Tree是多路搜索树，查询任意一个节点，IO次数是恒定的，最多需要访问h个节点（h为树高）。 mysql巧妙地将一个节点的大小设置了一页的大小，每次磁盘读取一页的数据就能在一次IO 的情况下将整个节点的数据读出来。 mysql B-Tree一般高度不超过3，100阶的高度为3的B-Tree，可以存100100100=10^6的数据，基本上3次IO就可以完成索引。 为什么不用红黑树因为红黑树是二叉树，存储大量数据的时候树的高度会非常大，会造成多次IO读取，性能很差。如上所述10^6的数据在红黑树中就不可能在3次IO内完成，因此红黑树不适合作为外存索引。 总结由此可以看出提高索引查询效率的关键在于减少磁盘的定位和IO次数，而减少磁盘IO关键在于降低树的高度。","categories":[{"name":"struct","slug":"struct","permalink":"/categories/struct/"}],"tags":[{"name":"struct","slug":"struct","permalink":"/tags/struct/"}],"keywords":[{"name":"struct","slug":"struct","permalink":"/categories/struct/"}]},{"title":"Java HashMap","slug":"2018-04-02-hash_map","date":"2018-04-01T16:00:00.000Z","updated":"2020-01-03T10:12:29.216Z","comments":true,"path":"2018/04/02/2018-04-02-hash_map/","link":"","permalink":"/2018/04/02/2018-04-02-hash_map/","excerpt":"Java HashMap的本质HashMap的本质其实就是数组加链表。 它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。","text":"Java HashMap的本质HashMap的本质其实就是数组加链表。 它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 HashMap寻址方式对新数据的插入或者读取数据时，HashMap将Key的哈希值对数组长度取模，结果作为该Entry在数组中的index。在计算机中，取模的代价远高于位操作的代价，因此HashMap要求数组的长度必须为2的N次方。此时将Key的哈希值对2^N-1进行与运算，其效果即与取模等效。HashMap并不要求用户在指定HashMap容量时必须传入一个2的N次方的整数，而是会通过Integer.highestOneBit算出比指定整数小的最大的2^N值。 public static int highestOneBit(int i) { i |= (i &gt;&gt; 1); i |= (i &gt;&gt; 2); i |= (i &gt;&gt; 4); i |= (i &gt;&gt; 8); i |= (i &gt;&gt; 16); return i - (i &gt;&gt;&gt; 1); } 由于Key的哈希值的分布直接决定了所有数据在哈希表上的分布或者说决定了哈希冲突的可能性，因此为防止糟糕的Key的hashCode实现（例如低位都相同，只有高位不相同，与2^N-1取与后的结果都相同），JDK 1.7的HashMap通过如下方法使得最终的哈希值的二进制形式中的1尽量均匀分布从而尽可能减少哈希冲突。 int h = hashSeed; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 为什么HashMap线程不安全HashMap的线程不安全主要体现在resize时的死循环及使用迭代器时的fast-fail上。 resize死循环当HashMap的size超过Capacity*loadFactor时，需要对HashMap进行扩容。具体方法是，创建一个新的，长度为原来Capacity两倍的数组，保证新的Capacity仍为2的N次方，从而保证上述寻址方式仍适用。同时需要通过如下transfer方法将原来的所有数据全部重新插入（rehash）到新的数组中。 void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } } } 这里假设有两个线程同时执行了put操作并引发了rehash，执行了transfer方法，并假设线程一进入transfer方法并执行完next = e.next后，因为线程调度所分配时间片用完而“暂停”，此时线程二完成了transfer方法的执行。此时状态如下。 线程一： e=key(5) next=e.next=key(9) 线程二： table[1]-&gt;key(9)-&gt;key(5)-&gt;null 接着线程1被唤醒，继续执行第一轮循环的剩余部分 e.next = newTable[1] = null newTable[1] = e = key(5) e = next = key(9) e.next=table[1]=null table[1]=e=key(5) e=next=key(9) next=e.next=key(5) e.next=table[1]=key(5) table[1]=e=key(9) e=next=key(5) next=e.next=null e.next=table[1]=key(9) table[1]=e=key(5) e=next=null 此时循环链表形成，并且key(11)无法加入到线程1的新数组。在下一次访问该链表时会出现死循环。 Fast-fail在使用迭代器的过程中如果HashMap中key被remove，那么ConcurrentModificationException将被抛出，也即Fast-fail策略。 当HashMap的iterator()方法被调用时，会构造并返回一个新的EntryIterator对象，并将EntryIterator的expectedModCount设置为HashMap的modCount（该变量记录了HashMap被修改的次数）。 HashIterator() { expectedModCount = modCount; if (size &gt; 0) { // advance to first entry Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; } } java8对HashMap的优化当成员变量 MIN_TREEIFY_CAPACITY（树形最小容量）&gt;64 &amp;&amp; TREEIFY_THRESHOLD（树形阈值）&gt;8 时将链表转为红黑树 ,当TREEIFY_THRESHOLD&lt;=6时会再将红黑树转化成链表。而将链表转为红黑树主要是为了加快hash冲突时的查询速度。整个红黑树的查找，插入和删除都是O(logN)的，原因就是整个红黑树的高度是logN，查找从根到叶，走过的路径是树的高度，删除和插入操作是从叶到根的，所以经过的路径都是logN。红黑树深入剖析","categories":[{"name":"struct","slug":"struct","permalink":"/categories/struct/"}],"tags":[{"name":"hashmap","slug":"hashmap","permalink":"/tags/hashmap/"}],"keywords":[{"name":"struct","slug":"struct","permalink":"/categories/struct/"}]},{"title":"成长曲线","slug":"2018-03-28-grow_up","date":"2018-03-27T16:00:00.000Z","updated":"2020-01-03T10:12:29.169Z","comments":true,"path":"2018/03/28/2018-03-28-grow_up/","link":"","permalink":"/2018/03/28/2018-03-28-grow_up/","excerpt":"技术成长之路15年那会大四即将毕业，面临着工作问题。由于并不是什么好学校，来校内招人的公司基本上是一些销售的活，偶有几个技术岗位也是走个过场而已，并没有什么好的机会。后来和同学一起参加了java培训，或许并不是为了希望学习到多少东西而报名，而是为了能够有个好的机会。","text":"技术成长之路15年那会大四即将毕业，面临着工作问题。由于并不是什么好学校，来校内招人的公司基本上是一些销售的活，偶有几个技术岗位也是走个过场而已，并没有什么好的机会。后来和同学一起参加了java培训，或许并不是为了希望学习到多少东西而报名，而是为了能够有个好的机会。三个月时间过去，和预想的一样，培训并没有教你多少东西，大部分还是基础。但是通过机构顺利拿到了几家公司的offer，最后通过自己的考虑选择了一家小的创业公司。 回想那一会，抱着忐忑的心情进入这家公司的技术部门，接手第一份工作和任务。各种新奇的开发工具，叫不上名字的众多软件框架，庞大的代码库。最主要的是有技术大牛。这一切都是在学校那会接触不到的，同样也感受到前所未有的压力。在开始那半年就像一株久旱的禾苗遇到了雨水，拼命地吸收着工作中遇到的每一点知识。学习新的开发工具，学习调试和解bug的技巧，也学习处理各种棘手的业务逻辑。虽然每天工作很累，但每天都是满满的收获。 而现在俨然在某些方面已经是老手了，对于工作中遇到的一般问题都能游刃有余的解决。计算遇到问题也是在现有的接口上修改添加最多重新设计一番，单问题是这样长此以往下去会变成温水煮青蛙，自然而然的发现现在没有以前刚开始那样突飞猛进的进步了。这时他想起一句大神说过的话，或许你需要经历一段没有大神带的阶段。或许是这样的，跟随大神能够学校和接触到很多新技术和知识点，但这种在大神的技术实力背后永远不会发现自己真正的问题和欠缺点在哪里，问题都是大神帮你解决了，思路大神也给你想好了。那么你做的是什么呢？就只是按照别人的思路去做事而已，永远不会培养出来自己的思维模式，不会独立思考。这有两种结局，一种自己努力成长学习培养出自己的思维模式，学会独立思考。或者在温水中随遇而安，慢慢的和大多数人一样就这样了，也就废了！ 成长曲线很多技术人员在工作一两年之后，对工作周围的事情都驾轻就熟了，就可能会陷入第一个瓶颈期，再难突破。 一般来说，突破的过程，绝非轻而易举，而且有可能伴随着阵痛。舒适期对人有一种天然的吸引力，我们只要待在这个阶段，就能获得最大的安全感，尽量少地遭受不确定性的折磨。很多人会选择待在这个区间，不再走出一步。而另一些人则会对这一阶段的缓慢增长产生焦虑，于是舒适期变成了瓶颈期，从而引发下一轮的突破。 突破的要领 根基：良好的基础有利于我们快速突破。 环境：过于宽松的环境自然不利于人的进步，而盲目的紧张也不利于人的成长。突破的过程需要付出大量的精力，所以需要投入足够的时间去从容完成。 正确的学习资料：需要系统地去补习知识架构，技巧应该建立在对于普遍规则的理解之上。在奔向技术专家的路上，阅读Spec，是不可逾越的一道功课。技术的正宗与野路子 独立思考：很多人喜欢把技术好的人喊作“大神”，这自然是代表一种尊重。很多人碰到问题就喜欢找身边“大神”去问，但殊不知问再多问题，你仍然无法真正地有所提高。普通人和“大神”之间真正的鸿沟在于，能否独立思考和解决问题。 机遇：不得不承认好的机遇让你遇到好的环境接触到好的人可以让你事半功倍，有时候技术的瓶颈往往只需要别人提点一二。","categories":[{"name":"me","slug":"me","permalink":"/categories/me/"}],"tags":[{"name":"grow","slug":"grow","permalink":"/tags/grow/"}],"keywords":[{"name":"me","slug":"me","permalink":"/categories/me/"}]},{"title":"mysql index","slug":"2018-03-24-mysql_index","date":"2018-03-23T16:00:00.000Z","updated":"2020-01-03T10:12:29.166Z","comments":true,"path":"2018/03/24/2018-03-24-mysql_index/","link":"","permalink":"/2018/03/24/2018-03-24-mysql_index/","excerpt":"什么是数据库索引（Mysql）用了这么久Mysql，Mysql索引相关知识点却不太清楚，可以说匮乏。正好看到有面试问道，稍微看看和整理一下。","text":"什么是数据库索引（Mysql）用了这么久Mysql，Mysql索引相关知识点却不太清楚，可以说匮乏。正好看到有面试问道，稍微看看和整理一下。 索引（index）是存储引擎（storage engine）的实现，不是server。不是所有存储引擎都支持所有的索引类型。即使多个存储引擎支持某一索引，它们的实现和行为也可能有所不同。 Mysql官方定义：帮助Mysql高效获取数据的数据结构,简单来说索引是一种数据结构，用以加快数据的查询速度。 索引的分类数据结构角度 B+树索引(O(log(n)))：关于B+树索引 hash索引 只能满足相等查询，不能使用范围查询 效率高、一次定位，不像B-Tree 索引需要从根节点遍历，Hash 索引的查询效率要远高于B-Tree 只有Memory存储引擎显示支持hash索引 FULLTEXT索引（MyISAM和InnoDB引擎都支持了） R-Tree索引（用于对GIS数据类型创建SPATIAL索引） 存储角度 聚集索引（clustered index） 辅助索引（non-clustered index） 参考 逻辑角度 主键索引：主键索引是一种特殊的唯一索引，不允许有空值 普通索引或者单列索引 多列索引（复合索引）：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。最左前缀原则 唯一索引 空间索引（MYISAM）：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。MYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。创建空间索引，必须声明为NOT NULL，只能在MYISAM的表中创建 主键索引和普通索引的区别特殊的唯一索引，不允许有空值，唯一索引可以为空。普通索引或单列索引只是字段数据单独使用数据结构存储加快查询速度，没有其他特殊约束。 有索引但不生效的情况 如果条件中有or，即使其中有部分条件带索引也不会使用，要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引。 对于多列索引，满足断点前生效规则，参考。 like查询是以%开头。 存在数据类型隐形转换（如：字段是字符串类型，查询时不加引号）。 where条件对字段有数学运算。 where条件对字段使用函数。 mysql判断使用全表扫描要比使用索引时（如：数据量极少时）。 不推荐使用索引的情况 数据唯一性差（取值只有几种，意味着索引树级别少，多是平级，无异于全表扫描） 频繁更新的字段不要使用索引（频繁变化导致索引也频繁变化，增大数据库工作量，降低效率） 字段不在where语句出现时 where后含IS NULL / IS NOT NULL/ like ‘%**%’ 等条件时 where 子句里对索引列使用不等于（&lt;&gt;），使用索引效果一般 覆盖索引（covering index）covering index 提升查询效率：MySQL只需要通过索引就可以返回查询所需要的数据，不必回表查询。参考 优化经验参考：https://blog.csdn.net/kaka1121/article/details/53395587参考：http://www.cnblogs.com/hongfei/archive/2012/10/19/2731342.html","categories":[{"name":"db","slug":"db","permalink":"/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}],"keywords":[{"name":"db","slug":"db","permalink":"/categories/db/"}]},{"title":"let's encrypt","slug":"2018-03-18-let's_encrypt","date":"2018-03-17T16:00:00.000Z","updated":"2020-01-03T10:12:29.243Z","comments":true,"path":"2018/03/18/2018-03-18-let's_encrypt/","link":"","permalink":"/2018/03/18/2018-03-18-let's_encrypt/","excerpt":"获取let’s encrypt免费泛域名证书首先DNS需要支持CAA，我用的CloudXNS，添加CAA记录记录值0 issue letsencrypt.org,vpscurl https://get.acme.sh|sh,添加环境变量","text":"获取let’s encrypt免费泛域名证书首先DNS需要支持CAA，我用的CloudXNS，添加CAA记录记录值0 issue letsencrypt.org,vpscurl https://get.acme.sh|sh,添加环境变量 export CX_Key=&quot;&quot; export CX_Secret=&quot;&quot; .acme.sh/acme.sh --issue --dns dns_cx -d *.sjis.me -d sjis.me --keylength ec-384 其中CX_key和CX_Secret分别是CloudXNS账户中的API KEY和SECRET KEY跑完之后会发现CloudXNS多了两条TXT记录。之后等待一两分钟等记录生效。最后直接验证签发证书 ~/.acme.sh/acme.sh --installcert --ecc -d *.sjis.me --key-file /etc/ssl/caddy/*.sjis.me.key --fullchain-file /etc/ssl/caddy/fullchain.cer","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"certificate","slug":"certificate","permalink":"/tags/certificate/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"CloudXNS DDNS","slug":"2018-03-18-cloudxns_ddns","date":"2018-03-17T16:00:00.000Z","updated":"2020-01-03T09:55:50.146Z","comments":true,"path":"2018/03/18/2018-03-18-cloudxns_ddns/","link":"","permalink":"/2018/03/18/2018-03-18-cloudxns_ddns/","excerpt":"DDNSDynamic Domain Name Server(动态域名服务)，DDNS是将用户的动态IP地址映射到一个固定的域名解析服务上，用户每次连接网络的时候客户端程序就会通过信息传递把该主机的动态IP地址传送给位于服务商主机上的服务器程序，服务器程序负责提供DNS服务并实现动态域名解析。","text":"DDNSDynamic Domain Name Server(动态域名服务)，DDNS是将用户的动态IP地址映射到一个固定的域名解析服务上，用户每次连接网络的时候客户端程序就会通过信息传递把该主机的动态IP地址传送给位于服务商主机上的服务器程序，服务器程序负责提供DNS服务并实现动态域名解析。 场景电信光纤虽然会给一个公网ip，但是这个公网ip经常会变，这就给我们部署些自己的博客等小服务带来很多麻烦，我们不可能总是在公网ip变了后先去看路由的公网ip然后再重新设置dns解析ip。这就给我们带来很多困扰，ddns就非常适合这种场景。 CloudXNS DDNS我这里用的一个github一个开源项目cloudxns-ddns，go语言封装的CloudXNS的api通过定时请求CloudXNS的api接口来动态设置对应域名的解析ip地址。","categories":[{"name":"dns","slug":"dns","permalink":"/categories/dns/"}],"tags":[{"name":"dns","slug":"dns","permalink":"/tags/dns/"}],"keywords":[{"name":"dns","slug":"dns","permalink":"/categories/dns/"}]},{"title":"ubuntu nat","slug":"2018-02-01-ubuntu_nat","date":"2018-01-31T16:00:00.000Z","updated":"2020-01-03T10:12:29.205Z","comments":true,"path":"2018/02/01/2018-02-01-ubuntu_nat/","link":"","permalink":"/2018/02/01/2018-02-01-ubuntu_nat/","excerpt":"场景公司iBox（ubuntu 16.04系统）需要与机床连接实现实现业务，iBox本身只有两个网口，需要一个口连接厂房内网路由，一个口与机床直连，机床本身需要能够连接外网。这时候就需要iBox实现nat转发并实现路由功能。","text":"场景公司iBox（ubuntu 16.04系统）需要与机床连接实现实现业务，iBox本身只有两个网口，需要一个口连接厂房内网路由，一个口与机床直连，机床本身需要能够连接外网。这时候就需要iBox实现nat转发并实现路由功能。 配置router打开路由转发： # cat /proc/sys/net/ipv4/ip_forward 查看是否打开路由转发，0则没有打开，可以通过如下方式打开： # echo 1 &gt; /proc/sys/net/ipv4/ip_forward 通过修改/etc/sysctl.conf文件，net.ipv4.ip_forward = 1，然后执行sysctl -p可以永久打开 SNAT这里eth0作为内网口，eth1作为外网口两种实现方式：第一种： # iptables -t nat -A POSTROUTING -s 192.168.6.0/24 -o eth1 -j MASQUERADE 第二种： iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT iptables -A FORWARD -i eth1 -o eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT 建议使用第二种方式，第一种存在的弊端是指定了内网网段，当需要修改内网ip时会导致内网机器无法访问外网了，第二种方式则是在两个网卡直接做流量转发，免去修改ip的问题。 保存配置# iptables-save &gt; /etc/iptables/rules.v4 安装iptables-persistentapt-get install iptables-persistent 系统启动时iptables-persistent会以守护进程启动加载/etc/iptables/rules.v4的配置 手动恢复配置# iptables-restore &lt; /etc/iptables/rules.v4 查看nat转发列表# iptables -t nat --list dhcp安装于配置安装# sudo apt-get install isc-dhcp-server 配置vim /etc/default/isc-dhcp-server # On what interfaces should the DHCP server (dhcpd) serve DHCP requests? # Separate multiple interfaces with spaces, e.g. &quot;eth0 eth1&quot;. INTERFACES=&quot;eth0&quot; INTERFACES修改为对应的内网网口 修改/etc/dhcp/dhcpd.conf配置文件 # vim /etc/dhcp/dhcpd.conf 修改为 subnet 192.168.6.0 netmask 255.255.255.0 { range 192.168.6.100 192.168.6.200; option routers 192.168.6.1; option broadcast-address 192.168.6.255; option domain-name-servers 114.114.114.114; default-lease-time 600; max-lease-time 7200; } 其中的ip、掩码等修改为对应内网网口的ip与掩码，这里以192.168.6.1为例。配置完之后重启dhcp服务service isc-dhcp-server restart","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"/tags/ubuntu/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"nginx request copy","slug":"2018-01-29-nginx_http_mirro","date":"2018-01-28T16:00:00.000Z","updated":"2020-01-03T09:35:02.392Z","comments":true,"path":"2018/01/29/2018-01-29-nginx_http_mirro/","link":"","permalink":"/2018/01/29/2018-01-29-nginx_http_mirro/","excerpt":"问题目前公司前端项目的图片文件全部是通过请求后台获取token接口获取七牛上传token，然后前端直接上传七牛。这个机制本身不存在问题，问题在于公司使用了一个第三方授权服务器，这个服务器需要下载拿到用户上传的图片。问题来了，这个服务器当初部署在内网，如果需要它访问到上传图片，需要将文件在用户上传后再备份到一个我们自己的服务器。","text":"问题目前公司前端项目的图片文件全部是通过请求后台获取token接口获取七牛上传token，然后前端直接上传七牛。这个机制本身不存在问题，问题在于公司使用了一个第三方授权服务器，这个服务器需要下载拿到用户上传的图片。问题来了，这个服务器当初部署在内网，如果需要它访问到上传图片，需要将文件在用户上传后再备份到一个我们自己的服务器。 实现首先就问题本身，我们能够给出以下几种方案： 上传时同时上传到我们自己的服务器 上传七牛后七牛自己回调我们的文件备份接口 上传后在七牛回调我们的接口时下载后再备份到服务器 首先看第一种，这种就是上传了两次文件，太浪费客户端流量和带宽，而且也不优雅。第二种想法如果可行的话当然是最好的方式，但是就目前来看七牛貌似没有类似的接口。那么就只有第三种方式。 具体实现通过七牛回调我们自己的接口再去备份，这里同样也有两种实现方式： 在回调接口中通过mq异步下载和备份文件 nginx中通过请求的拷贝|镜像请求实现将请求同时转给另一个专门用于处理文件备份的接口从程序的解耦和整体架构上看可能nginx流量的拷贝|镜像更为理想，同时实现稍显复杂，同时需要与运维的同事进行沟通交流并进行nginx的配置调试。直接通过原有接口mq异步备份文件也是一种实现方式，两者个人认为都是可行方案，如果条件允许的情况下建议第二种，毕竟程序员就是喜欢折腾-_-。通过mq异步进行文件备份的方式没什么好说的，就是发mq消息，下载上传。本文重点，如何通过nginx进行请求的copy|镜像转发。 post_action方式location / { uwsgi_pass unix:app.sock; post_action @post_action; } location @post_action { proxy_pass http://dst_host:dst_port; } 这种方式请求首先传递给unix：app.sock，并在完成时，post_action指令将请求传递给指定位置的@post_action。但是这种方式@post_action中的proxy_pass不能是uri，比如：http://ip:port/test;这种是不可以的。很可惜... mirror_modulenginx1.13.4之后开始支持ngx_http_mirror_module模块，这是一个请求镜像转发模块，相信听名字就知道是干嘛的了，请求镜像拷贝转发呗。 # original配置 location / { mirror /mirror; mirror_request_body off; proxy_pass http://127.0.0.1:9502; } # mirror配置 location /mirror { internal; proxy_pass http://127.0.0.1:8081$request_uri; proxy_set_header X-Original-URI $request_uri; } 参考https://www.centos.bz/2017/08/nginx-request-copy-ngx_http_mirror_module/","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[]},{"title":"openpgp签名与验签","slug":"2018-01-23-go_java_signature_check","date":"2018-01-22T16:00:00.000Z","updated":"2020-01-03T09:55:50.250Z","comments":true,"path":"2018/01/23/2018-01-23-go_java_signature_check/","link":"","permalink":"/2018/01/23/2018-01-23-go_java_signature_check/","excerpt":"存在的问题目前每次程序版本发布需要手动gpg签名并打包发布，存在的问题： 每次都需要人工手动使用gpg签名 如果如果需要签名发布的app变多也会增加很多重复劳动 其他项目或应用文件签名下发需要一个统一的接口","text":"存在的问题目前每次程序版本发布需要手动gpg签名并打包发布，存在的问题： 每次都需要人工手动使用gpg签名 如果如果需要签名发布的app变多也会增加很多重复劳动 其他项目或应用文件签名下发需要一个统一的接口 其实也考虑过写脚本进行签名，但是生产机器还需要装gpg比较麻烦。其实主要还是想学习一下如何代码进行gpg加密,哈哈… golang openpgp签名验签其实公司需要的是java版，但是最近一直在做go的项目，并且以后go的项目说不定也会用到。虽然语言不通，但也作为摸索签名加密的一种途径。 package main import ( &quot;bytes&quot; &quot;golang.org/x/crypto/openpgp&quot; &quot;os&quot; &quot;io/ioutil&quot; ) func main() { ArmoredDetachSign(&quot;d:/test.txt&quot;) CheckArmoredDetachedSignature(&quot;d:/test.txt.asc&quot;,&quot;d:/test.txt&quot;) } //CheckArmoredDetachedSignature openpgp验签 func CheckArmoredDetachedSignature(signatureFile, verificationFile string) (*openpgp.Entity, error) { pubring, err := ioutil.ReadFile(&quot;path to pubring.gpg&quot;) keyRingReader := bytes.NewBuffer(pubring) signature, err := os.Open(signatureFile) if err != nil { return nil, err } defer signature.Close() verification, err := os.Open(verificationFile) if err != nil { return nil, err } defer verification.Close() keyring, err := openpgp.ReadKeyRing(keyRingReader) if err != nil { return nil, err } entity, err := openpgp.CheckArmoredDetachedSignature(keyring, verification, signature) if err != nil { return nil, err } return entity, nil } //ArmoredDetachSign openpgp签名 func ArmoredDetachSign(signFile string) error { var entity *openpgp.Entity var entityList openpgp.EntityList pubring, err := ioutil.ReadFile(&quot;path to secring.gpg&quot;) if err!=nil{ return err } keyringFileBuffer := bytes.NewBuffer(pubring) entityList, err = openpgp.ReadKeyRing(keyringFileBuffer) if err != nil { return err } entity = entityList[0] passPhraseByte := []byte(&quot;your private key password&quot;) entity.PrivateKey.Decrypt(passPhraseByte) for _, subKey := range entity.Subkeys { subKey.PrivateKey.Decrypt(passPhraseByte) } sign, err := os.Open(signFile) if err != nil { return err } defer sign.Close() asc, err := os.Create(signFile + &quot;.asc&quot;) if err != nil { return err } defer asc.Close() err = openpgp.ArmoredDetachSign(asc, entity, sign, nil) if err != nil { return err } return nil } java openpgp签名验签maven引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.59&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcpg-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.59&lt;/version&gt; &lt;/dependency&gt; 记住一定要使用最新的1.59版本，之前的版本都是jdk1.5、1.6版本编译的，我们现在项目都是1.8版本差异太大无法编译。 import org.apache.commons.io.*; import org.bouncycastle.bcpg.*; import org.bouncycastle.jce.provider.*; import org.bouncycastle.openpgp.*; import org.bouncycastle.openpgp.jcajce.*; import org.bouncycastle.openpgp.operator.jcajce.*; import java.io.*; import java.security.*; import java.util.*; /** * OpenGPG签名工具类 * * @author SunJun */ public class OpenPgpService { public static void main(String[] args) throws Exception { Security.addProvider(new BouncyCastleProvider()); createSignature(&quot;d:/test.txt&quot;, &quot;d:/test.txt.asc&quot;, true); verifySignature(&quot;d:/test.txt&quot;,&quot;d:/test.txt.asc&quot;); } /** * verifySignature 签名验证 * * @param fileName 待验签文件名 * @param inputFileName asc文件 * @author SunJun */ public static boolean verifySignature(String fileName, String inputFileName) throws IOException, PGPException { InputStream in = null; InputStream keyIn = null; boolean result; try { in = new BufferedInputStream(new FileInputStream(inputFileName)); keyIn = new BufferedInputStream(new FileInputStream(new File(&quot;pubring.asc&quot;))); result = verifySignature(fileName, in, keyIn); } finally { IOUtils.closeQuietly(keyIn); IOUtils.closeQuietly(in); } return result; } /** * verifySignature * * @param fileName 待验签文件名 * @param in asc文件流 * @param keyIn 公钥文件 * @author SunJun */ private static boolean verifySignature(String fileName, InputStream in, InputStream keyIn) throws IOException, PGPException { PGPSignature sig; InputStream dIn = null; PGPSignatureList p3; in = PGPUtil.getDecoderStream(in); JcaPGPObjectFactory pgpFact = new JcaPGPObjectFactory(in); Object o = pgpFact.nextObject(); if (o instanceof PGPCompressedData) { PGPCompressedData c1 = (PGPCompressedData) o; pgpFact = new JcaPGPObjectFactory(c1.getDataStream()); p3 = (PGPSignatureList) pgpFact.nextObject(); } else { p3 = (PGPSignatureList) o; } PGPPublicKeyRingCollection pgpPubRingCollection = new PGPPublicKeyRingCollection(PGPUtil.getDecoderStream(keyIn), new JcaKeyFingerprintCalculator()); try { dIn = new BufferedInputStream(new FileInputStream(fileName)); sig = p3.get(0); PGPPublicKey key = pgpPubRingCollection.getPublicKey(sig.getKeyID()); sig.init(new JcaPGPContentVerifierBuilderProvider().setProvider(&quot;BC&quot;), key); int ch; while ((ch = dIn.read()) &gt;= 0) { sig.update((byte) ch); } } finally { IOUtils.closeQuietly(dIn); } return sig.verify(); } /** * createSignature 文件签名 * * @param inputFileName 待签名文件 * @param outputFileName 签名asc文件 * @param armor 是否生成ASSCII码形式签名文件 * @author SunJun */ public static void createSignature(String inputFileName, String outputFileName, boolean armor) throws IOException, PGPException { InputStream keyIn = null; OutputStream out = null; try { keyIn = new BufferedInputStream(new FileInputStream(new File(&quot;secring.asc&quot;))); out = new BufferedOutputStream(new FileOutputStream(outputFileName)); createSignature(inputFileName, keyIn, out, &quot;private key password&quot;.toCharArray(), armor); } finally { IOUtils.closeQuietly(keyIn); IOUtils.closeQuietly(out); } } /** * createSignature 文件签名 * * @param fileName 待签名文件名 * @param keyIn 私钥文件流 * @param outputStream 签名asc文件 * @param pass 私钥密码 * @param armor 是否生成ASSCII码形式签名文件 * @author SunJun */ private static void createSignature(String fileName, InputStream keyIn, OutputStream outputStream, char[] pass, boolean armor) throws IOException, PGPException { if (armor) { outputStream = new ArmoredOutputStream(outputStream); } PGPSecretKey pgpSec = readSecretKey(keyIn); PGPPrivateKey pgpPriKey = pgpSec.extractPrivateKey(new JcePBESecretKeyDecryptorBuilder().setProvider(&quot;BC&quot;).build(pass)); PGPSignatureGenerator sGen = new PGPSignatureGenerator( new JcaPGPContentSignerBuilder(pgpSec.getPublicKey().getAlgorithm(), PGPUtil.SHA1).setProvider(&quot;BC&quot;)); sGen.init(PGPSignature.BINARY_DOCUMENT, pgpPriKey); BCPGOutputStream bOut = new BCPGOutputStream(outputStream); InputStream fIn = null; try { int ch; fIn = new BufferedInputStream(new FileInputStream(fileName)); while ((ch = fIn.read()) &gt;= 0) { sGen.update((byte) ch); } sGen.generate().encode(bOut); } finally { IOUtils.closeQuietly(fIn); IOUtils.closeQuietly(bOut); } } /** * readSecretKey * * @param input 私钥文件流 * @return 私钥 */ private static PGPSecretKey readSecretKey(InputStream input) throws IOException, PGPException { PGPSecretKeyRingCollection pgpSec = new PGPSecretKeyRingCollection( PGPUtil.getDecoderStream(input), new JcaKeyFingerprintCalculator()); Iterator keyRingIterator = pgpSec.getKeyRings(); while (keyRingIterator.hasNext()) { PGPSecretKeyRing keyRing = (PGPSecretKeyRing) keyRingIterator.next(); Iterator keyIterator = keyRing.getSecretKeys(); while (keyIterator.hasNext()) { PGPSecretKey key = (PGPSecretKey) keyIterator.next(); if (key.isSigningKey()) { return key; } } } throw new IllegalArgumentException(&quot;Can&#39;t find signing key in key ring.&quot;); } }","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"algorithms","slug":"algorithms","permalink":"/tags/algorithms/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"Go项目实战总结","slug":"2017-09-25-start_golang","date":"2017-09-24T16:00:00.000Z","updated":"2020-01-03T10:12:29.209Z","comments":true,"path":"2017/09/25/2017-09-25-start_golang/","link":"","permalink":"/2017/09/25/2017-09-25-start_golang/","excerpt":"GOPATHgo get、go install等命令都会用到gopath，gopath告诉go命令和和其他相关工具到哪找安装在你系统上的go包。","text":"GOPATHgo get、go install等命令都会用到gopath，gopath告诉go命令和和其他相关工具到哪找安装在你系统上的go包。 windows设置gopath set GOPATH=c:/Users/user/go;d:/workspace/mygo linux设置gopath export GOPATH=/home/user/go:/usr/local/mygo gopath也是工作目录，用户源码文件放在gopath/src下,go install 生成的可执行文件在gopath/bin下，go install产生的中间文件.a文件在gopath/pkg下，go install编译器会判断源码是否有改动，如果没有改动就不会重新生成.a文件，可以加快编译速度。 vendorgolang版本依赖是个很头疼的问题，如果我们的项目依赖某一个特定的版本，或者第三方库不满足我们需要但是又很难让作者做出改动的时候，依赖问题都是个痛点。通过go get 获取的依赖在不同的电脑上可能出现编译不通过的情况。go1.5后提供了GO15VENDOREXPERIMENT环境变量，用于将go build、go install时的搜索路径修改为当前目录/vendor下，通过依赖项目vendor目录可以依赖一些特定版本的第三方依赖，或者是自己做出修改的依赖包但是又不属于项目本身业务代码，就可以放在vendor目录下。 bindata的使用我们的项目中可能带有前端页面，虽然是可以前后端分离，但是前端页面比较简单就没有必要单独分出来。如果前端代码也能直接编译到go的可执行程序中那不是一件很爽的事么，这就用到了go-bindata，他会把静态文件生成为.go文件。go-bindatago-bindata-assetfsgo-bindata与go-bindata-assetfs的区别: go-bindata仅仅用于生成bindata文件提供基本的方法 go-bindata-assetfs依赖go-bindata并提供方法让我们能够很轻松的向外提供FileServer 在项目中我们如何使用呢： go-bindata -pkg=asserts -o src/isesol.com/agentServer/asserts/bindata.go asserts/... pkg:指定生成的.go文件的package名 o:指定生成的.go文件的路径已经名称 asserts/…:asserts静态文件目录,…循环迭代asserts目录下的所有文件 自己的代码中 var fsh http.Handler if config.AgentConfig.LocalTest { fsh = http.FileServer(http.Dir(&quot;asserts&quot;)) } else { fsh = http.FileServer(&amp;assetfs.AssetFS{Asset: asserts.Asset, AssetDir: asserts.AssetDir, Prefix: &quot;asserts&quot;}) } r.PathPrefix(&quot;/&quot;).Handler(fsh) 我们可以通过配置项指定使用本地静态文件还是生成的bindata文件，需要注意的是其中: Prefix: &quot;asserts&quot;asserts一定要与静态文件目录同名。 xorm的使用xorm是一个数据库框架，我们的项目使用sqlite数据库，但是有个很恶心的地方就是sqlite3的驱动部分代码确是c，这导致windows上编译不了其他平台的代码。毕竟数据库发展这么多年，以前的数据库都是c写的，想要把几M的c代码全部替换成go确实也不是一件容易的事。xorm使用过程中的坑： 数据库主键自增:数据库设置了主键自增，我们insert的时候不设置主键值默认是0，struct中id字段tag必须加上autoincr 多表联合查询时:如果不同struct中有相同的字段struct A { ID int64 `xorm:&quot;id&quot; json:&quot;id&quot;` Name string `xorm:&quot;name&quot; json:&quot;name&quot;` } struct B { ID id int64 `json:&quot;id&quot;` } struct C { A B } var c C xorm.Find(&amp;c) A、B中都有id字段并且json的tag都是id，查询出来的结果会发现没有id字段，如果前端需要使用id字段那就会有问题,解决办法是在struct C中添加ID字段就可以了。 表名与struct名不一致，查询时一定要指定表名 分页操作需要count获取总条数，有时候我们查询时可能需要指定哪些字段，Select()语句不要放在Count()之前，否则生成的sql会变成select Select()中的字段而不是select count(*) 复杂的查询使用sql语句 xorm查询时不要new一个变量，应为查询时xorm会new一个，相当于申请了两次内存，会造成内存内存浪费。 go中cgi接口的设计go中实现一个web服务非常的容易: http.HandlerFunc(&quot;/test&quot;,func(w http.ResponseWriter, r *http.Request){ }) http.HandlerFunc(&quot;/cgi&quot;,func(w http.ResponseWriter, r *http.Request){ }) http.ListenAndServe(&quot;:8080&quot;, nil) 几行代码就可以实现一个http服务器，test、cgi就是对应的处理函数用于处理请求。但是如果我们想设计一个统一的cgi接口作为请求的唯的一入口而不是添加一个又一个HandlerFunc，该怎么做呢，首先为什么要设计一个post请求的cgi接口，以及他的好处： 用户不能通过url探测接口功能 参数不会显式的传递，一定程度上保证了接口安全性 即使接口被探测到，修改接口也非常方便，只需要修改body内容而不需要url地址实现：我们可以使用cgi作为统一的入口，直接上代码： func cgi(w http.ResponseWriter, r *http.Request) { if r.Body == nil { util.JSON(w, map[string]string{&quot;errorCode&quot;: &quot;-1&quot;, &quot;errorInfo&quot;: &quot;systemError&quot;}) return } defer r.Body.Close() var req map[string]interface{} err = jsoniter.NewDecoder(r.Body).Decode(&amp;req) if err != nil { util.JSON(w, map[string]string{&quot;errorCode&quot;: &quot;-1&quot;, &quot;errorInfo&quot;: &quot;systemError&quot;}) return } cmd := strings.TrimPrefix(req[&quot;cmd&quot;].(string), constant.ServiceName) if fn, ok := util.Cgi.Get(cmd); ok { res = fn(req) } else { util.JSON(w, model.RestError{ErrorCode: &quot;-1&quot;, ErrorInfo: &quot;cmd not found&quot;}) return } switch res.(type) { case error: util.JSON(w, model.RestError{ErrorCode: &quot;-1&quot;, ErrorInfo: res.(error).Error()}) case model.RestError: util.JSON(w, res) default: util.JSON(w, map[string]interface{}{&quot;cmd&quot;: constant.ServiceName + cmd, &quot;response&quot;: res}) } } 将request body中内容解析出来，cmd指向对应处理函数，我们通过一个map[string]interface,其中key是cmd的值value是接口类型。service包中通过 func init() { util.Cgi.Register(&quot;test/test&quot;, service) } func service(req interface{}) (interface{}) { //业务代码 } 来实现处理方法的注册，这里用到的其实是接口值。 结果通过res返回，这里一个很重要的地方就是接口的类型判断，通过switch res.(type)判断返回类型是系统异常、业务异常、正常返回中的哪种。这样一个简单的cgi接口就完成了。 如何统一返回格式化时间package model // JSONTime JSONTime type JSONTime time.Time // MarshalJSON MarshalJSON func (t JSONTime) MarshalJSON() ([]byte, error) { stamp := fmt.Sprintf(&quot;\\&quot;%s\\&quot;&quot;, time.Time(t).Format(&quot;2006-01-02 15:04:05&quot;)) return []byte(stamp), nil } //model type Subscription struct { ID int64 `xorm:&quot;id pk autoincr&quot; json:&quot;id&quot;` MachineID string `xorm:&quot;machine_id&quot; json:&quot;machineId&quot;` MsgID int64 `xorm:&quot;msg_id&quot; json:&quot;msgId&quot;` MessageID string `xorm:&quot;message_id&quot; json:&quot;messageId&quot;` UsingStatus string `json:&quot;usingStatus&quot;` MachineType string `json:&quot;machineType&quot;` Status string `json:&quot;status&quot;` RuleName string `json:&quot;ruleName&quot;` GmtCreate model.JSONTime `xorm:&quot;created&quot; json:&quot;-&quot;` GmtModify model.JSONTime `xorm:&quot;created updated&quot; json:&quot;-&quot;` } json.Marshal(sub) 当我们调json.Marshal时，其实就是调了这里我们这里我们自己实现的MarshalJSON方法，我们一层一层进去会发现。 // Marshaler is the interface implemented by types that // can marshal themselves into valid JSON. type Marshaler interface { MarshalJSON() ([]byte, error) } var ( marshalerType = reflect.TypeOf(new(Marshaler)).Elem() textMarshalerType = reflect.TypeOf(new(encoding.TextMarshaler)).Elem() ) // newTypeEncoder constructs an encoderFunc for a type. // The returned encoder only checks CanAddr when allowAddr is true. func newTypeEncoder(t reflect.Type, allowAddr bool) encoderFunc { if t.Implements(marshalerType) { return marshalerEncoder } if t.Kind() != reflect.Ptr &amp;&amp; allowAddr { if reflect.PtrTo(t).Implements(marshalerType) { return newCondAddrEncoder(addrMarshalerEncoder, newTypeEncoder(t, false)) } } if t.Implements(textMarshalerType) { return textMarshalerEncoder } if t.Kind() != reflect.Ptr &amp;&amp; allowAddr { if reflect.PtrTo(t).Implements(textMarshalerType) { return newCondAddrEncoder(addrTextMarshalerEncoder, newTypeEncoder(t, false)) } } switch t.Kind() { case reflect.Bool: return boolEncoder case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return intEncoder case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr: return uintEncoder case reflect.Float32: return float32Encoder case reflect.Float64: return float64Encoder case reflect.String: return stringEncoder case reflect.Interface: return interfaceEncoder case reflect.Struct: return newStructEncoder(t) case reflect.Map: return newMapEncoder(t) case reflect.Slice: return newSliceEncoder(t) case reflect.Array: return newArrayEncoder(t) case reflect.Ptr: return newPtrEncoder(t) default: return unsupportedTypeEncoder } } 这里会判断我们是不是实现了Marshaler接口，如果实现了就返回这个encoderFunc，否则t.Kind()判断是哪种类型返回对应的Encoder。 mapstructure使用mapstructure m.(map[string]interface{})[&quot;parameters&quot;] parameters参数与struct映射存在一个问题就是，后台struct使用string字段去接收时，前端传一个int型的值，会发生类型转换错误，一起开始没有发现解决方法。后来发现mapstructure有一个配置项 mapstructure.DecoderConfig.WeaklyTypedInput 默认是false，设置成true工具内部就会帮我们进行类型转换，前端传来的int或float类型会自动帮我们转成struct中对应的类型。 爽了！不用再去修改前端代码了！^_^ negroni中间件Negroni 不是一个框架，它是为了方便使用 net/http 而设计的一个库而已。可以用 Use 函数把这些 http.Handler 处理器引进到处理器链上来： mux := http.NewServeMux() n := negroni.New() n.Use(negroni.HandlerFunc(Test1)) n.Use(negroni.HandlerFunc(Test2)) n.UseHandler(mux) n.Run(&quot;:8080&quot;) 这个功能非常的好用，如果我们要在所有的处理函数之前添加一个权限校验或者其他的全局处理的handler，name这个功能是一个很好的选择。 内嵌ngrok与源码修改基于ngrok 1.7 添加/api/tunnels接口，显示本地暴露端口client/views/web/http.go: register方法添加 http.HandleFunc(&quot;/api/tunnels&quot;, func(w http.ResponseWriter, r *http.Request) { payloadData := whv.ctl.State().GetTunnels() payload, err := json.Marshal(payloadData) if err != nil { panic(err) } w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;) w.Write(payload) }) 去除ngrok启动页面 166 //var termView *term.TermView 167 //if config.LogTo != &quot;stdout&quot; { 168 // termView = term.NewTermView(ctl) 169 // ctl.AddView(termView) 170 //} 175 //if termView != nil { 176 // ctl.AddView(termView.NewHttpView(p)) 177 //} 修改启动配置项client/main.go: 部分代码注释 //func init() { // if runtime.GOOS == &quot;windows&quot; { // if mousetrap.StartedByExplorer() { // fmt.Println(&quot;Don&#39;t double-click ngrok!&quot;) // fmt.Println(&quot;You need to open cmd.exe and run it from the command line!&quot;) // time.Sleep(5 * time.Second) // os.Exit(1) // } // } //} client/cli.go: ParseArgs方法代码修改为 opts = &amp;Options{} configS := strings.Fields(config.AgentConfig.NgrokArgs) confMap := make(map[string]string, 0) commandSlice := make([]string, 0) for _, v := range configS { if strings.HasPrefix(v, &quot;-&quot;) { v = strings.TrimPrefix(v, &quot;-&quot;) if strings.Contains(v, &quot;=&quot;) { cfgVal := strings.Split(v, &quot;=&quot;) confMap[cfgVal[0]] = cfgVal[1] } else { confMap[v] = &quot;&quot; } } else { commandSlice = append(commandSlice, v) } } if v, ok := confMap[&quot;config&quot;]; ok { opts.config = v } if v, ok := confMap[&quot;log&quot;]; ok { opts.logto = v } if v, ok := confMap[&quot;log-level&quot;]; ok { opts.loglevel = v } if v, ok := confMap[&quot;authtoken&quot;]; ok { opts.authtoken = v } if v, ok := confMap[&quot;httpauth&quot;]; ok { opts.httpauth = v } if v, ok := confMap[&quot;subdomain&quot;]; ok { opts.subdomain = v } if v, ok := confMap[&quot;hostname&quot;]; ok { opts.hostname = v } opts.command = commandSlice[0] switch opts.command { case &quot;list&quot;: opts.args = commandSlice[1:] case &quot;start&quot;: opts.args = commandSlice[1:] case &quot;start-all&quot;: opts.args = commandSlice[1:] case &quot;version&quot;: fmt.Println(version.MajorMinor()) os.Exit(0) case &quot;help&quot;: flag.Usage() os.Exit(0) case &quot;&quot;: fmt.Errorf(&quot;Error: Specify a local port to tunnel to, or &quot; + &quot;an ngrok command.\\n\\nExample: To expose port 80, run &quot; + &quot;&#39;ngrok 80&#39;&quot;) return default: if len(commandSlice) &gt; 1 { fmt.Errorf(&quot;You may only specify one port to tunnel to on the command line, got %d: %v&quot;, len(commandSlice), commandSlice) return } opts.command = &quot;default&quot; opts.args = commandSlice } return client/config.go：LoadConfiguration代码修改 if matched, err = regexp.MatchString(&quot;^[0-9a-zA-Z_\\\\-!]+$&quot;, content); err != nil {} 之前代码替换成： configPath := opts.config if configPath == &quot;&quot; { configPath = defaultPath() } log.Info(&quot;Reading configuration file %s&quot;, configPath) // deserialize/parse the config config = &amp;Configuration{} // try to parse the old .ngrok format for backwards compatibility matched := false var content string //config from file if opts.config != &quot;&quot; { configBuf, err := ioutil.ReadFile(configPath) if err != nil { panic(&quot;Failed to read configuration file &quot; + configPath + &quot;: &quot; + err.Error()) } if err = yaml.Unmarshal(configBuf, &amp;config); err != nil { panic(&quot;Error parsing configuration file &quot; + configPath + &quot;: &quot; + err.Error()) } content = strings.TrimSpace(string(configBuf)) } else { config = &amp;Configuration{ ServerAddr: &quot;tunnel.qydev.com:4443&quot;, TrustHostRootCerts: false, Tunnels: map[string]*TunnelConfiguration{ &quot;ssh&quot;: { Protocols: map[string]string{&quot;tcp&quot;: &quot;127.0.0.1:22&quot;}, }, &quot;web&quot;: { Subdomain: strings.TrimPrefix(agentConf.AgentConfig.BoxName, &quot;LocalBox-&quot;), Protocols: map[string]string{&quot;http&quot;: &quot;80&quot;}, }, }, } } for name, t := range config.Tunnels { //添加 if &quot;web&quot; == name { t.Subdomain = strings.TrimPrefix(agentConf.AgentConfig.BoxName, &quot;LocalBox-&quot;) } for k, addr := range t.Protocols { //添加 if &quot;http&quot; == k { t.HttpAuth = agentConf.AgentConfig.BoxUserName + &quot;:&quot; + agentConf.AgentConfig.BoxPassword } } } 代码扫描工具go get github.com/alecthomas/gometalinter gometalinter --install --update gometalinter ./... --vendor -e asserts --fast","categories":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"/tags/golang/"}],"keywords":[{"name":"golang","slug":"golang","permalink":"/categories/golang/"}]},{"title":"七牛文件上传","slug":"2017-07-10-qiniu_upload","date":"2017-07-09T16:00:00.000Z","updated":"2020-01-03T10:12:29.219Z","comments":true,"path":"2017/07/10/2017-07-10-qiniu_upload/","link":"","permalink":"/2017/07/10/2017-07-10-qiniu_upload/","excerpt":"基本的环境搭建 这里使用的是七牛的Java sdk，框架用的是spring boot，至于spring boot的基本框架搭建就不说了。maven引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.qiniu&lt;/groupId&gt; &lt;artifactId&gt;qiniu-java-sdk&lt;/artifactId&gt; &lt;version&gt;[7.0.0, 7.2.99]&lt;/version&gt; &lt;/dependency&gt;","text":"基本的环境搭建 这里使用的是七牛的Java sdk，框架用的是spring boot，至于spring boot的基本框架搭建就不说了。maven引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.qiniu&lt;/groupId&gt; &lt;artifactId&gt;qiniu-java-sdk&lt;/artifactId&gt; &lt;version&gt;[7.0.0, 7.2.99]&lt;/version&gt; &lt;/dependency&gt; 上传token的获取public String getUpToken(String appId){ StringMap map = new StringMap(); StringMap putPolicy = new StringMap(); map.put(&quot;key&quot;, &quot;${key}&quot;); map.put(&quot;hash&quot;, &quot;${hash}&quot;); map.put(&quot;bucket&quot;, &quot;${bucket}&quot;); map.put(&quot;fsize&quot;, &quot;${fsize}&quot;); map.put(&quot;fname&quot;, &quot;${fname}&quot;); map.put(&quot;appId&quot;, appId); Auth auth = Auth.create(Constants.ACCESSS_KEY, Constants.SECRET_KEY); //上传文件不能重复(0:相同文件覆盖;1:相同文件不会覆盖) 只有在设置了key时才会生效,默认相同文件相同key不会覆盖正确返回 putPolicy.put(&quot;insertOnly&quot;, 0); //是否启用上传模式(1:启用;0:关闭)启用后 由回调的返回参数的key字段指定七牛中的资源名称 putPolicy.put(&quot;callbackFetchKey&quot;, 1); //回调Content-Type putPolicy.put(&quot;callbackBodyType&quot;, &quot;application/json&quot;); //回调URL putPolicy.put(&quot;callbackUrl&quot;, Constants.CALLBACK_URL); //回调post请求体 putPolicy.put(&quot;callbackBody&quot;, Json.encode(map)); String uploadToken = auth.uploadToken(Constants.BUCKET, null, Constants.EXPIRES_1H, putPolicy); return uploadToken; } 代码中的callbackBody定义上传返回的变量和用户自定义的魔法变量。其中：key：七牛存储空间中的资源名称，可以用户定义或者七牛自动生成一个随机的UUIDhash：文件的hash值bucket：存储空间名称fsize：文件大小fname：文件名appId：自定义变量callbackFetchKey：七牛通过回调的返回参数中的key来确定资源文件名称，用于自定义前缀 这里的appId用于确定哪个业务线，实际项目中应该定义固定的appId，拿到上传Token就可以进行上传操作了。 文件上传public Object callback(HttpServletRequest request) throws IOException{ String line = &quot;&quot;; StringBuilder sb = new StringBuilder(); BufferedReader br = new BufferedReader(new InputStreamReader(request.getInputStream())); while ((line = br.readLine()) != null){ sb.append(line); } String authorization = request.getHeader(&quot;Authorization&quot;); String contentType = request.getContentType(); Auth auth = Auth.create(Constants.ACCESSS_KEY, Constants.SECRET_KEY); //七牛回调鉴权 boolean isQiniuCallback = auth.isValidCallback(authorization, Constants.CALLBACK_URL, sb.toString().getBytes(), contentType); Map&lt;String, Object&gt; returnMap = Maps.newHashMap(); if (isQiniuCallback){ CallBackParam callBackParam = Json.decode(sb.toString(), CallBackParam.class); String appId = callBackParam.getAppId(); String key = callBackParam.getKey(); callBackParam.setKey(appId + &quot;/&quot; + key); returnMap.put(&quot;key&quot;, appId + &quot;/&quot; + key); returnMap.put(&quot;payload&quot;, callBackParam); } else{ returnMap.put(&quot;success&quot;, false); } return returnMap; } 回调中获得appId以及七牛生成的key，我们将其组装起来以自定义前缀。 我们会看到之前设置的自定义变量appId:blog是在生成token的时候设置的，上传完成后会返回给我们。可能有人会问，这里直接上传到七牛不就好了么，为什么还要写一个方法。其实原因很简单，从上面的生成token的方法中已经可以看出我们设置了callbackUrl以及callbackBody来让七牛在上传完成的时候本来回调我们自己的接口，只有当我们自己的接口回调成功整个上传才会成功，反之失败。而且通过回调我们可以知道我们什么时候上传完成，同时可以进行一系列的log、插表等操作来记录上传信息。 设置callbackFetchKey情况 未设置callbackFetchKey情况可以看到回参中有前缀但是七牛存储空间中是没有前缀的 因为七牛回调需要调用我们本地的接口，可是我们没有公网ip所以使用的ngrok内网穿透来实现的，ngrok的安装我的博客没有写，可以参考我同事的博客。","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"qiniu","slug":"qiniu","permalink":"/tags/qiniu/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"nginx lua预编译","slug":"2017-04-0-precompile_lua_module","date":"2017-04-04T16:00:00.000Z","updated":"2020-01-03T10:12:29.224Z","comments":true,"path":"2017/04/05/2017-04-0-precompile_lua_module/","link":"","permalink":"/2017/04/05/2017-04-0-precompile_lua_module/","excerpt":"前景提要前段时间nginx新版1.11.11更新之后nginx_http_connection_t结构体中的ngx_buf_t **busy从二级指针改为了ngx_chain_t *busy链表。如何修改见之前的blog。","text":"前景提要前段时间nginx新版1.11.11更新之后nginx_http_connection_t结构体中的ngx_buf_t **busy从二级指针改为了ngx_chain_t *busy链表。如何修改见之前的blog。 问题由于是直接将源码修改为支持nginx-1.11.11及之后版本，存在的问题是：如果我想编译nginx-1.11.11之前的版本怎么办呢？这样代码就不兼容之前版本的nginx了，之前只考虑到解决不兼容新版本的问题。晚上下班和大神聊到前端时间nginx-1.11.11新版本的第三方lua模块的不兼容问题，受大神的提示应该兼容不同版本的nginx包括新版和之前的老版本。 解决在lua模块中通过预编译判断nginx的版本来决定使用哪段代码，废话不多说上代码： if (hc-&gt;nbusy) { b = NULL; for (i = 0; i &lt; hc-&gt;nbusy; i++) { /*get first buf*/ /* b = hc-&gt;busy[i]; */ #if nginx_version &gt;= 1011011 b = hc-&gt;busy-&gt;buf; #else b = hc-&gt;busy[i]; #endif dd(&quot;busy buf: %d: [%.*s]&quot;, (int) i, (int) (b-&gt;pos - b-&gt;start), b-&gt;start); if (first == NULL) { if (mr-&gt;request_line.data &gt;= b-&gt;pos || mr-&gt;request_line.data + mr-&gt;request_line.len + line_break_len &lt;= b-&gt;start) { continue; } dd(&quot;found first at %d&quot;, (int) i); first = b; } dd(&quot;adding size %d&quot;, (int) (b-&gt;pos - b-&gt;start)); size += b-&gt;pos - b-&gt;start; /*buf next*/ #if nginx_version &gt;= 1011011 hc-&gt;busy = hc-&gt;busy-&gt;next; #endif } } 可以看到这里通过#if nginx_version &gt;= 1011011预编译指令判断nginx版本来决定使用b = hc-&gt;busy-&gt;buf;来兼容高版本还是使用b = hc-&gt;busy[i];兼容老版本，至于nginx_version是啥，见nginx源码中的nginx.h头文件： #ifndef _NGINX_H_INCLUDED_ #define _NGINX_H_INCLUDED_ #define nginx_version 1011013 #define NGINX_VERSION &quot;1.11.13&quot; #define NGINX_VER &quot;nginx/&quot; NGINX_VERSION #ifdef NGX_BUILD #define NGINX_VER_BUILD NGINX_VER &quot; (&quot; NGX_BUILD &quot;)&quot; #else #define NGINX_VER_BUILD NGINX_VER #endif #define NGINX_VAR &quot;NGINX&quot; #define NGX_OLDPID_EXT &quot;.oldbin&quot; #endif /* _NGINX_H_INCLUDED_ */ 为了便于判断nginx版本，nginx中定义了nginx_version的规则，1.11.13就是1011013,1.11.11就是1011011，由此解决第三方lua模块的兼容性问题。","categories":[{"name":"blog","slug":"blog","permalink":"/categories/blog/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"/categories/blog/"}]},{"title":"Nginx epoll事件驱动","slug":"2017-04-05-nginx_epoll","date":"2017-04-04T16:00:00.000Z","updated":"2020-01-03T09:55:50.314Z","comments":true,"path":"2017/04/05/2017-04-05-nginx_epoll/","link":"","permalink":"/2017/04/05/2017-04-05-nginx_epoll/","excerpt":"###Linux IO模式 在了解Nginx事件驱动机制之前首先我们需要知道的是Linux的IO模式，之所以需要了解Linux的IO模式是应为Nginx正是通过它实现的”非阻塞IO”与高并发。","text":"###Linux IO模式 在了解Nginx事件驱动机制之前首先我们需要知道的是Linux的IO模式，之所以需要了解Linux的IO模式是应为Nginx正是通过它实现的”非阻塞IO”与高并发。 Linux IO模式及select、poll、epoll同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？在不通的场景下不同的人给出的答案是不同的，所以本文限定一下我们讨论的上下文环境。 本文讨论的背景前提是Linux环境下的network IO。 一、基本概念在这之前我们需要知道几个概念： 用户空间和内核空间 进程间切换 进程的阻塞 文件描述符 缓存IO 用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程间切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 注：总而言之就是很耗资源，具体的可以参考这篇文章：进程间切换 进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程 进入阻塞状态，是不占用CPU资源的。 文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存IO缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 二、IO模式","categories":[{"name":"io","slug":"io","permalink":"/categories/io/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"io","slug":"io","permalink":"/categories/io/"}]},{"title":"nginx-1.11.11模块编译报错解决","slug":"2017-03-24-build_nginx_error","date":"2017-03-23T16:00:00.000Z","updated":"2020-01-03T09:55:50.279Z","comments":true,"path":"2017/03/24/2017-03-24-build_nginx_error/","link":"","permalink":"/2017/03/24/2017-03-24-build_nginx_error/","excerpt":"Nginx最新版1.11.11编译报错最近nginx官网更新了nginx的最新版nginx-1.11.11，于是下载下来想要搭在新买的vps上。但是发现在了一个问题，在编译添加openresty外部模块lua-nginx-module和echo-nginx-module时报错，报错如下：","text":"Nginx最新版1.11.11编译报错最近nginx官网更新了nginx的最新版nginx-1.11.11，于是下载下来想要搭在新买的vps上。但是发现在了一个问题，在编译添加openresty外部模块lua-nginx-module和echo-nginx-module时报错，报错如下： 报错分析由报错信息可以看出lua-nginx-module模块的ngx_http_lua_headers.c文件中的第151和第227行报类型不匹配的错误。分析lua-nginx-module源码可以看出：ngx_http_connection_t结构体中的busy参数,由ngx_buf_t的双重指针变为ngx_chain_t的ngx_buf_t链表:下面是报错变量和对应的报错位置：为何函数中b = hc-&gt;busy[i]改为b = hc-&gt;busy-&gt;buf和结尾的hc-&gt;busy = hc-&gt;busy-&gt;next就可以了呢？看下图:与上面的ngx_buf_t的双重指针对比发现了什么呢？没错，这里其实就是要取ngx_buf_t的值，1.10以及之前的做法是ngx_buf_t双重指针的循环遍历。而1.11开始使用ngx_chain_s链表来遍历获取ngx_buf_t的值。我们要做的只不过是将循环中的ngx_buf_t的双重指针的循环修改为链表的方式就好了。编译之后没有报错： 到此分析的也就差不多了呢，echo模块的报错原因也是如此，就是写blog的过程中官网又更新了最新版nginx-1.11.12版本,1.11.12版本这部分的代码是一样的，其他地方的修改就不知道了，不知道会不会还有别的和第三方模块不兼容的地方。等待后续验证！","categories":[{"name":"C","slug":"C","permalink":"/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"/tags/C/"}],"keywords":[{"name":"C","slug":"C","permalink":"/categories/C/"}]},{"title":"在搬瓦工VPS上搭建VPN服务","slug":"2017-03-24-build_vpn_in_vps","date":"2017-03-23T16:00:00.000Z","updated":"2020-01-03T09:55:50.292Z","comments":true,"path":"2017/03/24/2017-03-24-build_vpn_in_vps/","link":"","permalink":"/2017/03/24/2017-03-24-build_vpn_in_vps/","excerpt":"搭建Shadowsocks服务安装组件$ yum install m2crypto python-setuptools $ easy_install pip $ pip install shadowsocks","text":"搭建Shadowsocks服务安装组件$ yum install m2crypto python-setuptools $ easy_install pip $ pip install shadowsocks 安装完成以后配置服务端参数$ vi /etc/shadowsocks.json 写入如下配置 { &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:443, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;rc4-md5&quot;, &quot;fast_open&quot;: false, &quot;workers&quot;: 1 } 将上面的mypassword替换成你自己的密码server_port也可以修改，默认是443端口 运行启动Shadowsocks服务运行下面的命令 $ ssserver -c /etc/shadowsocks.json 如果想要后台运行，运行如下命令 ssserver -c /etc/shadowsocks.json -d start 添加开机启动项命令行输入如下命令 $ cd /etc/rc.d/rc.local 添加 ssserver -c /etc/shadowsocks.json -d start 配置多客户端登陆修改shadowsocks.json，修改配置文件内容为下面的配置 { &quot;server&quot;:&quot;23.105.215.43&quot;, #你服务器的ip &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:{ #端口以及对应的密码 &quot;9000&quot;:&quot;password&quot;, &quot;9001&quot;:&quot;password&quot;, &quot;9002&quot;:&quot;password&quot; }, &quot;timeout&quot;:300, &quot;method&quot;:&quot;rc4-md5&quot;, #选择的加密方式 &quot;fast_open&quot;: false } Shadowsocks客户端下载在这里下载安装请点击下载，选择自己想要的版本，这里提供的iphone版本需要购买貌似要40RMB，还是挺贵的，我选择的是appstore上的其他的版本的。直接在appstore上搜索shadowrocket，这个很好用而且只要6RMB。下载好客户端只要配置好ip、port、method（你的服务端配置的加密方式）、以及密码之后你就可以愉快的翻墙了！下面贴出了app上的连接，windows按照上面的地址下载好按照相应的配置好就行了。","categories":[{"name":"VPS","slug":"VPS","permalink":"/categories/VPS/"}],"tags":[{"name":"VPS","slug":"VPS","permalink":"/tags/VPS/"}],"keywords":[{"name":"VPS","slug":"VPS","permalink":"/categories/VPS/"}]},{"title":"lua脚本解析webhooks实现自动部署","slug":"2017-02-27-deploy_blog_by_lua","date":"2017-02-26T16:00:00.000Z","updated":"2020-01-03T09:55:50.216Z","comments":true,"path":"2017/02/27/2017-02-27-deploy_blog_by_lua/","link":"","permalink":"/2017/02/27/2017-02-27-deploy_blog_by_lua/","excerpt":"码云配置webhooks实现自动部署的方式有很多，可以把项目放在github用github的webhooks，但是github只能创建公开项目私有项目是要钱的。也可以使用一种相对麻烦的方式，可以在vps上建一个git仓库每次提交到vps上之后触发hook去更新blog，这种方法相对麻烦而且vps上的项目容易丢失。所以最后选择了码云，在码云上初始化一个blog项目，之后配置一个webhooks并且跑配置密码，防止url被恶意调用。","text":"码云配置webhooks实现自动部署的方式有很多，可以把项目放在github用github的webhooks，但是github只能创建公开项目私有项目是要钱的。也可以使用一种相对麻烦的方式，可以在vps上建一个git仓库每次提交到vps上之后触发hook去更新blog，这种方法相对麻烦而且vps上的项目容易丢失。所以最后选择了码云，在码云上初始化一个blog项目，之后配置一个webhooks并且跑配置密码，防止url被恶意调用。 如下图： nginx配置前提是安装了lua或者Luajit以及nginx编译了lua-nginx-module模块 location /webhooks { content_by_lua_file blog_hook.lua; } 现在开始编写lua脚本 ngx.req.read_body() local data = ngx.req.get_body_data() local args = ngx.req.get_uri_args() local cjson = require &quot;cjson&quot; value = cjson.decode(data) if value[&quot;password&quot;]==&quot;your_password&quot; then os.execute(&quot;sh /www/blog_update.sh&quot;); ngx.say(&quot;OK&quot;) ngx.exit(200) else ngx.exit(404) end 这里应该注意，在获取body内容之前必须ngx.req.read_body()，通过local data = ngx.req.get_body_data()获取body内容，通过local args = ngx.req.get_uri_args()获取url后面携带的参数。 “cjson”使用cjson模块解析json字符串 $ git clone https://github.com/openresty/lua-cjson.git $ cd ./lua-cjson 在编译之前需要修改一下Makefile 21 LUA_INCLUDE_DIR ?= $(PREFIX)/include 打开Makefile修改第21行，修改为 21 LUA_INCLUDE_DIR ?= $(PREFIX)/include/luajit-2.0 因为luajit默认将lua头文件放在 /usr/local/include/luajit-2.0 中，修改之后执行 $ make all $ make install 之后就可以通过cjson解析body中的json串了，密码验证通过才可以继续调用自动更新脚本否则返回404，更新的脚本这里也列出来。 编写shell脚本#!/bin/sh export PATH=$PATH:/usr/local/nodejs/bin export LANG=&quot;en_US.UTF-8&quot; unset GIT_DIR NowPath=`pwd` WebSite=&quot;/www&quot; BlogPath=&quot;/www/blog&quot; if [ ! -d &quot;$BlogPath&quot; ]; then cd $WebSite git clone git@git.oschina.net:sunblog/blog.git cd $BlogPath else cd $BlogPath git fetch --all git reset --hard origin/master fi hexo clean hexo g #awk &#39;{message=&quot;亲爱的&quot;$1&quot;，您所关注的博客Sun‘s Blog已更新，快去围观吧！https://www.sunsblog.cn&quot;;tittle=&quot;Sun’s Blog更新&quot;;system(&quot;echo \\&quot;&quot;message&quot;\\&quot; | mail -s \\&quot;&quot;tittle&quot;\\&quot; &quot;$2);}&#39; /www/mail.txt echo 您的博客已成功更新，https://www.sunsblog.cn | mail -s 博客更新成功 admin@sunsblog.cn cd $NowPath echo &quot;deploy done&quot; exit 0 脚本会去判断blog文件夹是否存在不存在就clone否则强制更新，每次更新成功之后都会向你自己或者其他人发送邮件，我这里使用awk读取mail.txt中的qq好友的邮箱账号去发送邮件，由于同一时间发送大量邮件可能会存在vps被封的风险。所以最好还是不要大量发送，只是每次给自己或者几个重要的人发送就好了。感兴趣的话可以自己研究awk以及sendmail。","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"vps","slug":"vps","permalink":"/tags/vps/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"突破算法之-选择排序","slug":"2016-10-14-select_sort","date":"2016-10-14T06:00:00.000Z","updated":"2020-01-03T10:12:29.156Z","comments":true,"path":"2016/10/14/2016-10-14-select_sort/","link":"","permalink":"/2016/10/14/2016-10-14-select_sort/","excerpt":"选择排序原理选择排序思想就是遍历找出最小值或者最大值后维护有序序列，从第一个元素开始依次往后找到最小或者最大元素，然后与第一个元素交换，然后从第二个元素开始依次往复直到倒数第二个元素找出最小数，时间复杂度O(n^2)。","text":"选择排序原理选择排序思想就是遍历找出最小值或者最大值后维护有序序列，从第一个元素开始依次往后找到最小或者最大元素，然后与第一个元素交换，然后从第二个元素开始依次往复直到倒数第二个元素找出最小数，时间复杂度O(n^2)。 Golang代码实现package main import &quot;fmt&quot; /** 1. 找到数组里最小的元素 2. 让它和数组的第一个元素交换 3. 在剩下元素中找到最小值，与数组的第二个元素交换 4. 如此往复，直到将整个数组排序 */ func main() { var minIndex int array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} for i := 0; i &lt; len(array)-1; i++ { minIndex = i for j := i + 1; j &lt; len(array); j++ { if array[j] &lt; array[minIndex] { minIndex = j } } array[i], array[minIndex] = array[minIndex], array[i] } fmt.Println(array) } github地址 选择排序的改进每次循环时可以分别找出最大值与最小值然后进行双向选择排序，外层判断minIndex==maxIndex+1跳出循环。 array := []int{6, 7, 9, 3, 6, 8, 1, 9, 3} len := len(array) for i := 0; i &lt; len-1; i++ { minIndex := i maxIndex := len - i - 1 if minIndex == maxIndex+1 { break } for j := i + 1; j &lt; len; j++ { if array[j] &lt; array[minIndex] { minIndex = j } else if array[j] &gt; array[maxIndex] &amp;&amp; j &lt; len-i-1 { maxIndex = j } } array[i], array[minIndex] = array[minIndex], array[i] array[len-i-1], array[maxIndex] = array[maxIndex], array[len-i-1] } fmt.Println(array)","categories":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}],"tags":[{"name":"sort","slug":"sort","permalink":"/tags/sort/"}],"keywords":[{"name":"algorithms","slug":"algorithms","permalink":"/categories/algorithms/"}]},{"title":"公司Rest架构启动过程分析","slug":"2016-10-14-rest_framework_analyse","date":"2016-10-13T16:00:00.000Z","updated":"2020-01-03T10:12:29.222Z","comments":true,"path":"2016/10/14/2016-10-14-rest_framework_analyse/","link":"","permalink":"/2016/10/14/2016-10-14-rest_framework_analyse/","excerpt":"什么是REST具象状态传输（英文：Representational State Transfer，简称REST），是Roy Thomas Fielding博士与2000年在他的博士论文 “Architectural Styles and the Design of Network-based Software Architectures” 中提出来的一种万维网软件架构风格。 REST相对于传统的SOAP到底有哪些不同，或者说两者分别更适合运用于什么样的场景。","text":"什么是REST具象状态传输（英文：Representational State Transfer，简称REST），是Roy Thomas Fielding博士与2000年在他的博士论文 “Architectural Styles and the Design of Network-based Software Architectures” 中提出来的一种万维网软件架构风格。 REST相对于传统的SOAP到底有哪些不同，或者说两者分别更适合运用于什么样的场景。 SOAP（简单对象访问协议）：是交换数据的一种协议规范，使用在计算机网络Web服务（web service）中，交换带结构信息。SOAP为了简化网页服务器（Web Server）从XML数据库中提取数据时，节省去格式化页面时间，以及不同应用程序之间按照HTTP通信协议，遵从XML格式执行资料互换，使其抽象于语言实现、平台和硬件。 REST：是一种架构风格，相对SOAP要相对简单，资源是由URI来指定，对资源的操作包括获取、创建、修改和删除资源，这些操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法，通过操作资源的表现形式来操作资源，资源的表现形式则是XML或者HTML，取决于读者是机器还是人，是消费web服务的客户软件还是web浏览器，当然也可以是任何其他的格式。REST的几个重要约束： 客户-服务器（Client-Server）通信只能由客户端单方面发起，表现为请求-响应的形式。 无状态（Stateless）通信的会话状态（Session State）应该全部由客户端负责维护。 缓存（Cache）响应内容可以在通信链的某处被缓存，以改善网络效率。 统一接口（Uniform Interface）通信链的组件之间通过统一的接口相互通信，以 提高交互的可见性。 分层系统（Layered System）通过限制组件的行为（即，每个组件只能“看到”与其交互的紧邻层），将架构分解为若干等级的层。 按需代码（Code-On-Demand，可选）支持通过下载并执行一些代码（例如Java Applet、Flash或JavaScript），对客户端的功能进行扩展。 这只是网上搜索出来的解释，更加深入的理解需要具体的项目开发中去慢慢理解。 公司架构源码以及启动过程分析1.首先加载参数spring.profiles.active设置项目环境信息（dev、test、product），配置contextConfigLocation配置Spring配置文件路径。紧接着容器会初始化一个ServletContext上下文，利用 ServletContext 能够获得 WEB 运用的配置信息, 实现在多个 Servlet 之间共享数据等。 2.ContextLoaderListener根据contextConfigLocation指定的Spring配置文件去初始化spring的WebApplicationContext上下文，并加入ServletContext中（ServletContext是整个WEB应用的上下文，在容器（Tomcat、JBoos等）完全启动WEB应用之前被创建，生命周期伴随真个WEB应用），之后可以通过自定义的框架上下文监听器获取ServletContex去获取WebApplicationContext上下文。 上述为什么是WebApplicationContext而不是ApplicationContext呢？ SpringMVC本身是不具备Web功能的，WebApplicationContext通过继承ApplicationContext去扩展获得Web功能，Spring MVC是如何在web环境中创建IoC容器呢？web环境中的IoC容器的结构又是什么结构呢？web环境中，spring IoC容器是怎么启动呢？ 在web.xml配置文件中，有两个主要的配置：ContextLoaderListener和DispatcherServlet。同样的关于spring配置文件的相关配置也有两部分：context-param和DispatcherServlet中的init-param。那么，这两部分的配置有什么区别呢？它们都担任什么样的职责呢？ 在Spring MVC中，Spring Context是以父子的继承结构存在的。Web环境中存在一个ROOT Context，这个Context是整个应用的根上下文，是其他context的双亲Context。同时Spring MVC也对应的持有一个独立的Context，它是ROOT Context的子上下文。 对于这样的Context结构在Spring MVC中是如何实现的呢？下面就先从ROOT Context入手，ROOT Context是在ContextLoaderListener中配置的，ContextLoaderListener读取context-param中的contextConfigLocation指定的配置文件，创建ROOT Context。 Spring MVC启动过程大致分为两个过程： ContextLoaderListener初始化，实例化IoC容器，并将此容器实例注册到ServletContext中； DispatcherServlet初始化； 其中ContextLoaderListener是在Web容器中初始化Spring的根上下文和实例化IOC容器，DispatcherServlet是初始化Spring MVC对应的上下文。具体分析请参考原文：深入分析Spring与Spring容器 3. ArchListener 我们可以通过实现ServletContextListener去实现我们自己的Servlet监听器，可以在我们的Servlet监听器中的contextInitialized方法中通过WebApplicationContextUtils.getRequiredWebApplicationContext(event.getServletContext());从应用上下文中去获取WebApplicationCotext上下文。基于这一方式，我们可以通过自己创建框架上下文监听类添整个框架所有的上下文实现对整个框架的所有上下文管理。 4.RestServiceListener（Rest服务监听） 从ServletContext中获取Spring的WebApplicationContext上下文： 获取到Spring的上下文后获取上面Spring配置文件 中的RestServiceConfiguration Bean Rest基础服务对象：用于保存前置拦截器、自定义前置拦截器、后置拦截器、自定义后置拦截器、CGI服务名称（配置文件指定）、CMD前缀（配置文件指定）、调用业务超时邮件接收者（配置文件指定）、系统异常邮件接收者（配置文件指定），之后通过RestServiceUtils.registerServices(configuration);（扫描自定义注解进行依赖注入）configuration.init();（初始化导入拦截器） 5.初始化一系列过滤器6.DispatcherServlet：初始化Spring MVC上下文7.PropertiesServlet：用于动态修改配置文件8.紧接着Spring接管MsgSenderService时@PostConstruct注解的init方法会自动执行初始化MQ发送端、初始化BaseConsumer","categories":[{"name":"framework","slug":"framework","permalink":"/categories/framework/"}],"tags":[{"name":"rest","slug":"rest","permalink":"/tags/rest/"}],"keywords":[{"name":"framework","slug":"framework","permalink":"/categories/framework/"}]},{"title":"Spring Boot配置定时任务以及注册启动类","slug":"2016-08-1-springboot_scheduling","date":"2016-08-13T16:00:00.000Z","updated":"2020-01-03T10:12:29.213Z","comments":true,"path":"2016/08/14/2016-08-1-springboot_scheduling/","link":"","permalink":"/2016/08/14/2016-08-1-springboot_scheduling/","excerpt":"配置Spring Boot定时任务在Spring Boot启动方法上加上@EnableScheduling开启定时任务","text":"配置Spring Boot定时任务在Spring Boot启动方法上加上@EnableScheduling开启定时任务 @SpringBootApplication @EnableScheduling @EnableAutoConfiguration @ComponentScan(basePackages = &quot;com.smtcl.machinetool&quot;) public class Main extends SpringBootServletInitializer{ private static Class&lt;Main&gt; applicationClass = Main.class; /*build spring boot*/ @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder){ return builder.sources(applicationClass); } /*spring boot 启动主方法*/ public static void main(String[] args){ SpringApplication.run(applicationClass, args); } } 开启定时任务 @Scheduled(fixedRate = 10000) public void updateOemTool(){ // List&lt;Name&gt; list = idao.executeQuery(&quot;from Name&quot;); // for (Name name : list){ // // System.out.println(name.getName()); // } } fixedRate=10000表示每十秒执行一次该方法，如果需要更精确的定时可以用 cron表达式 &quot;0 0 12 * * ?&quot; 每天中午十二点触发 &quot;0 15 10 ? * *&quot; 每天早上10：15触发 &quot;0 15 10 * * ?&quot; 每天早上10：15触发 &quot;0 15 10 * * ? *&quot; 每天早上10：15触发 &quot;0 15 10 * * ? 2005&quot; 2005年的每天早上10：15触发 &quot;0 * 14 * * ?&quot; 每天从下午2点开始到2点59分每分钟一次触发 &quot;0 0/5 14 * * ?&quot; 每天从下午2点开始到2：55分结束每5分钟一次触发 &quot;0 0/5 14,18 * * ?&quot; 每天的下午2点至2：55和6点至6点55分两个时间段内每5分钟一次触发 &quot;0 0-5 14 * * ?&quot; 每天14:00至14:05每分钟一次触发 &quot;0 10,44 14 ? 3 WED&quot; 三月的每周三的14：10和14：44触发 &quot;0 15 10 ? * MON-FRI&quot; 每个周一、周二、周三、周四、周五的10：15触发 配置随项目启动启动在application.propertites中添加如下配置 context.listener.classes=你要注册的类的路径 你要注册的类只要继承ApplicationListener&lt;ContextRefreshedEvent&gt;并且实现onApplicationEvent方法就可以了，这时项目启动的时候就会自动运行onApplicationEvent中的服务。 public class RFIDTCPService implements ApplicationListener&lt;ContextRefreshedEvent&gt;{ public void onApplicationEvent(ContextRefreshedEvent event){ /** * 获取服务端端口 */ Integer port = (Integer) event.getApplicationContext().getBean(&quot;SERVER_PORT&quot;); new Thread(new waitConnection(port)).start(); } }","categories":[{"name":"java","slug":"java","permalink":"/categories/java/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"/tags/SpringBoot/"}],"keywords":[{"name":"java","slug":"java","permalink":"/categories/java/"}]},{"title":"使用Spring事务时遇到的Session问题","slug":"2016-08-05-problem_hibernate_session","date":"2016-08-04T16:00:00.000Z","updated":"2020-01-03T10:12:29.241Z","comments":true,"path":"2016/08/05/2016-08-05-problem_hibernate_session/","link":"","permalink":"/2016/08/05/2016-08-05-problem_hibernate_session/","excerpt":"问题当开启事务时，我们需要更新一个表对象数据时，可能我们并没有所有的表对象的数据，这时我们需要先查看他的原始数据，然后更新数据到我们重新封装的实例对象中去。这时如果我们直接去save()或者update()这个对象就回报这样的错。","text":"问题当开启事务时，我们需要更新一个表对象数据时，可能我们并没有所有的表对象的数据，这时我们需要先查看他的原始数据，然后更新数据到我们重新封装的实例对象中去。这时如果我们直接去save()或者update()这个对象就回报这样的错。 org.springframework.dao.DuplicateKeyException: A different object with the same identifier value was already associated with the session 原因因为在查询对象数据的时候session中就已经保了这个对象，现在又要再次去更新或者保存这个对象，hibernate就会不知道到底哪一个才是我们需要去操作的对象。因为这时session中有了两个相同id但是不同的实体，这是我们需要先删除session中存在的对象。 删除session中缓存对象的方式：1.clear()(不推荐使用，会直接清空session，操作影响太大)2.evict()(定点删除session中的某个对象)这个也是我自己的做法，这样删除影响最小。 附上代码： //通过物料编号获取物料id String hql = &quot;from CGeneralMaterial cgm where cgm.materialNo=&#39;&quot; + materialNo + &quot;&#39;&quot;; List&lt;CGeneralMaterial&gt; materialIdList = dao.executeQuery(hql); Integer materialId = materialIdList.get(0).getMaterialId(); saveMaterial.setMaterialId(materialId); /** * 这里是删除session中相同的对象 */ session.evict(materialIdList.get(0));//少了这句就会报上面的错误 dao.update(saveMaterial);","categories":[{"name":"java","slug":"java","permalink":"/categories/java/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"/tags/SpringBoot/"}],"keywords":[{"name":"java","slug":"java","permalink":"/categories/java/"}]},{"title":"开启Spring事务机制","slug":"2016-08-04-spring_transaction","date":"2016-08-03T16:00:00.000Z","updated":"2020-01-03T10:12:29.186Z","comments":true,"path":"2016/08/04/2016-08-04-spring_transaction/","link":"","permalink":"/2016/08/04/2016-08-04-spring_transaction/","excerpt":"什么是事务？在企业级开发过程中，对于与无人员来说一个实际的对数据库的操作可能是多步结合进行的。","text":"什么是事务？在企业级开发过程中，对于与无人员来说一个实际的对数据库的操作可能是多步结合进行的。由于对数据库的操作在任何一个步骤都有可能会发生异常，异常会导致后续操作无法完成，可是此时业务逻辑没有正确完成，之前的操作数据也就不可靠，需要在这种情况下对之前的操作的数据进行回滚。 回滚是为了保证用户每一步的操作都是可靠的，事务中的每一步操作都必须正确完成，只要有一个异常的发生都需要回退到开始的未进行的状态。 事务管理也是Spring框架中最为常用的功能之一。 快速入门在Spring Boot中，当我们使用了spring-boot-starter-jdbc或spring-boot-starter-data-jpa依赖的时候，框架会自动默认分别注入 DataSourceTransactionManager或JpaTransactionManager。所以我们不需要任何额外配置就可以用@Transactional注解进行事务的使用。 通常在单元测试中，我们只是想要测试业务逻辑的正确与否，并不想保存测试数据。 @RunWith(SpringJUnit4ClassRunner.class) @SpringApplicationConfiguration(Application.class) public class ApplicationTests { @Autowired private UserRepository userRepository; @Test public void test() throws Exception { // 创建10条记录 userRepository.save(new User(&quot;AAA&quot;, 10)); userRepository.save(new User(&quot;BBB&quot;, 20)); userRepository.save(new User(&quot;CCC&quot;, 30)); userRepository.save(new User(&quot;DDD&quot;, 40)); userRepository.save(new User(&quot;EEE&quot;, 50)); userRepository.save(new User(&quot;FFF&quot;, 60)); userRepository.save(new User(&quot;GGG&quot;, 70)); userRepository.save(new User(&quot;HHH&quot;, 80)); userRepository.save(new User(&quot;III&quot;, 90)); userRepository.save(new User(&quot;JJJ&quot;, 100)); // 省略后续的一些验证操作 } } 可以看到，在这个单元测试用例中，使用UserRepository对象连续创建了10个User实体到数据库中，下面我们人为的来制造一些异常，看看会发生什么情况。 通过定义User的name属性长度为5，这样通过创建时User实体的name属性超长就可以触发异常产生。 @Entity public class User { @Id @GeneratedValue private Long id; @Column(nullable = false, length = 5) private String name; @Column(nullable = false) private Integer age; // 省略构造函数、getter和setter } 修改测试用例中创建记录的语句，将一条记录的name长度超过5，如下：name为HHHHHHHHH的User对象将会抛出异常。 // 创建10条记录 userRepository.save(new User(&quot;AAA&quot;, 10)); userRepository.save(new User(&quot;BBB&quot;, 20)); userRepository.save(new User(&quot;CCC&quot;, 30)); userRepository.save(new User(&quot;DDD&quot;, 40)); userRepository.save(new User(&quot;EEE&quot;, 50)); userRepository.save(new User(&quot;FFF&quot;, 60)); userRepository.save(new User(&quot;GGG&quot;, 70)); userRepository.save(new User(&quot;HHHHHHHHHH&quot;, 80)); userRepository.save(new User(&quot;III&quot;, 90)); userRepository.save(new User(&quot;JJJ&quot;, 100)); 执行测试用例，可以看到控制台中抛出了如下异常，name字段超长： 2016-05-27 10:30:35.948 WARN 2660 --- [ main] o.h.engine.jdbc.spi.SqlExceptionHelper : SQL Error: 1406, SQLState: 22001 2016-05-27 10:30:35.948 ERROR 2660 --- [ main] o.h.engine.jdbc.spi.SqlExceptionHelper : Data truncation: Data too long for column &#39;name&#39; at row 1 2016-05-27 10:30:35.951 WARN 2660 --- [ main] o.h.engine.jdbc.spi.SqlExceptionHelper : SQL Warning Code: 1406, SQLState: HY000 2016-05-27 10:30:35.951 WARN 2660 --- [ main] o.h.engine.jdbc.spi.SqlExceptionHelper : Data too long for column &#39;name&#39; at row 1 org.springframework.dao.DataIntegrityViolationException: could not execute statement; SQL [n/a]; nested exception is org.hibernate.exception.DataException: could not execute statement 此时查数据库中，创建了name从AAA到GGG的记录，没有HHHHHHHHHH、III、JJJ的记录。而若这是一个希望保证完整性操作的情况下，AAA到GGG的记录希望能在发生异常的时候被回退，这时候就可以使用事务让它实现回退，做法非常简单，我们只需要在test函数上添加@Transactional注解即可。 @Test @Transactional public void test() throws Exception { // 省略测试内容 } 事务详解上面的例子中我们使用了默认的事务配置，可以满足一些基本的事务需求，但是当我们项目较大较复杂时（比如，有多个数据源等），这时候需要在声明事务时，指定不同的事务管理器。对于不同数据源的事务管理配置可以见《Spring Boot多数据源配置与使用》中的设置。在声明事务时，只需要通过value属性指定配置的事务管理器名即可，例如：@Transactional(value=”transactionManagerPrimary”)。 除了指定不同的事务管理器之后，还能对事务进行隔离级别和传播行为的控制，下面分别详细解释： 隔离级别隔离级别是指若干个并发的事务之间的隔离程度，与我们开发时候主要相关的场景包括：脏读取、重复读、幻读。 我们可以看org.springframework.transaction.annotation.Isolation枚举类中定义了五个表示隔离级别的值： public enum Isolation { DEFAULT(-1), READ_UNCOMMITTED(1), READ_COMMITTED(2), REPEATABLE_READ(4), SERIALIZABLE(8); } DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是：READ_COMMITTED。 READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 指定方法：通过使用isolation属性设置，例如： @Transactional(propagation = Propagation.REQUIRED) 实现回滚的三种方式1.直接抛出异常发生异常时在catch中直接通过throw抛出异常，因为Spring事务默认接收到抛出异常时回滚，从@Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)中的配置也可以看出。 2.抛出RunTimeException或者在catch中通过自己手动throw new RunTimeException();抛出异常。 3.TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();通过这种放发可以不需要抛出异常直接回滚。在前台需要返回异常信息时用这种方法就可以起到很好的作用。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"/categories/SpringBoot/"}],"tags":[{"name":"transaction","slug":"transaction","permalink":"/tags/transaction/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"/categories/SpringBoot/"}]},{"title":"通过账号密码查看Nginx日志","slug":"2016-08-01-nginx_logs_account","date":"2016-07-31T16:00:00.000Z","updated":"2020-01-03T10:12:29.189Z","comments":true,"path":"2016/08/01/2016-08-01-nginx_logs_account/","link":"","permalink":"/2016/08/01/2016-08-01-nginx_logs_account/","excerpt":"修改虚拟主机配置文件在server中添加如下代码 $ vim /usr/local/nginx/nginx.conf server{ location /logs { alias /usr/local/nginx/logs; autoindex on; autoindex_exact_size off; autoindex_localtime on; add_header Cache-Control no-store; auth_basic &quot;Restricted&quot;; auth_basic_user_file /usr/local/nginx/conf/loguser; } }","text":"修改虚拟主机配置文件在server中添加如下代码 $ vim /usr/local/nginx/nginx.conf server{ location /logs { alias /usr/local/nginx/logs; autoindex on; autoindex_exact_size off; autoindex_localtime on; add_header Cache-Control no-store; auth_basic &quot;Restricted&quot;; auth_basic_user_file /usr/local/nginx/conf/loguser; } } 其中auth_basic_user_file /usr/local/nginx/conf/loguser;指定登陆账号的用户名和密码。 通过htpasswd命令生成用户名及对应密码数据库文件$ htpasswd -c /usr/local/nginx/conf/loguser admin //admin为认证用户名 $ New password: ******** //输入认证密码 Re-type new password: ******** //再次输入认证密码 Adding password for user admin $ chmod 666 /usr/local/nginx/conf //修改网站认证数据库权限 $ cat /usr/local/nginx/conf/loguser //可以看到通过htpasswd生成的密码为加密格式 配置点击日志文件打开文件而不是下载打开文件 $ vim /usr/local/nginx/mime.types 添加 text/log log; 重启nginx$ nginx -s reload 访问站点查看日志https://sunsblog.cn/logs 输入用户名密码登陆 htpasswd命令汇总语法htppasswd(选项)（参数） 选项-c：创建一个加密文件； -n：不更新加密文件，只将加密后的用户名密码显示在屏幕上； -m：默认采用MD5算法对密码进行加密； -d：采用CRYPT算法对密码进行加密； -p：不对密码进行进行加密，即明文密码； -s：采用SHA算法对密码进行加密； -b：在命令行中一并输入用户名和密码而不是根据提示输入密码； -D：删除指定的用户。 参数用户： 要创建或者更新密码的用户名密码： 用户的新密码 实例利用htppasswd命令添加用户$ htpasswd -bc .passwd www.linuxde.net php 在bin目录下生成一个.passwd文件，用户名www.linuxde.net，密码：php，默认采用MD5加密方式。 在原有密码文件中增加下一个用户$ htpasswd -b .passwd Jack 123456 去掉-c选项，即可在第一个用户之后添加第二个用户，依此类推。 不更新密码文件，只显示加密后的用户名和密码$ htpasswd -nb Jack 123456 不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码。 利用htpasswd命令删除用户名和密码$ htpasswd -D .passwd Jack 利用htpasswd命令修改密码$ htpasswd -D .passwd Jack $ htpasswd -b .passwd Jack 123456 即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"nginx lua环境搭建","slug":"2016-07-31-nginx_lua","date":"2016-07-30T16:00:00.000Z","updated":"2020-01-03T10:12:29.192Z","comments":true,"path":"2016/07/31/2016-07-31-nginx_lua/","link":"","permalink":"/2016/07/31/2016-07-31-nginx_lua/","excerpt":"安装nginx及搭建本地测试环境下载最新版的nginx $ cd /usr/local/src $ wget http://nginx.org/download/nginx-1.11.3.tar.gz $ tar -zxvf nginx-1.11.3.tar.gz","text":"安装nginx及搭建本地测试环境下载最新版的nginx $ cd /usr/local/src $ wget http://nginx.org/download/nginx-1.11.3.tar.gz $ tar -zxvf nginx-1.11.3.tar.gz 安装Luajit(最新版) $ cd /usr/local/src $ wget http://luajit.org/download/LuaJIT-2.1.0-beta2.tar.gz $ tar -zxvf LuaJIT-2.1.0-beta2.tar.gz $ cd ./LuaJIT-2.1.0-beta2 $ make &amp;&amp; make install 在/usr/local/src目录下创建nginx-module目录，在nginx-module目录下下载第三方模块：lua-nginx-module: $ git clone https://github.com/chaoslawful/lua-nginx-module.git $ git clone https://github.com/agentzh/echo-nginx-module.git $ git clone https://github.com/simpl/ngx_devel_kit.git 为了避免冲突我这里是重新创建一个安装目录 $ mkdir /usr/local/nginx2 修改nginx配置：进入nginx-1.11.3目录 $ ./configure --prefix=/usr/local/nginx \\ --add-module=../nginx-module/lua-nginx-module \\ --add-module=../nginx-module/echo-nginx-module \\ --add-module=../nginx-module/ngx_devel_kit \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-pcre=/usr/local/src/pcre \\ --with-zlib=/usr/local/src/zlib \\ --with-openssl=/usr/local/src/openssl \\ $ make &amp;&amp; make install 这里就不详细的介绍各个模块的下载和安装，不清楚的可以到之前的文章查看详细安装配置编译Nginx 导入lualuajit默认是安装在/usr/local/lib目录下，但是nginx是从/usr/lib下找luajit，因此两种解决办法，一种是安装完成后手动cp luajit库移一下，另一种是安装nginx config前先导入环境变量,告诉nginx去哪里找luajit $ export LUAJIT_LIB=/usr/local/lib $ export LUAJIT_INC=/usr/local/include/luajit-2.0 如果运行nginx报error while loading shared libraries: libluajit-5.1.so.2: cannot open shared 错误，说明上面的修改还是没有生效。 尝试一下下面的方法： 如果共享库文件安装到了/usr/local/lib(很多开源的共享库都会安装到该目录下)或其它”非/lib或/usr/lib”目录下, 那么在执行ldconfig命令前, 还要把新共享库目录加入到共享库配置文件/etc/ld.so.conf中, 操作如下: $ cat /etc/ld.so.conf $ echo &quot;/usr/local/lib&quot; &gt;&gt; /etc/ld.so.conf $ ldconfig 测试环境$ vim /usr/example/lua/test.lua 添加如下内容： $ ngx.say(&quot;hello world&quot;); 然后修改nginx安装目录下的nginx.conf location /test { default_type &#39;text/html&#39;; content_by_lua_file /usr/example/lua/test.lua; } 在浏览器中输入：yourServerIp/test 如果出现：出现：hello world 恭喜！nginx+lua开发环境就安装成功了！","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"Hexo新建Markdown自动打开编辑器","slug":"2016-07-2-open_markdown_pad","date":"2016-07-27T16:00:00.000Z","updated":"2020-01-03T10:12:29.152Z","comments":true,"path":"2016/07/28/2016-07-2-open_markdown_pad/","link":"","permalink":"/2016/07/28/2016-07-2-open_markdown_pad/","excerpt":"添加js脚本在站点目录下新建一个js，内容如下： var exec = require(&#39;child_process&#39;).exec; hexo.on(&#39;new&#39;, function(data){ exec(&#39;start &quot;D:\\dev\\editor\\MarkdownPad2\\MarkdownPad2.exe&quot; &#39; + data.path); });","text":"添加js脚本在站点目录下新建一个js，内容如下： var exec = require(&#39;child_process&#39;).exec; hexo.on(&#39;new&#39;, function(data){ exec(&#39;start &quot;D:\\dev\\editor\\MarkdownPad2\\MarkdownPad2.exe&quot; &#39; + data.path); }); 应该看的很明显，start后的第一个字符串就是你的编辑器的可执行程序的路径，data.path就是你新生成的Markdown文件的路径。这就话就是用你指定的编辑器打开生成的Markdown文件。 $ hexo new &quot;newMarkdown&quot; 新建markdown文件的时候就会自动打开你的编辑器啦！","categories":[{"name":"blog","slug":"blog","permalink":"/categories/blog/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"/categories/blog/"}]},{"title":"替换Hexo代码段样式为google code prettify","slug":"2016-07-28-google_code_prettify","date":"2016-07-27T16:00:00.000Z","updated":"2020-01-03T10:12:29.146Z","comments":true,"path":"2016/07/28/2016-07-28-google_code_prettify/","link":"","permalink":"/2016/07/28/2016-07-28-google_code_prettify/","excerpt":"下载google code prettifynode下载： npm install google-code-prettify --save","text":"下载google code prettifynode下载： npm install google-code-prettify --save GitHub下载：https://github.com/google/code-prettify 替换hexo模板将下载好的prettify.js和themes文件夹放在主题下的source/js/google-code-prettify目录下。现在就开始修改主题的模板文件，修改主题目录下的layout/_layout.swig文件，在&lt;/head&gt;前添加 &lt;link href=&quot;/js/google-code-prettify/themes/tomorrow-night.min.css&quot; type=&quot;text/css&quot; rel=&quot;stylesheet&quot; /&gt; 其中tomorrow-night.min.css可以换成你想要的样式，参考这里选择一个自己喜欢的样式。在&lt;/body&gt;前添加： &lt;script type=&quot;text/javascript&quot; src=&quot;/js/google-code-prettify/prettify.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(&quot;pre&quot;).addClass(&quot;prettyprint&quot;); $(&quot;pre&quot;).addClass(&quot;linenums&quot;); $(&quot;pre&quot;).addClass(&quot;lang-html&quot;); $(&quot;pre&quot;).addClass(&quot;prettyprinted&quot;); prettyPrint(); &lt;/script&gt; 现在你只要在你只需要用&lt;pre&gt;&lt;/pre&gt;将你需要高亮的代码包裹起来就可以了。","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"vps安装ocserv","slug":"2016-07-28-install_ocserv","date":"2016-07-27T16:00:00.000Z","updated":"2020-01-03T10:12:29.231Z","comments":true,"path":"2016/07/28/2016-07-28-install_ocserv/","link":"","permalink":"/2016/07/28/2016-07-28-install_ocserv/","excerpt":"一、安装必要组件$ yum install epel-release $ yum install ocserv","text":"一、安装必要组件$ yum install epel-release $ yum install ocserv 二、生成证书1、生成ca证书生成ca私钥$ mkdir -p /etc/ocserv/pki/ca $ cd /etc/ocserv/pki/ca $ certtool --generate-privkey --outfile ca-key.pem 编写模板cat &lt;&lt; _EOF_ &gt;ca.tmpl cn = &quot;VPN CA&quot; organization = &quot;sunsblog&quot; serial = 1 expiration_days = 999 ca signing_key cert_signing_key crl_signing_key _EOF_ 生成ca证书$ certtool --generate-self-signed --load-privkey ca-key.pem --template ca.tmpl --outfile ca-cert.pem 2、生成server端证书生成server端私钥$ mkdir -p /etc/ocserv/pki/server $ cd /etc/ocserv/pki/server $ certtool --generate-privkey --outfile server-key.pem 生成server证书模板cat &lt;&lt; _EOF_ &gt;server.tmpl cn = &quot;sunsblog.cn&quot; organization = &quot;sunsblog.cn&quot; expiration_days = 999 signing_key encryption_key tls_www_server _EOF_ 生成server端证书$ certtool --generate-certificate --load-privkey server-key.pem --load-ca-certificate ../ca/ca-cert.pem --load-ca-privkey ../ca/ca-key.pem --template server.tmpl --outfile server-cert.pem 3、生成client端证书生成client端私钥$ certtool --generate-privkey --outfile user-key.pem 生成client端证书模板cat &lt;&lt; _EOF_ &gt;user.tmpl cn = &quot;user&quot; unit = &quot;admins&quot; expiration_days = 999 signing_key tls_www_client _EOF_ 生成client端证书$ certtool --generate-certificate --load-privkey user-key.pem --load-ca-certificate ../ca/ca-cert.pem --load-ca-privkey ../ca/ca-key.pem --template user.tmpl --outfile user-cert.pem 4、使用OpenSSL将客户端证书 user-cert.pem转化成.p12格式可以按照提示设置证书密码 $ openssl pkcs12 -export -inkey user-key.pem -in user-cert.pem -name &quot;client&quot; -certfile ../ca/ca-cert.pem -caname &quot;VPN CA&quot; -out user-cert.p12 三、修改配置文件$ vim /etc/ocserv/ocserv.conf #设置使用证书登录 auth = &quot;certificate&quot; #设置端口 tcp-port = 9000 udp-port = 9000 #将以ocserv这个用户和用户组去运行工作进程 run-as-user = ocserv run-as-group = ocserv #socket文件 socket-file = ocserv.sock #服务的默认目录 chroot-dir = /var/lib/ocserv #时候启用工作进程的命名空间隔离，进程隔离限定特定的libc版本以及系统内核版本，如果启动失败请关闭选项 isolate-workers = true #客户端最大连接数 max-clients = 16 #同一个账号最多可登陆的终端数 max-same-clients = 16 keepalive = 32400 # Dead Peer检测频率 dpd = 240 # 移动设备用的, 只要发送了 X-AnyConnect-Identifier-DeviceType 头, ocs 就会发送这个 dpd mobile-dpd = 1800 try-mtu-discovery = true # server端证书和私钥 server-cert = /etc/ocserv/pki/server/server-cert.pem server-key = /etc/ocserv/pki/server/server-key.pem # ca证书 ca-cert = /etc/ocserv/pki/ca/ca-cert.pem # 客户端id CN = 2.5.4.3, UID = 0.9.2342.19200300.100.1.1 cert-user-oid = 2.5.4.3 tls-priorities = &quot;NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0&quot; # 认证超时时间 auth-timeout = 240 # 最短的认证重新连接时间 min-reauth-time = 300 max-ban-score = 50 ban-reset-time = 300 # cookie保存时间，手机关屏在时间内打开会自动重新连接 cookie-timeout = 86400 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true # 进程id保存位置 pid-file = /var/run/ocserv.pid # 设备名称，做策略可能会用到 device = vpns predictable-ips = true # 默认绑定域名 default-domain = sunsblog.cn # 客户端分配的网段 ipv4-network = 192.168.101.0 ipv4-netmask = 255.255.255.0 # dns ip dns = 172.20.10.60 dns = 8.8.8.8 ping-leases = false # anyconnect 连接需要设置 true cisco-client-compat = true dtls-legacy = true user-profile = profile.xml #route = ip网段/子网掩码 # 全局路由 #route = 0.0.0.0/128.0.0.0 #route = 128.0.0.0/128.0.0.0 route-add-cmd = &quot;ip route add %{R} dev %{D}&quot; route-del-cmd = &quot;ip route delete %{R} dev %{D}&quot; #no-route 去掉vps ip地址和国内的网段 custom-header = &quot;X-DTLS-MTU: 1200&quot; custom-header = &quot;X-CSTP-MTU: 1200&quot; 添加开机启动项并且启动 $ systemctl enable ocserv $ systemctl start ocserv 允许流量转发,修改/etc/sysctl.conf net.ipv4.ip_forward = 1 使配置生效 sysctl -p /etc/sysctl.conf 在vpn虚拟网卡和vps外网网卡之间做流量转发 $ iptables -A FORWARD -i vpns0 -o venet0:0 -j ACCEPT $ iptables -A FORWARD -i venet0:0 -o vpns0 -m state --state ESTABLISHED,RELATED -j ACCEPT 开启NAT并且让iptables来协商MTU值 $ iptables -t nat -A POSTROUTING -j MASQUERADE $ iptables -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu 保存iptables配置，否则重启之后配置会失效 $ sysctl iptables save","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"vpn","slug":"vpn","permalink":"/tags/vpn/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"编译Nginx","slug":"2016-07-28-nginx","date":"2016-07-27T16:00:00.000Z","updated":"2020-01-03T09:55:50.170Z","comments":true,"path":"2016/07/28/2016-07-28-nginx/","link":"","permalink":"/2016/07/28/2016-07-28-nginx/","excerpt":"###安装gcc g++环境(vps已有编译环境可以省略) 安装make环境：yum -y install automake autoconf libtool libtool-ltdl libtool-ltdl-devel make安装gcc g++：yum -y install gcc gcc-c++","text":"###安装gcc g++环境(vps已有编译环境可以省略) 安装make环境：yum -y install automake autoconf libtool libtool-ltdl libtool-ltdl-devel make安装gcc g++：yum -y install gcc gcc-c++ 开始安装Nginx在安装nginx之前一般先安装pcre、zlib，前者为了nginx中rewrite功能，后者为了解压gzip。 ####源码目录我们一般选择/usr/local/src $ cd /usr/local/src 安装pcre库$ cd /usr/local/src $ wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.34.tar.gz $ tar -zxvf pcre-8.34.tar.gz $ mv pcre-8.34.tar.gz pcre $ cd pcre $ ./configure $ make &amp; make install 安装zlib库$ cd /usr/local/src $ wget http://zlib.net/zlib-1.2.8.tar.选择源码目录gz $ tar -zxvf zlib-1.2.8.tar.gz $ mv zlib-1.2.8.tar.gz zlib $ cd zlib $ ./configure $ make &amp; make install 安装SSl模块（某些vps默认没有安装SSL）$ cd /usr/local/src $ wget http://www.openssl.org/source/openssl-1.0.1c.tar.gz $ tar -zxvf openssl-1.0.1c.tar.gz $ mv openssl-1.0.1c.tar.gz openssl $ ./config $ make &amp; make install 安装Nginx$ cd /usr/local/src $ wget http://nginx.org/download/nginx-1.9.9.tar.gz $ tar -zxvf nginx-1.9.9.tar.gz $ cd nginx-1.9.9 $ ./configure --prefix=/usr/local/nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-pcre=/usr/local/src/pcre \\ --with-zlib=/usr/local/src/zlib \\ --with-openssl=/usr/local/src/openssl $ make &amp; make install 安装成功以后进入nginx目录启动nginx $ /usr/local/nginx/nginx 启动nginx如果提示80端口被占用。可以查看80端口被哪些进程占用，然后杀死该进程之后重启nginx 6.编译可能遇到的错误在更新了系统的openssl之后编译安装nginx的时候（具体是在make的时候）出现了如下报错： nginx的configure参数中的–with-openssl已经指向了openssl的源代码了，这个时候需要修改nginx源码下的auto/lib/openssl中的conf文件 $ vim nginx-1.6.1/auto/lib/openssl/conf 改为 即指定nginx所需openssl中的头文件和库文件的位置。保存之后，重新编译。 make成功，make install完成安装。","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"Hexo配置Git一键部署","slug":"2016-07-25-deploy_hexo","date":"2016-07-24T16:00:00.000Z","updated":"2020-01-03T09:55:50.276Z","comments":true,"path":"2016/07/25/2016-07-25-deploy_hexo/","link":"","permalink":"/2016/07/25/2016-07-25-deploy_hexo/","excerpt":"走过的坑本以为在自己服务器上装一个Git服务，然后Hexo直接配置一下就好了很简单的事，可是实践证明并不是那么简单，虽然走了一点弯路但是最后还是全部配置好了。","text":"走过的坑本以为在自己服务器上装一个Git服务，然后Hexo直接配置一下就好了很简单的事，可是实践证明并不是那么简单，虽然走了一点弯路但是最后还是全部配置好了。 服务器安装Git服务之后 使用XShell用SSH连接服务器失败（暂时没有找出原因）使用SSH登陆的时候，连接服务器时服务器总是会提示输入密码，这是因为/home/git/.ssh目录没有改变目录所属用户，将.ssh目录所属用户改为git就行了。使用Git Bash通过SSH连接远程VPS的时候会说连接被决绝，因为默认远程登录使用22端口，在.ssh目录添加config配置文件修改连接服务器的端口。Host sunsblog.cn User git Hostname 23.105.215.43 Port 27765 设置Git用户名、生成SSH密钥git config --global user.email &quot;email@example.com&quot; git config --global user.name &quot;username&quot; ssh-keygen -t rsa -C &quot;email@example.com&quot; #一路回车生成公钥和密钥，一会会要用到公钥id_rsa.pub 开始安装Git（CentOS）yum update &amp;&amp; apt-get upgrade -y #更新内核 yum install git-core 新建git用户添加sudo权限adduser git chmod 740 /etc/sudoers vim /etc/sudoers 在编辑器中找到如下内容 ##Allow root to run any commands anywhere root ALL=(ALL) ALL 添加下面一行 git ALL=(ALL) ALL 保存并退出后执行 chmod 440 /etc/sudoers 创建git仓库，并配置ssh登录su git cd /home/git mkdir .ssh &amp;&amp; cd .ssh touch authorized_keys vi authorized_keys //在这个文件中粘贴进刚刚申请的key（在id_rsa.pub文件中） chown git .ssh //注意如果.ssh目录不是git用户创建的一定要修改目录所属用户 修改文件权限 chmod 600 authorized_keys chmod 700 ~/.ssh 设置SSH，打开密钥登陆功能编辑/etc/ssh/sshd_conf文件，进行如下设置： RSAAuthentication yes PubkeyAuthentication yes 另外，请留意 root 用户能否通过 SSH 登录： PermitRootLogin yes 当你完成全部设置，并以密钥方式登录成功后，可禁用密码登录： PasswordAuthentication no 初始化Git仓库cd /home/git mkdir hexo.git &amp;&amp; cd hexo.git git init --bare 创建网站根目录并赋予git用户对网站目录的所有权cd /www mkdir blog chown git:git -R /www/blog 配置Git hooks脚本配置项目部署后自动更新网站su git cd /home/git/hexo.git/hooks vim post-receive 输入如下内容 #!/bin/sh unset GIT_DIR NowPath=`pwd` DeployPath=&quot;/www/blog&quot; cd $DeployPath git fetch --all git reset --hard origin/master cd $NowPath echo &quot;deploy done&quot; exit 0 改变脚本执行权限 chmod +x post-receive 部署和更新博客deploy: type: git message: update repo:git@sunsblog.cn:hexo.git,master","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"vps","slug":"vps","permalink":"/tags/vps/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]},{"title":"Nginx配置ssl模块","slug":"2016-07-24-ssl_nginx","date":"2016-07-23T16:00:00.000Z","updated":"2020-01-03T10:12:29.160Z","comments":true,"path":"2016/07/24/2016-07-24-ssl_nginx/","link":"","permalink":"/2016/07/24/2016-07-24-ssl_nginx/","excerpt":"什么是HTTPS？根据维基百科的解释： 超文本传输安全协议（缩写：HTTPS，英语：Hypertext Transfer Protocol Secure）是超文本传输协议和SSL/TLS的组合，用以提供加密通讯及对网络服务器身份的鉴定。","text":"什么是HTTPS？根据维基百科的解释： 超文本传输安全协议（缩写：HTTPS，英语：Hypertext Transfer Protocol Secure）是超文本传输协议和SSL/TLS的组合，用以提供加密通讯及对网络服务器身份的鉴定。HTTPS连接经常被用于万维网上的交易支付和企业信息系统中敏感信息的传输。HTTPS不应与在RFC 2660中定义的安全超文本传输协议（S-HTTP）相混。HTTPS 目前已经是所有注重隐私和安全的网站的首选，随着技术的不断发展，HTTPS 网站已不再是大型网站的专利，所有普通的个人站长和博客均可以自己动手搭建一个安全的加密的网站。 如果一个网站没有加密，那么你的所有帐号密码都是明文传输。可想而知，如果涉及到隐私和金融问题，不加密的传输是多么可怕的一件事。 鉴于本博客的读者都是接近专业人士，我们不再多费口舌，直接进入正题吧。 使用OpenSSL生成SSL Key和CSR由于只有浏览器或者系统信赖的 CA 才可以让所有的访问者通畅的访问你的加密网站，而不是出现证书错误的提示。所以我们跳过自签证书的步骤，直接开始签署第三方可信任的 SSL 证书吧。 OpenSSL 在 Linux、OS X 等常规的系统下默认都安装了，因为一些安全问题，一般现在的第三方 SSL 证书签发机构都要求起码 2048 位的 RSA 加密的私钥。 同时，普通的 SSL 证书认证分两种形式，一种是 DV（Domain Validated），还有一种是 OV （Organization Validated），前者只需要验证域名，后者需要验证你的组织或公司，在安全性方面，肯定是后者要好。 无论你用 DV 还是 OV 生成私钥，都需要填写一些基本信息，这里我们假设如下： 域名，也称为 Common Name，因为特殊的证书不一定是域名：example.com 组织或公司名字（Organization）：Example, Inc. 部门（Department）：可以不填写，这里我们写 Web Security 城市（City）：Beijing 省份（State / Province）：Beijing 国家（Country）：CN 加密强度：2048 位，如果你的机器性能强劲，也可以选择 4096 位 按照以上信息，使用 OpenSSL 生成 key 和 csr 的命令如下 openssl req -new -newkey rsa:2048 -sha256 -nodes -out example_com.csr -keyout example_com.key -subj “/C=CN/ST=Beijing/L=Beijing/O=Example Inc./OU=Web Security/CN=example.com”PS：如果是泛域名证书，则应该填写 *.example.com 你可以在系统的任何地方运行这个命令，会自动在当前目录生成 example_com.csr 和 example_com.key 这两个文件 接下来你可以查看一下 example_com.csr，得到类似这么一长串的文字 —–BEGIN CERTIFICATE REQUEST—–MIICujCCAaICAQAwdTELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaWppbmcxEDAOBgNVBAcTB0JlaWppbmcxFTATBgNVBAoTDEV4YW1wbGUgSW5jLjEVMBMGA1UECxMMV2ViIFNlY3VyaXR5MRQwEgYDVQQDEwtleGFtcGxlLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAPME+nvVCdGN9VWn+vp7JkMoOdpOurYMPvclIbsIiD7mGN982Ocl22O9wCV/4tL6DpTcXfNX+eWd7CNEKT4i+JYGqllqP3/CojhkemiYSF3jwncvP6VoST/HsZeMyNB71XwYnxFCGqSyE3QjxmQ9ae38H2LIpCllfd1l7iVpAX4i2+HvGTHFzb0XnmMLzq4HyVuEIMoYwiZX8hq+kwEAhKpBdfawkOcIRkbOlFewSEjLyHY+nruXutmQx1d7lzZCxut5Sm5At9al0bf5FOaaJylTEwNEpFkP3L29GtoUqg1t9Q8WufIfK9vXqQqwg8J1muK7kksnbYcoPnNgPx36kZsCAwEAAaAAMA0GCSqGSIb3DQEBBQUAA4IBAQCHgIuhpcgrsNwDuW6731/DeVwq2x3ZRqRBuj9/M8oONQen1QIacBifEMr+Ma+C+wIpt3bHvtXEF8cCAJAR9sQ4Svy7M0w25DwrwaWIjxcf/J8UaudL/029CkAuewFCdBILTRAAeDqxsAsUyiBIGTIT+uqi+EpGG4OlyKK/MF13FxDj/oKyrSJDtp1Xr9R7iqGCs/Zl5qWmDaLN7/qxBK6vX2R/HLhOK0aKi1ZQ4cZeP7Mr8EzjDIAko87Nb/aIsFyKrt6Ze3jOF0/vnnpw7pMvhq+folWdTVXddjd9Dpr2x1ncy5hnop4k6kVRXDjQ4OTduQq4P+SzU4hb41GIQEz4—–END CERTIFICATE REQUEST—–这个 CSR 文件就是你需要提交给 SSL 认证机构的，当你的域名或组织通过验证后，认证机构就会颁发给你一个 example_com.crt 而 example_com.key 是需要用在 Nginx 配置里和 example_com.crt 配合使用的，需要好好保管，千万别泄露给任何第三方。 可靠的第三方SSL签发机构本站使用的是沃通的免费电子证书申请免费电子证书，在网站填写域名登陆并提交申请验证域名所有权之后我们就可以拿到生成的证书，解压以后选择对应服务器的证书配置服务器即可，本站使用的是Nginx所以选择Nginx包下的证书。 Nginx配置HTTPS网站以及增加安全的配置前面已经提到，你需要提交 CSR 文件给第三方 SSL 认证机构，通过认证后，他们会颁发给你一个 CRT 文件，我们命名为 example_com.crt 同时，为了统一，你可以把这三个文件都移动到 /etc/nginx/ 目录。 然后可以修改 Nginx 配置文件 nginx.conf server{ listen 80; listen [::]:80 ssl ipv6only=on; listen 443 ssl; listen [::]:443 ssl ipv6only=on; server_name example.com; ssl on; ssl_certificate /etc/nginx/example_com.crt; ssl_certificate_key /etc/nginx/example_com.key; } 但是这么做并不安全，默认是 SHA-1 形式，而现在主流的方案应该都避免 SHA-1，为了确保更强的安全性，我们可以采取迪菲－赫尔曼密钥交换。首先，进入 /etc/ssl/certs 目录并生成一个 dhparam.pem cd /etc/ssl/certs openssl dhparam -out dhparam.pem 2048 #如果你的机器性能足够强大，可以用 4096 位加密 生成完毕后，在 Nginx 的 SSL 配置后面加入 sl_dhparam /etc/ssl/certs/dhparam.pem; 并且配置网站所有请求跳转https location / { rewrite ^/(.*) https://&amp;host/$1 permanent; } https默认跳转443端口，在nginx下新建vhosts文件夹放不同主机配置。这里以本网站域名为例，贴上配置，以供参考： server { server_name sunsblog.cn www.sunsblog.cn; listen 443 ssl http2 default_server; ssl on; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_certificate /etc/nginx/sunsblog.cn.crt; ssl_certificate_key /etc/nginx/sunsblog.cn.key; ssl_prefer_server_ciphers on; ssl_ciphers &quot;ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4&quot;; #ssl_dhparam /etc/ssl/dhparam4096.pem; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; #ssl_stapling on; #ssl_stapling_verify on; root /root/ddatsh.github.io; #Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; add_header Strict-Transport-Security &quot;max-age=63072000; includeSubdomains;preload&quot;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; if ( $host ~* &quot;\\d+\\.\\d+\\.\\d+\\.\\d+&quot; ) { return 503; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location / { root /www/blog; #proxy_pass http://localhost:4000; #root /usr/local/nginx/html; #root /usr/local/ink/blog/public; #root /usr/local/www_root/app-wzst; # alias d:/ddatsh.github.io; index index.html; } location /logs { alias /usr/local/nginx/logs; autoindex on; autoindex_exact_size off; autoindex_localtime on; add_header Cache-Control no-store; auth_basic &quot;Restricted&quot;; auth_basic_user_file /usr/local/nginx/conf/loguser; } }","categories":[{"name":"net","slug":"net","permalink":"/categories/net/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"/tags/nginx/"}],"keywords":[{"name":"net","slug":"net","permalink":"/categories/net/"}]},{"title":"安装和配置Hexo","slug":"2016-07-24-start_hexo","date":"2016-07-23T16:00:00.000Z","updated":"2020-01-03T10:12:29.175Z","comments":true,"path":"2016/07/24/2016-07-24-start_hexo/","link":"","permalink":"/2016/07/24/2016-07-24-start_hexo/","excerpt":"HexoHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。","text":"HexoHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 首先安装node.js和Git官方下载地址https://nodejs.org/en/download/，下载和安装好nodes.js之后。继续安装Git，https://git-scm.com/(已经有node和Git可以跳过这一步)。 安装Hexo$ npm install -g hexo-cli 使用Hexo构建博客新建一个文件夹如：hexo，进入hexo目录下运行 $ hexo init 之后一个博客的基本框架就购建好了，接下来如果不想修改主题的话就可以开始愉快的写博客了。通过： hexo new [layout] &lt;title&gt; 其中（layout）指定文章的布局，默认为post，可以修改站点配置文件_config.yml中的default_layout参数来修改指定默认布局。 接下来通过： $ heox g 将我们写好的markdown文档生成静态的html文档（生成的站点文件默认放在public文件夹下），之后修改站点配置文件_config.yml deploy: - type: git repo: 一个正确的部署配置中至少要有type参数，repo指定git仓库位置。如果通过git进行部署，我们需要先安装hexo-deployer-git $ npm install hexo-deployer-git --save 修改默认主题可以现在github上下载好自己想要的主题，本站用的是next主题在此下载，下载好主题之后我们就可以替换hexo的默认主题了，将下载好的主题放在themes文件夹下修改站点配置文件_config.yml中的theme参数为：next。这样我们就应用了next主题，之后next主题的配置请参照这里进行自己想要的配置就可以了。","categories":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"/tags/hexo/"}],"keywords":[{"name":"vps","slug":"vps","permalink":"/categories/vps/"}]}]}